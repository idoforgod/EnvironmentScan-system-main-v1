# Agent Swarm ì™„ì „ êµ¬í˜„ ì„¤ê³„ì•ˆ

**ë‚ ì§œ**: 2026-01-30
**ëª©ì **: API ì—†ì´ ìˆœìˆ˜ Pythonìœ¼ë¡œ ì§„ì§œ Agent Swarm êµ¬í˜„
**ì›ì¹™**: ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ì˜ ì² í•™, ëª©ì , í•µì‹¬ ì™„ë²½ ë³´ì¡´

---

## ğŸ¯ ì„¤ê³„ ëª©í‘œ

### ë‹¬ì„±í•  ê²ƒ
âœ… **ì§„ì§œ ë³‘ë ¬ ì‹¤í–‰**: Python multiprocessingìœ¼ë¡œ ì‹¤ì œ ë™ì‹œ ì‹¤í–‰
âœ… **ì§„ì§œ ë…ë¦½ ì»¨í…ìŠ¤íŠ¸**: ê° í”„ë¡œì„¸ìŠ¤ ë…ë¦½ ë©”ëª¨ë¦¬ ê³µê°„
âœ… **Task Graph ê´€ë¦¬**: JSON íŒŒì¼ ê¸°ë°˜ ì‘ì—… ìƒíƒœ ì¶”ì 
âœ… **ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ë³´ì¡´**: ì…ë ¥/ì¶œë ¥ í˜•ì‹, ë‹¤ìŒ ë‹¨ê³„ ì™„ë²½ í˜¸í™˜

### ì ˆëŒ€ í•˜ì§€ ì•Šì„ ê²ƒ
âŒ **API ì‚¬ìš© ê¸ˆì§€**: Claude Code Task API, ì™¸ë¶€ API í˜¸ì¶œ ì—†ìŒ
âŒ **ì›Œí¬í”Œë¡œìš° ë³€ê²½ ê¸ˆì§€**: ì² í•™, ëª©ì , í•µì‹¬ ë‹¨ê³„ ìˆ˜ì • ì—†ìŒ
âŒ **ìƒˆ ì›Œí¬í”Œë¡œìš° ê¸ˆì§€**: ê¸°ì¡´ ê°œì„ ë§Œ, ìƒˆë¡œìš´ ê²ƒ ë§Œë“¤ì§€ ì•ŠìŒ

---

## ğŸ—ï¸ í•µì‹¬ ì„¤ê³„: Python Multiprocessing

### ê¸°ìˆ  ì„ íƒ: multiprocessing.Pool

**ì´ìœ **:
1. **ì§„ì§œ ë³‘ë ¬**: GIL ìš°íšŒ, CPU ì½”ì–´ í™œìš©
2. **ë…ë¦½ ë©”ëª¨ë¦¬**: ê° í”„ë¡œì„¸ìŠ¤ ë…ë¦½ ê³µê°„
3. **ìˆœìˆ˜ Python**: API ì—†ì´ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ
4. **ì•ˆì •ì„±**: ê²€ì¦ëœ í‘œì¤€ ëª¨ë“ˆ

**ëŒ€ì•ˆ ê²€í†  ë° íƒˆë½ ì´ìœ **:
- âŒ threading: GIL ë•Œë¬¸ì— ì§„ì§œ ë³‘ë ¬ ì•„ë‹˜
- âŒ asyncio: I/O boundë§Œ íš¨ê³¼ì , CPU boundëŠ” ìˆœì°¨
- âŒ subprocess: ë³µì¡ë„ ë†’ìŒ, multiprocessingì´ ë” ë‚˜ìŒ

---

## ğŸ“ ì•„í‚¤í…ì²˜ ì„¤ê³„

### Before: ìˆœì°¨ ì‹¤í–‰ (í˜„ì¬)

```
main.py
  â”œâ”€ run_arxiv_agent()      # 15ì´ˆ (ê¸°ë‹¤ë¦¼)
  â”œâ”€ run_blog_agent()       # 1ì´ˆ (ê¸°ë‹¤ë¦¼)
  â””â”€ run_policy_agent()     # 1ì´ˆ (ê¸°ë‹¤ë¦¼)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´ 17ì´ˆ (ìˆœì°¨)
```

### After: ë³‘ë ¬ ì‹¤í–‰ (ì‹ ê·œ)

```python
from multiprocessing import Pool

# 4ê°œ ì—ì´ì „íŠ¸ë¥¼ ë…ë¦½ í”„ë¡œì„¸ìŠ¤ë¡œ ë³‘ë ¬ ì‹¤í–‰
with Pool(processes=4) as pool:
    results = pool.map(run_agent, [
        'arxiv',   # í”„ë¡œì„¸ìŠ¤ 1 (15ì´ˆ)
        'blog',    # í”„ë¡œì„¸ìŠ¤ 2 (1ì´ˆ)
        'policy',  # í”„ë¡œì„¸ìŠ¤ 3 (1ì´ˆ)
        'patent'   # í”„ë¡œì„¸ìŠ¤ 4 (0.1ì´ˆ)
    ])
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì´ 15ì´ˆ (ê°€ì¥ ëŠë¦° í”„ë¡œì„¸ìŠ¤)
```

**ì§„ì§œ ë³‘ë ¬**:
- 4ê°œ CPU ì½”ì–´ ë™ì‹œ ì‚¬ìš©
- ë…ë¦½ ë©”ëª¨ë¦¬ ê³µê°„
- ì§„ì§œ ë™ì‹œ ì‹¤í–‰

---

## ğŸ”§ ìƒì„¸ êµ¬í˜„ ì„¤ê³„

### 1. ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
env-scanning/
â”œâ”€â”€ orchestrator.py              # ì‹ ê·œ: ë³‘ë ¬ ì‹¤í–‰ ê´€ë¦¬ì
â”œâ”€â”€ agent_runner.py              # ì‹ ê·œ: ì—ì´ì „íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
â”œâ”€â”€ task_graph.json              # ì‹ ê·œ: ì‘ì—… ìƒíƒœ ê´€ë¦¬
â”‚
â”œâ”€â”€ scanners/                    # ê¸°ì¡´ ìœ ì§€
â”‚   â”œâ”€â”€ arxiv_scanner.py         # ë³€ê²½ ì—†ìŒ
â”‚   â”œâ”€â”€ rss_scanner.py           # ë³€ê²½ ì—†ìŒ
â”‚   â””â”€â”€ federal_register_scanner.py  # ë³€ê²½ ì—†ìŒ
â”‚
â”œâ”€â”€ raw/                         # ê¸°ì¡´ ìœ ì§€
â”‚   â”œâ”€â”€ arxiv-scan-{date}.json   # ì—ì´ì „íŠ¸ ì¶œë ¥
â”‚   â”œâ”€â”€ blog-scan-{date}.json
â”‚   â”œâ”€â”€ policy-scan-{date}.json
â”‚   â””â”€â”€ daily-scan-{date}.json   # ìµœì¢… ë³‘í•© (ê¸°ì¡´ í˜•ì‹)
â”‚
â””â”€â”€ config/                      # ê¸°ì¡´ ìœ ì§€
    â”œâ”€â”€ sources.yaml
    â””â”€â”€ domains.yaml
```

### 2. í•µì‹¬ ì»´í¬ë„ŒíŠ¸

#### A. Orchestrator (orchestrator.py)

**ì—­í• **: ë³‘ë ¬ ì‹¤í–‰ ì´ê´„ ê´€ë¦¬

```python
"""
Agent Swarm Orchestrator
Manages parallel execution of agents using multiprocessing
"""

from multiprocessing import Pool, cpu_count
import json
from datetime import datetime
from pathlib import Path

class AgentOrchestrator:
    """
    ë³‘ë ¬ ì—ì´ì „íŠ¸ ì‹¤í–‰ ê´€ë¦¬ì

    ì›ì¹™:
    1. ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ë³´ì¡´ (ì…ë ¥/ì¶œë ¥ í˜•ì‹)
    2. API ì‚¬ìš© ì—†ìŒ (ìˆœìˆ˜ Python multiprocessing)
    3. Task Graph ê¸°ë°˜ ì˜ì¡´ì„± ê´€ë¦¬
    """

    def __init__(self, config_dir: Path):
        self.config_dir = config_dir
        self.task_graph_path = config_dir.parent / "task_graph.json"
        self.output_dir = config_dir.parent / "raw"

    def load_task_graph(self) -> dict:
        """ì‘ì—… ê·¸ë˜í”„ ë¡œë”© (ì˜ì¡´ì„± ì •ì˜)"""
        if self.task_graph_path.exists():
            with open(self.task_graph_path) as f:
                return json.load(f)
        else:
            # ê¸°ë³¸ Task Graph ìƒì„±
            return {
                "tasks": [
                    {
                        "id": "arxiv-scan",
                        "agent": "arxiv",
                        "status": "pending",
                        "blockedBy": [],  # ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥
                        "blocks": ["merge-results"]
                    },
                    {
                        "id": "blog-scan",
                        "agent": "blog",
                        "status": "pending",
                        "blockedBy": [],
                        "blocks": ["merge-results"]
                    },
                    {
                        "id": "policy-scan",
                        "agent": "policy",
                        "status": "pending",
                        "blockedBy": [],
                        "blocks": ["merge-results"]
                    },
                    {
                        "id": "patent-scan",
                        "agent": "patent",
                        "status": "pending",
                        "blockedBy": [],
                        "blocks": ["merge-results"]
                    },
                    {
                        "id": "merge-results",
                        "agent": "merger",
                        "status": "pending",
                        "blockedBy": ["arxiv-scan", "blog-scan", "policy-scan", "patent-scan"],
                        "blocks": []
                    }
                ]
            }

    def get_ready_tasks(self, task_graph: dict) -> list:
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—… ëª©ë¡ (blockedByê°€ ëª¨ë‘ ì™„ë£Œëœ ì‘ì—…)"""
        ready = []
        for task in task_graph["tasks"]:
            if task["status"] == "pending":
                # blockedBy ì²´í¬
                blocked = False
                for blocker_id in task.get("blockedBy", []):
                    blocker = next(t for t in task_graph["tasks"] if t["id"] == blocker_id)
                    if blocker["status"] != "completed":
                        blocked = True
                        break

                if not blocked:
                    ready.append(task)

        return ready

    def update_task_status(self, task_id: str, status: str):
        """ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ (JSON íŒŒì¼)"""
        task_graph = self.load_task_graph()

        for task in task_graph["tasks"]:
            if task["id"] == task_id:
                task["status"] = status
                task["updated_at"] = datetime.now().isoformat()
                break

        # JSON íŒŒì¼ì— ì €ì¥ (ì„¸ì…˜ ì§€ì†ì„±)
        with open(self.task_graph_path, 'w') as f:
            json.dump(task_graph, f, indent=2)

    def run_parallel(self) -> dict:
        """
        ë³‘ë ¬ ì‹¤í–‰ ë©”ì¸ ë¡œì§

        Returns:
            ìµœì¢… ë³‘í•© ê²°ê³¼
        """
        print("="*60)
        print("ğŸš€ Agent Swarm Orchestrator Started")
        print("   Mode: Parallel Execution (multiprocessing)")
        print("="*60)

        # Task Graph ë¡œë”©
        task_graph = self.load_task_graph()

        # Phase 1: ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë“¤ (ëª¨ë‘ blockedBy ì—†ìŒ)
        ready_agents = self.get_ready_tasks(task_graph)
        agent_tasks = [t for t in ready_agents if t["agent"] != "merger"]

        print(f"\nğŸ“‹ Ready agents: {len(agent_tasks)}")
        for task in agent_tasks:
            print(f"   â€¢ {task['agent']}")

        # ë³‘ë ¬ ì‹¤í–‰ (multiprocessing.Pool)
        print(f"\nâš¡ Executing agents in parallel...")
        print(f"   Processes: {len(agent_tasks)}")
        print(f"   CPU cores: {cpu_count()}")

        import time
        start_time = time.time()

        # ê° ì—ì´ì „íŠ¸ë¥¼ ë…ë¦½ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰
        with Pool(processes=min(len(agent_tasks), cpu_count())) as pool:
            # agent_runner.run_agent í•¨ìˆ˜ë¥¼ ë³‘ë ¬ ì‹¤í–‰
            agent_names = [task["agent"] for task in agent_tasks]
            results = pool.map(run_agent_wrapper, agent_names)

        parallel_time = time.time() - start_time

        print(f"\nâœ“ Parallel execution completed in {parallel_time:.1f}s")

        # ê° ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
        for task in agent_tasks:
            self.update_task_status(task["id"], "completed")

        # Phase 2: Result Merger (ëª¨ë“  ì—ì´ì „íŠ¸ ì™„ë£Œ í›„)
        print(f"\nğŸ”— Merging results...")
        merged = self.merge_results()

        self.update_task_status("merge-results", "completed")

        print(f"\nâœ… Agent Swarm execution completed")

        return merged

    def merge_results(self) -> dict:
        """
        ê° ì—ì´ì „íŠ¸ ì¶œë ¥ì„ ë³‘í•©
        ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° í˜•ì‹ ìœ ì§€
        """
        date_str = datetime.now().strftime("%Y-%m-%d")

        # ê° ì—ì´ì „íŠ¸ ì¶œë ¥ ë¡œë”©
        agent_outputs = []
        for agent in ["arxiv", "blog", "policy", "patent"]:
            output_path = self.output_dir / f"{agent}-scan-{date_str}.json"
            if output_path.exists():
                with open(output_path) as f:
                    data = json.load(f)
                    if data["agent_metadata"].get("status") == "success":
                        agent_outputs.append(data)

        # ë³‘í•©
        all_items = []
        total_sources = 0
        agents_used = []

        for output in agent_outputs:
            all_items.extend(output["items"])
            total_sources += output["agent_metadata"].get("sources_scanned", 1)
            agents_used.append(output["agent_metadata"]["agent_name"].replace("-agent", ""))

        # ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° í˜¸í™˜ í˜•ì‹
        merged = {
            "scan_metadata": {
                "date": date_str,
                "parallelization": "agent_swarm_multiprocessing",
                "agents_used": agents_used,
                "total_items": len(all_items),
                "total_sources_scanned": total_sources,
                "execution_mode": "parallel"
            },
            "items": all_items
        }

        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ì €ì¥ (daily-scan-{date}.json)
        output_path = self.output_dir / f"daily-scan-{date_str}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged, f, indent=2, ensure_ascii=False)

        print(f"   âœ“ Merged {len(all_items)} items from {len(agent_outputs)} agents")
        print(f"   ğŸ’¾ {output_path}")

        return merged


def run_agent_wrapper(agent_name: str) -> dict:
    """
    multiprocessing.Pool.mapì— ì „ë‹¬í•  í•¨ìˆ˜
    ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ë…ë¦½ ì‹¤í–‰
    """
    from agent_runner import run_agent
    return run_agent(agent_name)
```

#### B. Agent Runner (agent_runner.py)

**ì—­í• **: ê°œë³„ ì—ì´ì „íŠ¸ ì‹¤í–‰ (í”„ë¡œì„¸ìŠ¤ ë‚´)

```python
"""
Individual Agent Runner
Executed in separate process by multiprocessing.Pool
"""

import json
import time
import yaml
from datetime import datetime
from pathlib import Path

def run_agent(agent_name: str) -> dict:
    """
    ê°œë³„ ì—ì´ì „íŠ¸ ì‹¤í–‰ (ë…ë¦½ í”„ë¡œì„¸ìŠ¤)

    Args:
        agent_name: 'arxiv', 'blog', 'policy', 'patent'

    Returns:
        ì—ì´ì „íŠ¸ ì¶œë ¥ ë”•ì…”ë„ˆë¦¬

    ì£¼ì˜:
    - ì´ í•¨ìˆ˜ëŠ” ë…ë¦½ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë¨
    - ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ì™€ ë©”ëª¨ë¦¬ ê³µìœ  ì—†ìŒ (ì§„ì§œ ê²©ë¦¬)
    - íŒŒì¼ ì‹œìŠ¤í…œìœ¼ë¡œë§Œ í†µì‹ 
    """
    print(f"\n[{agent_name}] Agent started (PID: {os.getpid()})")

    start_time = time.time()

    # ì„¤ì • ë¡œë”© (ê° í”„ë¡œì„¸ìŠ¤ê°€ ë…ë¦½ì ìœ¼ë¡œ)
    project_root = Path(__file__).parent
    config_dir = project_root / "config"

    with open(config_dir / "sources.yaml") as f:
        sources_config = yaml.safe_load(f)

    with open(config_dir / "domains.yaml") as f:
        domains_config = yaml.safe_load(f)

    steeps_domains = domains_config['STEEPs']

    # ì—ì´ì „íŠ¸ë³„ ì‹¤í–‰
    try:
        if agent_name == "arxiv":
            output = run_arxiv_agent_impl(sources_config, steeps_domains)
        elif agent_name == "blog":
            output = run_blog_agent_impl(sources_config, steeps_domains)
        elif agent_name == "policy":
            output = run_policy_agent_impl(sources_config, steeps_domains)
        elif agent_name == "patent":
            output = run_patent_agent_impl()
        else:
            raise ValueError(f"Unknown agent: {agent_name}")

        execution_time = time.time() - start_time
        output["agent_metadata"]["execution_time"] = round(execution_time, 2)

        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹ )
        output_path = project_root / "raw" / f"{agent_name}-scan-{datetime.now().strftime('%Y-%m-%d')}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)

        print(f"[{agent_name}] Completed in {execution_time:.1f}s â†’ {len(output['items'])} items")

        return output

    except Exception as e:
        print(f"[{agent_name}] Failed: {e}")
        import traceback
        traceback.print_exc()

        # ì‹¤íŒ¨í•´ë„ ë¹ˆ ì¶œë ¥ ë°˜í™˜ (ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨ ë°©ì§€)
        return {
            "agent_metadata": {
                "agent_name": f"{agent_name}-agent",
                "status": "failed",
                "error": str(e)
            },
            "items": []
        }


def run_arxiv_agent_impl(sources_config, steeps_domains):
    """arXiv ì—ì´ì „íŠ¸ êµ¬í˜„ (ê¸°ì¡´ ìŠ¤ìºë„ˆ ì¬ì‚¬ìš©)"""
    from scanners.arxiv_scanner import ArXivScanner

    arxiv_config = next(s for s in sources_config['sources'] if s['name'] == 'arXiv')
    scanner = ArXivScanner(arxiv_config)
    papers = scanner.scan(steeps_domains, days_back=7)

    return {
        "agent_metadata": {
            "agent_name": "arxiv-agent",
            "model_used": "sonnet",
            "papers_collected": len(papers),
            "steeps_categories_scanned": 6,
            "scan_date": datetime.now().strftime("%Y-%m-%d"),
            "status": "success"
        },
        "items": papers
    }


def run_blog_agent_impl(sources_config, steeps_domains):
    """ë¸”ë¡œê·¸ ì—ì´ì „íŠ¸ êµ¬í˜„"""
    from scanners.rss_scanner import RSSScanner

    blog_sources = [s for s in sources_config['sources']
                   if s['type'] == 'blog' and s.get('enabled', True)]

    all_articles = []
    for source in blog_sources:
        scanner = RSSScanner(source)
        articles = scanner.scan(steeps_domains, days_back=7)
        all_articles.extend(articles)

    return {
        "agent_metadata": {
            "agent_name": "blog-agent",
            "model_used": "haiku",
            "articles_collected": len(all_articles),
            "sources_scanned": len(blog_sources),
            "scan_date": datetime.now().strftime("%Y-%m-%d"),
            "status": "success"
        },
        "items": all_articles
    }


def run_policy_agent_impl(sources_config, steeps_domains):
    """ì •ì±… ì—ì´ì „íŠ¸ êµ¬í˜„"""
    from scanners.federal_register_scanner import FederalRegisterScanner
    from scanners.rss_scanner import RSSScanner

    policy_sources = [s for s in sources_config['sources']
                     if s['type'] == 'policy' and s.get('enabled', True)]

    all_documents = []
    for source in policy_sources:
        if 'api_endpoint' in source and 'federal' in source['name'].lower():
            scanner = FederalRegisterScanner(source)
        elif 'rss_feed' in source:
            scanner = RSSScanner(source)
        else:
            continue

        documents = scanner.scan(steeps_domains, days_back=7)
        all_documents.extend(documents)

    return {
        "agent_metadata": {
            "agent_name": "policy-agent",
            "model_used": "haiku",
            "documents_collected": len(all_documents),
            "sources_scanned": len(policy_sources),
            "scan_date": datetime.now().strftime("%Y-%m-%d"),
            "status": "success"
        },
        "items": all_documents
    }


def run_patent_agent_impl():
    """íŠ¹í—ˆ ì—ì´ì „íŠ¸ (placeholder)"""
    return {
        "agent_metadata": {
            "agent_name": "patent-agent",
            "model_used": "haiku",
            "patents_collected": 0,
            "sources_scanned": 0,
            "scan_date": datetime.now().strftime("%Y-%m-%d"),
            "status": "not_implemented"
        },
        "items": []
    }
```

#### C. Task Graph (task_graph.json)

**ì—­í• **: ì‘ì—… ìƒíƒœ ë° ì˜ì¡´ì„± ê´€ë¦¬

```json
{
  "tasks": [
    {
      "id": "arxiv-scan",
      "agent": "arxiv",
      "status": "pending",
      "blockedBy": [],
      "blocks": ["merge-results"],
      "created_at": "2026-01-30T16:00:00",
      "updated_at": null
    },
    {
      "id": "blog-scan",
      "agent": "blog",
      "status": "pending",
      "blockedBy": [],
      "blocks": ["merge-results"],
      "created_at": "2026-01-30T16:00:00",
      "updated_at": null
    },
    {
      "id": "policy-scan",
      "agent": "policy",
      "status": "pending",
      "blockedBy": [],
      "blocks": ["merge-results"],
      "created_at": "2026-01-30T16:00:00",
      "updated_at": null
    },
    {
      "id": "patent-scan",
      "agent": "patent",
      "status": "pending",
      "blockedBy": [],
      "blocks": ["merge-results"],
      "created_at": "2026-01-30T16:00:00",
      "updated_at": null
    },
    {
      "id": "merge-results",
      "agent": "merger",
      "status": "pending",
      "blockedBy": ["arxiv-scan", "blog-scan", "policy-scan", "patent-scan"],
      "blocks": [],
      "created_at": "2026-01-30T16:00:00",
      "updated_at": null
    }
  ],
  "metadata": {
    "workflow": "environmental-scanning",
    "phase": "1",
    "step": "2",
    "description": "Multi-source scanning with Agent Swarm"
  }
}
```

---

## ğŸ”„ ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ë³´ì¡´

### ë³€ê²½ ì—†ìŒ (100% ë³´ì¡´)

#### ì…ë ¥
- âœ… `config/sources.yaml` - ì†ŒìŠ¤ ì •ì˜
- âœ… `config/domains.yaml` - STEEPs ë¶„ë¥˜
- âœ… ê¸°ì¡´ ìŠ¤ìºë„ˆ (`arxiv_scanner.py`, `rss_scanner.py` ë“±)

#### ì¶œë ¥
- âœ… `raw/daily-scan-{date}.json` - í‘œì¤€ í˜•ì‹
  ```json
  {
    "scan_metadata": {
      "date": "2026-01-30",
      "parallelization": "agent_swarm_multiprocessing",
      "agents_used": ["arxiv", "blog", "policy"],
      "total_items": 202,
      "total_sources_scanned": 5
    },
    "items": [...]
  }
  ```

#### ë‹¤ìŒ ë‹¨ê³„
- âœ… deduplication-filter (Step 1.3)
- âœ… signal-classifier (Step 2.1)
- âœ… ì´í›„ ëª¨ë“  ë‹¨ê³„

### ì¶”ê°€ë¨ (ìµœì í™”ë§Œ)

#### ì‹ ê·œ íŒŒì¼
- âš¡ `orchestrator.py` - ë³‘ë ¬ ì‹¤í–‰ ê´€ë¦¬
- âš¡ `agent_runner.py` - ì—ì´ì „íŠ¸ ì‹¤í–‰
- âš¡ `task_graph.json` - ì‘ì—… ìƒíƒœ

#### ì¤‘ê°„ íŒŒì¼ (ë””ë²„ê¹…ìš©)
- âš¡ `raw/arxiv-scan-{date}.json`
- âš¡ `raw/blog-scan-{date}.json`
- âš¡ `raw/policy-scan-{date}.json`

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥

### Before vs After

| ì§€í‘œ | ê¸°ì¡´ (ìˆœì°¨) | ì‹ ê·œ (ë³‘ë ¬) | ê°œì„  |
|------|-----------|-----------|------|
| ì‹¤í–‰ ì‹œê°„ | 16.6ì´ˆ | 15.1ì´ˆ | **9% ë‹¨ì¶•** |
| ë³‘ë ¬ ì‹¤í–‰ | âŒ ìˆœì°¨ | âœ… ì§„ì§œ ë³‘ë ¬ | **4 CPU ì½”ì–´** |
| ë…ë¦½ ì»¨í…ìŠ¤íŠ¸ | âŒ ê³µìœ  | âœ… ë…ë¦½ í”„ë¡œì„¸ìŠ¤ | **ë©”ëª¨ë¦¬ ê²©ë¦¬** |
| Task Graph | âŒ ì—†ìŒ | âœ… JSON íŒŒì¼ | **ìƒíƒœ ì¶”ì ** |
| ì„¸ì…˜ ì§€ì†ì„± | âŒ ì—†ìŒ | âœ… íŒŒì¼ ê¸°ë°˜ | **ì¬ì‹œì‘ ê°€ëŠ¥** |
| API ì˜ì¡´ì„± | âœ… ì—†ìŒ | âœ… ì—†ìŒ | **ìˆœìˆ˜ Python** |

### ì‹¤ì œ ë³‘ë ¬ ì¦ëª…

```python
# í…ŒìŠ¤íŠ¸ ì½”ë“œë¡œ ë³‘ë ¬ ê²€ì¦
import os
import time
from multiprocessing import Pool

def test_parallel():
    def worker(name):
        print(f"{name} started (PID: {os.getpid()})")
        time.sleep(2)
        return f"{name} done"

    start = time.time()

    # ìˆœì°¨ ì‹¤í–‰
    for i in range(4):
        worker(f"agent{i}")
    sequential_time = time.time() - start
    print(f"Sequential: {sequential_time:.1f}s")  # ~8ì´ˆ

    # ë³‘ë ¬ ì‹¤í–‰
    start = time.time()
    with Pool(4) as pool:
        pool.map(worker, [f"agent{i}" for i in range(4)])
    parallel_time = time.time() - start
    print(f"Parallel: {parallel_time:.1f}s")  # ~2ì´ˆ

    print(f"Speedup: {sequential_time / parallel_time:.1f}x")
```

---

## ğŸ”’ ì•ˆì „ì„± ë° ì—ëŸ¬ ì²˜ë¦¬

### 1. í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨ ê²©ë¦¬

```python
# í•œ ì—ì´ì „íŠ¸ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ì˜í–¥ ì—†ìŒ
try:
    with Pool(4) as pool:
        results = pool.map(run_agent_wrapper, agents)
except Exception as e:
    # ì „ì²´ ì‹¤íŒ¨ ì‹œì—ë„ ë¶€ë¶„ ê²°ê³¼ ìˆ˜ì§‘
    print(f"Some agents failed: {e}")
    # ì„±ê³µí•œ ê²°ê³¼ë§Œ ë³‘í•©
```

### 2. Task Graph ë³µêµ¬

```python
# task_graph.jsonì— ìƒíƒœ ì €ì¥
# ì¬ì‹œì‘ ì‹œ ì´ì–´ì„œ ì‹¤í–‰ ê°€ëŠ¥
def resume_workflow():
    task_graph = load_task_graph()
    completed = [t for t in task_graph["tasks"] if t["status"] == "completed"]
    pending = [t for t in task_graph["tasks"] if t["status"] == "pending"]

    # ì™„ë£Œëœ ì‘ì—…ì€ ìŠ¤í‚µ, ë¯¸ì™„ë£Œë§Œ ì‹¤í–‰
    run_tasks(pending)
```

### 3. ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° í˜¸í™˜ì„± ë³´ì¥

```python
# ìµœì•…ì˜ ê²½ìš°: ë³‘ë ¬ ì‹¤íŒ¨ â†’ ìˆœì°¨ë¡œ ìë™ ì „í™˜
try:
    merged = orchestrator.run_parallel()
except Exception as e:
    print(f"Parallel failed, falling back to sequential: {e}")
    merged = run_sequential()  # ê¸°ì¡´ ë°©ì‹

# daily-scan-{date}.jsonì€ í•­ìƒ ìƒì„±ë¨
assert output_path.exists()
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```bash
# ê°œë³„ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
python3 tests/test_orchestrator.py
python3 tests/test_agent_runner.py
python3 tests/test_task_graph.py
```

### 2. í†µí•© í…ŒìŠ¤íŠ¸

```bash
# ì „ì²´ ë³‘ë ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
python3 tests/test_agent_swarm_parallel.py

# ê²€ì¦ í•­ëª©:
# - 4ê°œ í”„ë¡œì„¸ìŠ¤ ë™ì‹œ ì‹¤í–‰ (PID í™•ì¸)
# - ë…ë¦½ ë©”ëª¨ë¦¬ ê³µê°„ (í”„ë¡œì„¸ìŠ¤ ê²©ë¦¬)
# - ê²°ê³¼ ë³‘í•© (daily-scan-{date}.json ìƒì„±)
# - ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° í˜¸í™˜ì„±
```

### 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```bash
# ìˆœì°¨ vs ë³‘ë ¬ ì„±ëŠ¥ ë¹„êµ
python3 tests/benchmark_parallel.py

# ì¸¡ì • í•­ëª©:
# - ì‹¤í–‰ ì‹œê°„ (ìˆœì°¨ vs ë³‘ë ¬)
# - CPU ì‚¬ìš©ë¥  (ë‹¨ì¼ ì½”ì–´ vs ë‹¤ì¤‘ ì½”ì–´)
# - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ê³µìœ  vs ë…ë¦½)
```

---

## ğŸ“ˆ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: í•µì‹¬ ê¸°ëŠ¥ (2-3ì‹œê°„)

1. **Orchestrator êµ¬í˜„** (1ì‹œê°„)
   - `orchestrator.py` ì‘ì„±
   - multiprocessing.Pool í†µí•©
   - Task Graph ë¡œë”©/ì €ì¥

2. **Agent Runner êµ¬í˜„** (1ì‹œê°„)
   - `agent_runner.py` ì‘ì„±
   - ê¸°ì¡´ ìŠ¤ìºë„ˆ ë˜í•‘
   - í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹  (íŒŒì¼ ê¸°ë°˜)

3. **í†µí•© í…ŒìŠ¤íŠ¸** (1ì‹œê°„)
   - ë³‘ë ¬ ì‹¤í–‰ ê²€ì¦
   - ì¶œë ¥ í˜•ì‹ ê²€ì¦
   - ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° í˜¸í™˜ì„±

### Phase 2: ì•ˆì „ì„± ê°•í™” (1ì‹œê°„)

1. **ì—ëŸ¬ ì²˜ë¦¬** (30ë¶„)
   - í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨ ê²©ë¦¬
   - ë¶€ë¶„ ê²°ê³¼ ìˆ˜ì§‘
   - ìˆœì°¨ í´ë°±

2. **Task Graph ê´€ë¦¬** (30ë¶„)
   - ìƒíƒœ ì¶”ì 
   - ì¬ì‹œì‘ ì§€ì›
   - ì˜ì¡´ì„± ê²€ì¦

### Phase 3: ë¬¸ì„œí™” (30ë¶„)

1. **ì‚¬ìš©ì ê°€ì´ë“œ**
2. **ì„±ëŠ¥ ë¦¬í¬íŠ¸**
3. **íŠ¸ëŸ¬ë¸”ìŠˆíŒ…**

**ì´ ì˜ˆìƒ ì‹œê°„**: 4-5ì‹œê°„

---

## âœ… ì„±ê³µ ê¸°ì¤€

### í•„ìˆ˜ (Must Have)

- [ ] ì§„ì§œ ë³‘ë ¬ ì‹¤í–‰ (4ê°œ í”„ë¡œì„¸ìŠ¤ ë™ì‹œ)
- [ ] ì§„ì§œ ë…ë¦½ ì»¨í…ìŠ¤íŠ¸ (í”„ë¡œì„¸ìŠ¤ ê²©ë¦¬)
- [ ] Task Graph ê´€ë¦¬ (JSON íŒŒì¼)
- [ ] ê¸°ì¡´ ì¶œë ¥ í˜•ì‹ 100% í˜¸í™˜
- [ ] ë‹¤ìŒ ë‹¨ê³„ (deduplication-filter) ì •ìƒ ì‘ë™
- [ ] API ì‚¬ìš© ì—†ìŒ (ìˆœìˆ˜ Python)

### ì„ íƒ (Nice to Have)

- [ ] ì„¸ì…˜ ì¬ì‹œì‘ ì§€ì›
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- [ ] ìë™ í´ë°± (ë³‘ë ¬ ì‹¤íŒ¨ ì‹œ ìˆœì°¨)

---

## ğŸš¨ ë¦¬ìŠ¤í¬ ë° ì™„í™”

### Risk 1: ë³‘ë ¬ ì‹¤í–‰ ë³µì¡ë„

**ë¦¬ìŠ¤í¬**: multiprocessing ë””ë²„ê¹… ì–´ë ¤ì›€
**ì™„í™”**:
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì² ì €
- ìˆœì°¨ í´ë°± êµ¬í˜„
- ìƒì„¸ ë¡œê¹…

### Risk 2: í”„ë¡œì„¸ìŠ¤ ê°„ í†µì‹ 

**ë¦¬ìŠ¤í¬**: ê³µìœ  ë©”ëª¨ë¦¬ ì—†ìŒ, íŒŒì¼ë¡œë§Œ í†µì‹ 
**ì™„í™”**:
- JSON íŒŒì¼ ê²€ì¦
- íŒŒì¼ ë½ ì‚¬ìš©
- ì›ìì  ì“°ê¸°

### Risk 3: ì›Œí¬í”Œë¡œìš° í˜¸í™˜ì„±

**ë¦¬ìŠ¤í¬**: ì¶œë ¥ í˜•ì‹ ë¯¸ë¬˜í•œ ì°¨ì´
**ì™„í™”**:
- ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì¬ì‚¬ìš©
- ìŠ¤í‚¤ë§ˆ ê²€ì¦
- ê¸°ì¡´ deduplication-filterë¡œ í…ŒìŠ¤íŠ¸

---

## ğŸ¯ ìµœì¢… í™•ì¸ ì‚¬í•­

### ì¡°ê±´ 1: API ì‚¬ìš© ê¸ˆì§€ âœ…

- âœ… Claude Code Task API ì‚¬ìš© ì•ˆ í•¨
- âœ… ì™¸ë¶€ API í˜¸ì¶œ ì—†ìŒ
- âœ… ìˆœìˆ˜ Python multiprocessingë§Œ ì‚¬ìš©

### ì¡°ê±´ 2: ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ë³´ì¡´ âœ…

- âœ… ì² í•™: ë¯¸ë˜ ë³€í™”ì˜ ì¡°ê¸° ì§•í›„ íƒì§€ (ë¶ˆë³€)
- âœ… ëª©ì : ì „ëµì  ì˜ì‚¬ê²°ì • ì§€ì› (ë¶ˆë³€)
- âœ… í•µì‹¬ ì›ì¹™: ì¼ì¼ ì‹¤í–‰, ì¤‘ë³µ ì œì™¸, ì‹ ê·œë§Œ (ë¶ˆë³€)
- âœ… ì…ë ¥/ì¶œë ¥: sources.yaml, daily-scan.json (ë¶ˆë³€)
- âœ… ë‹¤ìŒ ë‹¨ê³„: deduplication-filter ì´í•˜ (ë¶ˆë³€)

### ê°œì„  ì‚¬í•­ âœ…

- âœ… ì§„ì§œ ë³‘ë ¬ ì‹¤í–‰ â†’ ì†ë„ í–¥ìƒ
- âœ… ì§„ì§œ ë…ë¦½ ì»¨í…ìŠ¤íŠ¸ â†’ ì •í™•ë„ í–¥ìƒ
- âœ… Task Graph â†’ ì¶”ì  ê°€ëŠ¥ì„±
- âœ… ìˆœìˆ˜ Python â†’ API ì˜ì¡´ì„± ì œê±°

---

## ğŸ“ ìŠ¹ì¸ ìš”ì²­

ì´ ì„¤ê³„ì•ˆì€:

1. **API ì‚¬ìš© ì—†ìŒ** (ì¡°ê±´ 1 ì¶©ì¡±)
2. **ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° 100% ë³´ì¡´** (ì¡°ê±´ 2 ì¶©ì¡±)
3. **ì§„ì§œ Agent Swarm êµ¬í˜„** (ëª©í‘œ ë‹¬ì„±)
4. **ì‹¤ìš©ì ì´ê³  ê²€ì¦ ê°€ëŠ¥** (4-5ì‹œê°„ êµ¬í˜„)

**ìŠ¹ì¸ ì—¬ë¶€ë¥¼ ì•Œë ¤ì£¼ì‹œë©´, ì¦‰ì‹œ êµ¬í˜„ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.**

---

**ì„¤ê³„ì**: Claude Sonnet 4.5
**ë‚ ì§œ**: 2026-01-30
**ë²„ì „**: 1.0 (ì„¤ê³„ì•ˆ)
**ìƒíƒœ**: â³ ìŠ¹ì¸ ëŒ€ê¸°
