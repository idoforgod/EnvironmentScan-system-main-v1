"""
Source Explorer — Stage C Core Logic
=====================================
Discovers new sources via gap analysis + random exploration.
Includes ExplorationLearningLoop for SIE-independent recursive learning.

Reuses (unmodified):
  - env-scanning/core/source_health_checker.py
  - env-scanning/scanners/rss_scanner.py

Version: 1.0.0
"""

import json
import os
import random
import shutil
from collections import Counter
from datetime import datetime, timezone
from pathlib import Path
from typing import Any


class SourceExplorer:
    """Core exploration logic — used by evaluator agent and single-agent fallback."""

    def __init__(self, config: dict, data_root: str):
        """
        Args:
            config: SOT source_exploration section.
            data_root: WF1 data_root absolute path.
        """
        self.config = config
        self.data_root = Path(data_root)
        self.max_candidates = config.get("max_candidates_per_scan", 5)
        self.max_test_signals = config.get("max_test_signals_per_candidate", 10)
        self.coverage_gap_threshold = config.get("coverage_gap_threshold", 0.15)
        self.min_signals_for_viable = config.get("min_signals_for_viable", 2)
        self.candidate_retention_days = config.get("candidate_retention_days", 30)

    # ------------------------------------------------------------------
    # Gap Analysis
    # ------------------------------------------------------------------

    def analyze_coverage_gaps(
        self, classified_signals: list[dict], domains: dict
    ) -> dict:
        """Analyze STEEPs distribution and identify coverage gaps.

        Args:
            classified_signals: List of classified signal dicts with 'category' field.
            domains: STEEPs domain definitions from domains.yaml.

        Returns:
            {
                "category_distribution": {cat: pct, ...},
                "gaps": [cat, ...],
                "gap_keywords": {cat: [keywords], ...},
                "total_signals": int,
            }
        """
        total = len(classified_signals)
        if total == 0:
            # All categories are gaps
            all_cats = list(domains.keys())
            return {
                "category_distribution": {},
                "gaps": all_cats,
                "gap_keywords": {
                    cat: domains.get(cat, []) for cat in all_cats
                },
                "total_signals": 0,
            }

        # Count categories
        counter: Counter = Counter()
        for sig in classified_signals:
            cat = sig.get("category") or sig.get("preliminary_category", "")
            if cat:
                counter[cat] += 1

        distribution = {cat: count / total for cat, count in counter.items()}

        # Identify gaps: categories below threshold OR entirely missing
        all_expected = set(domains.keys())
        gaps = []
        for cat in all_expected:
            pct = distribution.get(cat, 0.0)
            if pct < self.coverage_gap_threshold:
                gaps.append(cat)

        # Build keywords for gap categories
        gap_keywords = {}
        for cat in gaps:
            kws = domains.get(cat, [])
            gap_keywords[cat] = kws if isinstance(kws, list) else []

        return {
            "category_distribution": distribution,
            "gaps": gaps,
            "gap_keywords": gap_keywords,
            "total_signals": total,
        }

    # ------------------------------------------------------------------
    # Exclusion Management
    # ------------------------------------------------------------------

    def load_excluded_sources(self) -> list[str]:
        """Load excluded-sources.json generated by validate_registry.py (SOT-037).

        Returns:
            List of source names that exploration must NOT discover.
        """
        path = self.data_root / "exploration" / "excluded-sources.json"
        if not path.exists():
            return []
        try:
            with open(path, encoding="utf-8") as f:
                data = json.load(f)
            return data.get("excluded_sources", [])
        except (json.JSONDecodeError, KeyError):
            return []

    def filter_against_exclusions(
        self, candidates: list[dict], excluded: list[str]
    ) -> list[dict]:
        """Remove candidates whose names match the exclusion list.

        Uses case-insensitive comparison for robustness.
        """
        excluded_lower = {name.lower() for name in excluded}
        return [
            c for c in candidates
            if c.get("name", "").lower() not in excluded_lower
        ]

    # ------------------------------------------------------------------
    # Health Check
    # ------------------------------------------------------------------

    def health_check_candidates(self, candidates: list[dict]) -> list[dict]:
        """Run SourceHealthChecker on candidate sources.

        Returns candidates with 'health' field set.
        """
        try:
            from core.source_health_checker import SourceHealthChecker
        except ImportError:
            # Fallback: skip health check, mark all as unknown
            for c in candidates:
                c["health"] = "unknown"
                c["health_reason"] = "health_checker_unavailable"
            return candidates

        # Convert candidates to the format SourceHealthChecker expects
        sources_for_check = []
        for c in candidates:
            sources_for_check.append({
                "name": c.get("name", "Unknown"),
                "enabled": True,
                "rss_feed": c.get("url", ""),
                "type": c.get("type", "blog"),
            })

        health_dir = str(self.data_root / "exploration" / "health")
        checker = SourceHealthChecker(sources_for_check, health_dir)
        report = checker.check_all_sources()

        # Merge health results back into candidates
        for c in candidates:
            name = c.get("name", "")
            src_health = report.get("sources", {}).get(name, {})
            c["health"] = src_health.get("health", "unknown")
            c["health_reason"] = src_health.get("reason", "unchecked")

        return candidates

    # ------------------------------------------------------------------
    # Test Scanning
    # ------------------------------------------------------------------

    def test_scan_candidates(
        self,
        candidates: list[dict],
        domains: dict,
        scan_window_start: datetime | None = None,
        scan_window_end: datetime | None = None,
        date: str = "",
    ) -> dict:
        """Test-scan candidate sources using RSSScanner.

        Args:
            candidates: List of candidate dicts with 'url' field.
            domains: STEEPs domain keywords.
            scan_window_start: Temporal window start.
            scan_window_end: Temporal window end.
            date: Date string (YYYY-MM-DD) for signal ID generation.

        Returns:
            {
                "viable": [...],
                "non_viable": [...],
                "signals": [...],  # All collected exploration signals
                "scan_results": {name: {...}, ...},
            }
        """
        try:
            from scanners.rss_scanner import RSSScanner
        except ImportError:
            return {
                "viable": [],
                "non_viable": candidates,
                "signals": [],
                "scan_results": {},
                "error": "RSSScanner unavailable",
            }

        viable = []
        non_viable = []
        all_signals: list[dict] = []
        scan_results: dict[str, Any] = {}

        for candidate in candidates:
            name = candidate.get("name", "Unknown")
            url = candidate.get("url", "")

            if not url:
                candidate["scan_status"] = "no_url"
                non_viable.append(candidate)
                continue

            # Skip unhealthy candidates
            if candidate.get("health") == "unhealthy":
                candidate["scan_status"] = "unhealthy"
                non_viable.append(candidate)
                continue

            try:
                scanner_config = {
                    "name": name,
                    "type": candidate.get("type", "blog"),
                    "rss_feed": url,
                    "timeout": 15,
                    "max_results": self.max_test_signals,
                    "enabled": True,
                }
                scanner = RSSScanner(scanner_config)
                signals = scanner.scan(
                    steeps_domains=domains,
                    scan_window_start=scan_window_start,
                    scan_window_end=scan_window_end,
                )

                scan_results[name] = {
                    "signal_count": len(signals),
                    "url": url,
                    "status": "scanned",
                }

                if len(signals) >= self.min_signals_for_viable:
                    candidate["scan_status"] = "viable"
                    candidate["signal_count"] = len(signals)
                    viable.append(candidate)
                    # Tag signals with exploration tier and rewrite IDs
                    date_str = date or datetime.now(timezone.utc).strftime("%Y%m%d")
                    date_compact = date_str.replace("-", "")
                    # Sanitize source name for ID (lowercase, hyphens only)
                    safe_name = name.lower().replace(" ", "-")
                    for idx, sig in enumerate(signals, start=1):
                        sig["original_scanner_id"] = sig.get("id", "")
                        sig["id"] = f"explore-{date_compact}-{safe_name}-{idx:03d}"
                        if "source" not in sig:
                            sig["source"] = {}
                        sig["source"]["tier"] = "exploration"
                        sig["exploration_source"] = name
                    all_signals.extend(signals)
                else:
                    candidate["scan_status"] = "insufficient_signals"
                    candidate["signal_count"] = len(signals)
                    non_viable.append(candidate)

            except Exception as e:
                candidate["scan_status"] = "scan_error"
                candidate["scan_error"] = str(e)[:200]
                non_viable.append(candidate)
                scan_results[name] = {"signal_count": 0, "error": str(e)[:200]}

        return {
            "viable": viable,
            "non_viable": non_viable,
            "signals": all_signals,
            "scan_results": scan_results,
        }

    # ------------------------------------------------------------------
    # Scoring
    # ------------------------------------------------------------------

    def score_candidates(
        self, scan_results: dict, classified_signals: list[dict]
    ) -> list[dict]:
        """Score viable candidates based on quality metrics.

        Returns:
            Sorted list of candidates with 'quality_score' field.
        """
        viable = scan_results.get("viable", [])
        existing_titles = {
            sig.get("title", "").lower()
            for sig in classified_signals
            if sig.get("title")
        }

        for candidate in viable:
            name = candidate.get("name", "")
            result = scan_results.get("scan_results", {}).get(name, {})

            # signal_yield: 0-1 normalized by max_test_signals
            yield_count = result.get("signal_count", 0)
            signal_yield = min(yield_count / max(self.max_test_signals, 1), 1.0)

            # uniqueness: fraction of signals not matching existing titles
            candidate_signals = [
                s for s in scan_results.get("signals", [])
                if s.get("exploration_source") == name
            ]
            if candidate_signals:
                unique = sum(
                    1 for s in candidate_signals
                    if s.get("title", "").lower() not in existing_titles
                )
                uniqueness = unique / len(candidate_signals)
            else:
                uniqueness = 0.0

            # reliability: health-based
            health = candidate.get("health", "unknown")
            reliability = {"healthy": 1.0, "suspect": 0.5, "unknown": 0.3}.get(
                health, 0.0
            )

            # Combined score (equal weights)
            score = (signal_yield * 0.3 + uniqueness * 0.4 + reliability * 0.3)
            candidate["quality_score"] = round(score, 3)
            candidate["score_breakdown"] = {
                "signal_yield": round(signal_yield, 3),
                "uniqueness": round(uniqueness, 3),
                "reliability": round(reliability, 3),
            }

        # Sort by score descending
        viable.sort(key=lambda c: c.get("quality_score", 0), reverse=True)
        return viable

    # ------------------------------------------------------------------
    # Persistence
    # ------------------------------------------------------------------

    def save_candidates(self, results: dict, date: str) -> str:
        """Save exploration candidates to JSON file.

        Returns:
            Path to the saved file.
        """
        candidates_dir = self.data_root / "exploration" / "candidates"
        candidates_dir.mkdir(parents=True, exist_ok=True)
        output_path = candidates_dir / f"exploration-candidates-{date}.json"

        output = {
            "date": date,
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "viable_count": len(results.get("viable", [])),
            "non_viable_count": len(results.get("non_viable", [])),
            "total_exploration_signals": len(results.get("signals", [])),
            "viable_candidates": results.get("viable", []),
            "non_viable_candidates": results.get("non_viable", []),
            "scan_results": results.get("scan_results", {}),
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output, f, indent=2, ensure_ascii=False, default=str)

        return str(output_path)

    # ------------------------------------------------------------------
    # User Decision Application
    # ------------------------------------------------------------------

    def apply_user_decisions(
        self, decisions: list[dict], sources_yaml_path: str
    ) -> dict:
        """Apply user decisions on exploration candidates.

        Args:
            decisions: List of dicts, each with:
                - name: source name
                - decision: "approved" | "discarded" | "deferred"
                - url: source URL (needed for approved sources)
                - type: source type (needed for approved sources)
                - target_steeps: list of STEEPs categories (optional)
            sources_yaml_path: Path to sources.yaml to add approved sources.

        Returns:
            Summary of actions taken.
        """
        try:
            import yaml
        except ImportError:
            return {"error": "PyYAML not installed"}

        history = ExplorationHistory(str(self.data_root / "exploration" / "history"))
        history_data = history.load()

        approved = []
        discarded = []
        deferred = []

        for item in decisions:
            source_name = item.get("name", "")
            decision = item.get("decision", "deferred")
            entry = {
                "name": source_name,
                "decision": decision,
                "decided_at": datetime.now(timezone.utc).isoformat(),
                "url": item.get("url", ""),
                "type": item.get("type", "blog"),
            }

            if decision == "approved":
                approved.append(source_name)
                history_data.setdefault("approved", []).append(entry)
                # Actually add to sources.yaml
                self._add_source_to_yaml(
                    sources_yaml_path,
                    source_name=source_name,
                    url=item.get("url", ""),
                    source_type=item.get("type", "blog"),
                    target_steeps=item.get("target_steeps", []),
                )
            elif decision == "discarded":
                discarded.append(source_name)
                history_data.setdefault("discarded", []).append(entry)
            else:  # deferred
                deferred.append(source_name)
                history_data.setdefault("deferred", []).append(entry)

        history.save(history_data)

        return {
            "approved": approved,
            "discarded": discarded,
            "deferred": deferred,
        }

    def _add_source_to_yaml(
        self,
        sources_yaml_path: str,
        source_name: str,
        url: str,
        source_type: str = "blog",
        target_steeps: list[str] | None = None,
    ) -> None:
        """Add an approved exploration source to sources.yaml.

        Uses atomic write pattern: backup → modify → write → verify.
        """
        try:
            import yaml
        except ImportError:
            return

        yaml_path = Path(sources_yaml_path)
        if not yaml_path.exists():
            return

        # 1. Backup
        backup_path = yaml_path.with_suffix(".yaml.bak")
        shutil.copy2(yaml_path, backup_path)

        try:
            # 2. Read current sources
            with open(yaml_path, encoding="utf-8") as f:
                data = yaml.safe_load(f)

            if not data or "sources" not in data:
                return

            # 3. Check for duplicate name
            existing_names = {
                s.get("name", "").lower() for s in data["sources"]
            }
            if source_name.lower() in existing_names:
                return  # Already exists

            # 4. Build new source entry matching existing structure
            new_source = {
                "name": source_name,
                "tier": "exploration",
                "type": source_type,
                "enabled": True,
                "rss_feed": url,
                "timeout": 15,
                "critical": False,
                "max_results": 20,
                "description": f"Discovered by Stage C exploration",
                "added_date": datetime.now(timezone.utc).strftime("%Y-%m-%d"),
                "health_status": None,
                "last_health_check": None,
                "resolved_url": None,
                "fetch_strategy": None,
                "consecutive_failures": 0,
            }
            if target_steeps:
                new_source["target_steeps"] = target_steeps

            # 5. Append
            data["sources"].append(new_source)

            # 6. Write (preserve comments by re-dumping — comment loss is
            #    acceptable for programmatic additions)
            tmp_path = yaml_path.with_suffix(".yaml.tmp")
            with open(tmp_path, "w", encoding="utf-8") as f:
                yaml.dump(data, f, default_flow_style=False,
                          allow_unicode=True, sort_keys=False)

            # 7. Verify
            with open(tmp_path, encoding="utf-8") as f:
                verify = yaml.safe_load(f)
            if source_name not in [s.get("name") for s in verify.get("sources", [])]:
                raise ValueError("Verification failed: source not found after write")

            # 8. Atomic replace
            os.replace(tmp_path, yaml_path)

        except Exception:
            # Restore from backup on any failure
            if backup_path.exists():
                shutil.copy2(backup_path, yaml_path)
            tmp_path = yaml_path.with_suffix(".yaml.tmp")
            if tmp_path.exists():
                tmp_path.unlink()
            raise


class ExplorationHistory:
    """Persistent exploration history — DB atomicity pattern applied."""

    def __init__(self, history_dir: str):
        self.history_dir = Path(history_dir)
        self.history_dir.mkdir(parents=True, exist_ok=True)
        self.history_file = self.history_dir / "exploration-history.json"
        self.backup_file = self.history_dir / "exploration-history.json.bak"

    def load(self) -> dict:
        """Load history, creating empty if not exists."""
        if not self.history_file.exists():
            return {
                "version": "1.0.0",
                "created_at": datetime.now(timezone.utc).isoformat(),
                "scans": [],
                "approved": [],
                "discarded": [],
                "deferred": [],
                "learning": {},
            }
        try:
            with open(self.history_file, encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, OSError):
            return {
                "version": "1.0.0",
                "created_at": datetime.now(timezone.utc).isoformat(),
                "scans": [],
                "approved": [],
                "discarded": [],
                "deferred": [],
                "learning": {},
                "recovery_note": "Previous history file was corrupted",
            }

    def save(self, data: dict) -> None:
        """Atomic write: snapshot -> write -> verify -> (restore on failure)."""
        # 1. Create snapshot
        if self.history_file.exists():
            shutil.copy2(self.history_file, self.backup_file)

        # 2. Write
        tmp_path = self.history_file.with_suffix(".tmp")
        try:
            with open(tmp_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False, default=str)

            # 3. Verify written file
            with open(tmp_path, encoding="utf-8") as f:
                json.load(f)  # Validates JSON integrity

            # 4. Atomic rename
            os.replace(tmp_path, self.history_file)

        except Exception:
            # Restore from snapshot
            if self.backup_file.exists():
                shutil.copy2(self.backup_file, self.history_file)
            if tmp_path.exists():
                tmp_path.unlink()
            raise

    def get_discarded_sources(self) -> list[str]:
        """Names of previously discarded sources."""
        data = self.load()
        return [e["name"] for e in data.get("discarded", []) if "name" in e]

    def get_approved_sources(self) -> list[str]:
        """Names of approved sources."""
        data = self.load()
        return [e["name"] for e in data.get("approved", []) if "name" in e]

    def get_deferred_sources(self) -> list[str]:
        """Names of deferred (pending retry) sources."""
        data = self.load()
        return [e["name"] for e in data.get("deferred", []) if "name" in e]

    def cleanup_expired(self, retention_days: int) -> int:
        """Remove deferred candidates older than retention_days.

        Returns:
            Number of entries removed.
        """
        data = self.load()
        now = datetime.now(timezone.utc)
        deferred = data.get("deferred", [])

        kept = []
        removed = 0
        for entry in deferred:
            decided_str = entry.get("decided_at", "")
            try:
                decided = datetime.fromisoformat(decided_str)
                if (now - decided).days <= retention_days:
                    kept.append(entry)
                else:
                    removed += 1
            except (ValueError, TypeError):
                kept.append(entry)  # Keep entries with unparseable dates

        if removed > 0:
            data["deferred"] = kept
            self.save(data)

        return removed


class ExplorationLearningLoop:
    """Exploration-specific mini-RLM (SIE-independent)."""

    def __init__(self, history: ExplorationHistory):
        self.history = history

    def analyze_history(self) -> dict:
        """Analyze past exploration patterns.

        Returns:
            Analysis dict with success/failure patterns, approval rates, etc.
        """
        data = self.history.load()
        scans = data.get("scans", [])
        approved = data.get("approved", [])
        discarded = data.get("discarded", [])

        if not scans:
            return {
                "total_scans": 0,
                "message": "No exploration history yet",
                "query_success_patterns": [],
                "query_failure_patterns": [],
                "source_type_approval_rate": {},
                "steeps_discovery_difficulty": {},
                "method_effectiveness": {},
            }

        # Analyze query patterns
        successful_queries = []
        failed_queries = []
        method_stats = {"gap_directed": {"total": 0, "viable": 0},
                        "random": {"total": 0, "viable": 0}}

        for scan in scans:
            for candidate in scan.get("candidates", []):
                query = candidate.get("discovery_query", "")
                method = candidate.get("discovery_method", "unknown")
                viable = candidate.get("scan_status") == "viable"

                if method in method_stats:
                    method_stats[method]["total"] += 1
                    if viable:
                        method_stats[method]["viable"] += 1

                if viable:
                    successful_queries.append(query)
                else:
                    failed_queries.append(query)

        # Approval rate by source type
        approved_names = {e["name"] for e in approved if "name" in e}
        discarded_names = {e["name"] for e in discarded if "name" in e}

        return {
            "total_scans": len(scans),
            "total_approved": len(approved),
            "total_discarded": len(discarded),
            "query_success_patterns": successful_queries[-20:],
            "query_failure_patterns": failed_queries[-20:],
            "method_effectiveness": {
                method: {
                    "total": stats["total"],
                    "viable": stats["viable"],
                    "rate": (stats["viable"] / stats["total"])
                    if stats["total"] > 0 else 0.0,
                }
                for method, stats in method_stats.items()
            },
        }

    def generate_strategy_hints(self, analysis: dict) -> dict:
        """Generate strategy hints for the next exploration run.

        Returns:
            {
                "alpha_hints": {...},  # Gap-directed strategy adjustments
                "beta_hints": {...},   # Random exploration adjustments
                "avoid_patterns": [...],  # Patterns to avoid
            }
        """
        hints: dict[str, Any] = {
            "alpha_hints": {},
            "beta_hints": {},
            "avoid_patterns": [],
        }

        method_eff = analysis.get("method_effectiveness", {})

        # Alpha hints: prioritize gap categories with higher discovery success
        gap_rate = method_eff.get("gap_directed", {}).get("rate", 0.0)
        random_rate = method_eff.get("random", {}).get("rate", 0.0)

        if gap_rate > random_rate:
            hints["alpha_hints"]["priority"] = "high"
            hints["alpha_hints"]["note"] = "Gap-directed has higher success rate"
        else:
            hints["alpha_hints"]["priority"] = "normal"

        # Beta hints: use success patterns to boost keyword weights
        success_patterns = analysis.get("query_success_patterns", [])
        if success_patterns:
            hints["beta_hints"]["boost_similar_to"] = success_patterns[-5:]

        # Avoid patterns from repeated failures
        failure_patterns = analysis.get("query_failure_patterns", [])
        if len(failure_patterns) > 3:
            hints["avoid_patterns"] = failure_patterns[-5:]

        return hints

    def update_frontiers_weights(
        self, frontiers_path: str, results: dict
    ) -> bool:
        """Update exploration-frontiers.yaml keyword weights (MINOR, within bounds).

        Args:
            frontiers_path: Path to exploration-frontiers.yaml.
            results: Current scan results for learning feedback.

        Returns:
            True if weights were updated.
        """
        # This is a MINOR SIE change — only adjusts keyword selection weights
        # The actual keyword pool modification requires user approval (MAJOR)
        # For now, learning is recorded in history; weight updates are logged
        # but the YAML file itself is not auto-modified (safety-first approach)

        data = self.history.load()
        learning = data.get("learning", {})
        learning["last_update"] = datetime.now(timezone.utc).isoformat()
        learning["scans_analyzed"] = learning.get("scans_analyzed", 0) + 1

        # Record which keywords led to viable sources
        for candidate in results.get("viable", []):
            query = candidate.get("discovery_query", "")
            if query:
                keyword_successes = learning.get("keyword_successes", {})
                keyword_successes[query] = keyword_successes.get(query, 0) + 1
                learning["keyword_successes"] = keyword_successes

        data["learning"] = learning
        self.history.save(data)

        return True

    def record_scan(self, scan_summary: dict) -> None:
        """Record a completed exploration scan in history."""
        data = self.history.load()
        data.setdefault("scans", []).append({
            "date": scan_summary.get("date", ""),
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "method_used": scan_summary.get("method_used", "unknown"),
            "candidates_discovered": scan_summary.get("candidates_discovered", 0),
            "viable_count": scan_summary.get("viable_count", 0),
            "signals_collected": scan_summary.get("signals_collected", 0),
            "gaps_analyzed": scan_summary.get("gaps_analyzed", []),
            "candidates": scan_summary.get("candidates", []),
        })

        # Keep only last 30 scans
        if len(data["scans"]) > 30:
            data["scans"] = data["scans"][-30:]

        self.history.save(data)
