{
  "agent_metadata": {
    "agent_name": "arxiv-agent",
    "model_used": "sonnet",
    "papers_collected": 120,
    "steeps_categories_scanned": 6,
    "scan_date": "2026-01-30",
    "status": "success",
    "execution_time": 15.57,
    "process_id": 48568
  },
  "items": [
    {
      "id": "arxiv-2601.22159v1",
      "title": "RedSage: A Cybersecurity Generalist LLM",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22159v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
        "keywords": [
          "cs.CR",
          "cs.CL",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22159v1",
        "authors": [
          "Naufal Suryanto",
          "Muzammal Naseer",
          "Pengfei Li"
        ],
        "arxiv_categories": [
          "cs.CR",
          "cs.CL",
          "cs.AI"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.430922",
      "entities": [
        "Cybersecurity Generalist",
        "Framework",
        "SECURE",
        "LLM",
        "CTI",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22158v1",
      "title": "One-step Latent-free Image Generation with Pixel Mean Flows",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22158v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22158v1",
        "authors": [
          "Yiyang Lu",
          "Susie Lu",
          "Qiao Sun"
        ],
        "arxiv_categories": [
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.431069",
      "entities": [
        "Pixel Mean Flows Modern",
        "Image Generation",
        "Guideline",
        "Fusion",
        "Act",
        "FID",
        "NSF",
        "EPA",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22157v1",
      "title": "Discovering Hidden Gems in Model Repositories",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22157v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.",
        "keywords": [
          "cs.CL",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22157v1",
        "authors": [
          "Jonathan Kahana",
          "Eliahu Horwitz",
          "Yedid Hoshen"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.431200",
      "entities": [
        "Model Repositories Public",
        "Discovering Hidden Gems",
        "Sequential Halving",
        "Armed Bandit",
        "Llama-3.1",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22156v1",
      "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22156v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22156v1",
        "authors": [
          "Yingfa Chen",
          "Zhen Leng Thai",
          "Zihan Zhou"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.431369",
      "entities": [
        "Extremely Long Contexts Hybrid",
        "Hybrid Linear Attention Done",
        "Effective Architectures",
        "Efficient Distillation",
        "Layer Optimization",
        "Hybrid Attention",
        "Neural Network",
        "Transformer",
        "HALO",
        "RNN",
        "NSF",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22155v1",
      "title": "UEval: A Benchmark for Unified Multimodal Generation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22155v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.",
        "keywords": [
          "cs.CL",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22155v1",
        "authors": [
          "Bo Li",
          "Yida Yin",
          "Wenhao Chai"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.431538",
      "entities": [
        "Unified Multimodal Generation We",
        "Large Language Models",
        "GPT-5",
        "MLLM",
        "LLM",
        "GPT",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22154v1",
      "title": "Exploring Reasoning Reward Model for Agents",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22154v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.",
        "keywords": [
          "cs.CL",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22154v1",
        "authors": [
          "Kaixuan Fan",
          "Kaituo Feng",
          "Manyuan Zhang"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.431686",
      "entities": [
        "Agents Agentic Reinforcement Learning",
        "Exploring Reasoning Reward Model",
        "Agent Reasoning Reward Model",
        "GAIA",
        "RRM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22153v1",
      "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22153v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.",
        "keywords": [
          "cs.RO",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22153v1",
        "authors": [
          "Haozhe Xie",
          "Beichen Wen",
          "Jiarui Zheng"
        ],
        "arxiv_categories": [
          "cs.RO",
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.431852",
      "entities": [
        "Dynamic Object Manipulation Manipulating",
        "Dynamic Object Manipulation",
        "Continuous Inference",
        "Action Streaming",
        "Action Model",
        "Framework",
        "Act",
        "VLA",
        "DOM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22150v1",
      "title": "Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22150v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Large Vision-Language Models (VLMs) often answer classic visual illusions \"correctly\" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22150v1",
        "authors": [
          "Xiaoxiao Sun",
          "Mingyang Li",
          "Kun yuan"
        ],
        "arxiv_categories": [
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.432026",
      "entities": [
        "Classic Visual Illusions Large",
        "Probing Visual Perception",
        "Template Fixation Index",
        "Flip Consistency",
        "Language Models",
        "Framework",
        "Opus-4.1",
        "Google",
        "GPT-5",
        "Act",
        "MIT",
        "GPT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22149v1",
      "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22149v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.",
        "keywords": [
          "cs.CL",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22149v1",
        "authors": [
          "Hang Ding",
          "Peidong Liu",
          "Junqiao Wang"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.432201",
      "entities": [
        "Based Reinforcement Learning",
        "Large Language Models",
        "Framework",
        "Policy",
        "MBRL",
        "Act",
        "LLM",
        "AI"
      ]
    },
    {
      "id": "arxiv-2601.22146v1",
      "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22146v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .",
        "keywords": [
          "cs.CL",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22146v1",
        "authors": [
          "Ajay Patel",
          "Colin Raffel",
          "Chris Callison-Burch"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.432375",
      "entities": [
        "Scaling Synthetic Instructions",
        "Training Scale Due",
        "Standard",
        "Bill",
        "MIT",
        "LLM",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22143v1",
      "title": "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22143v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.",
        "keywords": [
          "cs.GR",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22143v1",
        "authors": [
          "Anthony Chen",
          "Naomi Ken Korem",
          "Tavi Halperin"
        ],
        "arxiv_categories": [
          "cs.GR",
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.432540",
      "entities": [
        "Visual Foundation Models",
        "Visual Diffusion Audio",
        "Video Dubbing",
        "Joint Audio",
        "Fusion",
        "JUST",
        "DUB",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22141v1",
      "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22141v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.",
        "keywords": [
          "cs.AI",
          "cs.LG",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22141v1",
        "authors": [
          "Grzegorz Stefanski",
          "Alberto Presta",
          "Michal Byra"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.LG",
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.432685",
      "entities": [
        "Lottery Ticket Hypothesis",
        "Heterogeneous Data In",
        "Adaptive Subnetworks",
        "Deep Learning",
        "Framework",
        "RTL",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22140v1",
      "title": "Quantum fluctuations in hydrodynamics and quantum long-time tails",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22140v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "We construct a quantum Schwinger-Keldysh (SK) effective field theory for the diffusive hydrodynamics of a conserved scalar field. Quantum corrections within the SK framework are guided by fluctuation-dissipation relations, enforced via a dynamical Kubo-Martin-Schwinger (KMS) symmetry. We find that the KMS symmetry necessarily generates fluctuation contributions in the SK effective action at all orders in the noise field, thereby giving rise to intrinsically non-Gaussian noise. We use our results to compute one-loop quantum corrections to the two-point density-density retarded correlation function, leading to a quantum generalization of hydrodynamic long-time tails. Our results apply at arbitrarily high orders in $\\hbar$. The one-loop results for retarded correlation functions have been expressed in terms of a family of polynomials. We also provide a closed-form expression for the one-loop results at leading order in the wavevector expansion.",
        "keywords": [
          "math-ph",
          "cond-mat.stat-mech",
          "quant-ph",
          "hep-ph",
          "hep-th"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22140v1",
        "authors": [
          "Akash Jain"
        ],
        "arxiv_categories": [
          "math-ph",
          "cond-mat.stat-mech",
          "quant-ph",
          "hep-ph",
          "hep-th"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.432803",
      "entities": [
        "Framework",
        "Act",
        "KMS",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22139v1",
      "title": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22139v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\\% higher accuracy, 22.90\\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}",
        "keywords": [
          "cs.CL",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22139v1",
        "authors": [
          "Xin Chen",
          "Feng Jiang",
          "Yiqian Zhang"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.432988",
      "entities": [
        "Transforming Reasoning Large Language",
        "Proactive Interactive Reasoning",
        "Proactive Inquirers Reasoning",
        "Reasoning While Asking",
        "Large Language Models",
        "Passive Solvers",
        "Framework",
        "Policy",
        "AIRI",
        "BLEU",
        "SUAT",
        "NSF",
        "MIT",
        "LLM",
        "PIR"
      ]
    },
    {
      "id": "arxiv-2601.22137v1",
      "title": "PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22137v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.",
        "keywords": [
          "math.NA",
          "cs.AI",
          "cs.LG",
          "math.OC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22137v1",
        "authors": [
          "Shenghao Yang",
          "Zhichao Wang",
          "Oleg Balabanov"
        ],
        "arxiv_categories": [
          "math.NA",
          "cs.AI",
          "cs.LG",
          "math.OC"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.433133",
      "entities": [
        "Accelerating Neural Network Training",
        "Randomized Iterative Sketching",
        "Adaptive Computation",
        "Machine Learning",
        "Matrix Functions",
        "Neural Network",
        "Framework",
        "PRISM",
        "MIT",
        "GPU",
        "WTO",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22136v1",
      "title": "StepShield: When, Not Whether to Intervene on Rogue Agents",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22136v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.",
        "keywords": [
          "cs.CR",
          "cs.SE",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22136v1",
        "authors": [
          "Gloria Felicia",
          "Michael Eniolade",
          "Jinfeng He"
        ],
        "arxiv_categories": [
          "cs.CR",
          "cs.SE",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.433299",
      "entities": [
        "Early Intervention Rate",
        "Rogue Agents Existing",
        "Intervention Gap",
        "Tokens Saved",
        "Not Whether",
        "Standard",
        "LLM",
        "EIR",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22135v1",
      "title": "PI-Light: Physics-Inspired Diffusion for Full-Image Relighting",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22135v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($π$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $π$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22135v1",
        "authors": [
          "Zhexin Liang",
          "Zhaoxi Chen",
          "Yongwei Chen"
        ],
        "arxiv_categories": [
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.433696",
      "entities": [
        "Image Relighting Full",
        "Inspired Diffusion",
        "Framework",
        "Fusion",
        "MIT",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22134v1",
      "title": "Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22134v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P < 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22134v1",
        "authors": [
          "Wenxuan Li",
          "Pedro R. A. S. Bassi",
          "Lizhou Wu"
        ],
        "arxiv_categories": [
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.433890",
      "entities": [
        "Computed Tomography Pancreatic",
        "Prediagnostic Detection",
        "Artificial Intelligence",
        "Pancreatic Cancer",
        "Intel",
        "PDAC",
        "Act",
        "EPA",
        "AUC",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22130v1",
      "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22130v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.",
        "keywords": [
          "cs.SE",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22130v1",
        "authors": [
          "Lakshya Gupta",
          "Litao Li",
          "Yizhe Liu"
        ],
        "arxiv_categories": [
          "cs.SE",
          "cs.AI"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.434050",
      "entities": [
        "Enterprise Systems Frontier",
        "Bringing World Models",
        "Act",
        "MIT",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22129v1",
      "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22129v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.",
        "keywords": [
          "cs.SE",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22129v1",
        "authors": [
          "Yifeng Ding",
          "Lingming Zhang"
        ],
        "arxiv_categories": [
          "cs.SE",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T18:50:41.434220",
      "entities": [
        "Software Engineering Agents Test",
        "Large Language Model",
        "Efficient Test",
        "Bench Verified",
        "Time Scaling",
        "Bench Pro",
        "Standard",
        "SWE",
        "MIT",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22113v1",
      "title": "Diverse Approaches to Optimal Execution Schedule Generation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22113v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation, suggesting opportunities for ensemble approaches that combine improved specialists with the baseline PPO policy. Results indicate that quality-diversity methods offer promise for regime-adaptive execution, though substantial computational resources per behavioural cell may be required for robust specialist development across all market conditions. To ensure experimental integrity, we develop a calibrated Gymnasium environment focused on order scheduling rather than tactical placement decisions. The simulator features a transient impact model with exponential decay and square-root volume scaling, fit to 400+ U.S. equities with R^2>0.02 out-of-sample. Within this environment, two Proximal Policy Optimization architectures - both MLP and CNN feature extractors - demonstrate substantial improvements over industry baselines, with the CNN variant achieving 2.13 bps arrival slippage versus 5.23 bps for VWAP on 4,900 out-of-sample orders ($21B notional). These results validate both the simulation realism and provide strong single-policy baselines for quality-diversity methods.",
        "keywords": [
          "cs.LG",
          "q-fin.TR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22113v1",
        "authors": [
          "Robert de Witt",
          "Mikko S. Pakkanen"
        ],
        "arxiv_categories": [
          "cs.LG",
          "q-fin.TR"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.463094",
      "entities": [
        "Optimal Execution Schedule Generation",
        "Proximal Policy Optimization",
        "Diverse Approaches",
        "Policy",
        "VWAP",
        "Act",
        "MLP",
        "PPO",
        "CNN",
        "MAP",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21749v1",
      "title": "Fast and user-friendly econometrics estimations: The R package fixest",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21749v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "fixest is an R package for fast and flexible econometric estimation, providing a comprehensive toolkit for applied researchers. The package particularly excels at fixed-effects estimation, supported by a novel fixed-point acceleration algorithm implemented in C++. This algorithm achieves rapid convergence across a broad class of data contexts and further enables estimation of complex models, including those with varying slopes, in a highly efficient manner. Beyond computational speed, fixest provides a unified syntax for a wide variety of models: ordinary least squares, instrumental variables, generalized linear models, maximum likelihood, and difference-in-differences estimators. An expressive formula interface enables multiple estimations, stepwise regressions, and variable interpolation in a single call, while users can make on-the-fly inference adjustments using a variety of built-in robust standard errors. Finally, fixest provides methods for publication-ready regression tables and coefficient plots. Benchmarks against leading alternatives in R, Python, and Julia demonstrate best-in-class performance, and the paper includes many worked examples illustrating the core functionality.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21749v1",
        "authors": [
          "Laurent R. Bergé",
          "Kyle Butts",
          "Grant McDermott"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.463305",
      "entities": [
        "Standard",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2601.21534v1",
      "title": "Electoral Polls and Economic Uncertainty: an Analysis of the Last Two U.S. Presidential Elections",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21534v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "This paper examines the dynamic relationship between electoral polls and indicators of economic and financial uncertainty during the last two U.S. presidential elections (2020 and 2024). Using daily polling data on Donald Trump and measures such as the Aruoba-Diebold-Scotti Business Conditions Index, the 5-year Breakeven Inflation Rate, the Trade Policy Uncertainty index, and the VIX, we estimate conditional correlation models to capture time-varying interactions. The analysis reveals that in 2020, correlations between polls and uncertainty measures were highly dynamic and event-driven, reflecting the influence of exogenous shocks (COVID-19, oil price collapse) and political milestones (primaries, debates). In contrast, during the 2024 campaign, correlations remained close to zero, stable, and largely unresponsive to shocks, suggesting that entrenched polarization and non-economic events (e.g., assassination attempt, candidate changes) muted the economic channel. The study highlights how the interplay between voter sentiment, financial markets, and uncertainty varies across electoral contexts, offering a methodological contribution through the application of Dynamic Conditional Correlation models to political data and policy-relevant insights on the conditions under which economic fundamentals influence electoral dynamics.",
        "keywords": [
          "econ.EM",
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21534v1",
        "authors": [
          "Giampiero M. Gallo",
          "Demetrio Lacava",
          "Edoardo Otranto"
        ],
        "arxiv_categories": [
          "econ.EM",
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.463533",
      "entities": [
        "Scotti Business Conditions Index",
        "Dynamic Conditional Correlation",
        "Trade Policy Uncertainty",
        "Breakeven Inflation Rate",
        "Economic Uncertainty",
        "Electoral Polls",
        "Donald Trump",
        "COVID-19",
        "Last Two",
        "Policy",
        "COVID",
        "Act",
        "VIX",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21470v1",
      "title": "PPI-SVRG: Unifying Prediction-Powered Inference and Variance Reduction for Semi-Supervised Optimization",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21470v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "We study semi-supervised stochastic optimization when labeled data is scarce but predictions from pre-trained models are available. PPI and SVRG both reduce variance through control variates -- PPI uses predictions, SVRG uses reference gradients. We show they are mathematically equivalent and develop PPI-SVRG, which combines both. Our convergence bound decomposes into the standard SVRG rate plus an error floor from prediction uncertainty. The rate depends only on loss geometry; predictions affect only the neighborhood size. When predictions are perfect, we recover SVRG exactly. When predictions degrade, convergence remains stable but reaches a larger neighborhood. Experiments confirm the theory: PPI-SVRG reduces MSE by 43--52\\% under label scarcity on mean estimation benchmarks and improves test accuracy by 2.7--2.9 percentage points on MNIST with only 10\\% labeled data.",
        "keywords": [
          "econ.EM",
          "stat.ML",
          "cs.LG",
          "math.OC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21470v1",
        "authors": [
          "Ruicheng Ao",
          "Hongyu Chen",
          "Haoyang Liu"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.ML",
          "cs.LG",
          "math.OC"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.463692",
      "entities": [
        "Supervised Optimization We",
        "Unifying Prediction",
        "Variance Reduction",
        "Powered Inference",
        "Standard",
        "MNIST",
        "NIST",
        "SVRG",
        "Act",
        "MSE",
        "PPI",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21272v1",
      "title": "Finite-Sample Properties of Model Specification Tests for Multivariate Dynamic Regression Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21272v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "This paper proposes a new multivariate model specification test that generalizes Durbin regression to a seemingly unrelated regression framework and reframes the Durbin approach as a GLS-class estimator. The proposed estimator explicitly models cross-equation dependence and the joint second-order dynamics of regressors and disturbances. It remains consistent under a comparatively weak dependence condition in which conventional OLS- and GLS-based estimators can be inconsistent, and it is asymptotically efficient under stronger conditions. Monte Carlo experiments indicate that the associated Wald test achieves improved size control and competitive power in finite samples, especially when combined with a bootstrap-based bias correction. An empirical application further illustrates that the proposed procedure delivers stable inference and is practically useful for multi-equation specification testing.",
        "keywords": [
          "q-fin.PR",
          "econ.EM",
          "q-fin.ST"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21272v1",
        "authors": [
          "Koichiro Moriya",
          "Akihiko Noda"
        ],
        "arxiv_categories": [
          "q-fin.PR",
          "econ.EM",
          "q-fin.ST"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.463855",
      "entities": [
        "Multivariate Dynamic Regression Models",
        "Model Specification Tests",
        "Sample Properties",
        "Monte Carlo",
        "Framework",
        "Act",
        "OLS",
        "GLS",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21036v1",
      "title": "Experimental Design for Matching",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21036v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Matching mechanisms play a central role in operations management across diverse fields including education, healthcare, and online platforms. However, experimentally comparing a new matching algorithm against a status quo presents some fundamental challenges due to matching interference, where assigning a unit in one matching may preclude its assignment in the other. In this work, we take a design-based perspective to study the design of randomized experiments to compare two predetermined matching plans on a finite population, without imposing outcome or behavioral models. We introduce the notation of a disagreement set, which captures the difference between the two matching plans, and show that it admits a unique decomposition into disjoint alternating paths and cycles with useful structural properties. Based on these properties, we propose the Alternating Path Randomized Design, which sequentially randomizes along these paths and cycles to effectively manage interference. Within a minimax framework, we optimize the conditional randomization probability and show that, for long paths, the optimal choice converges to $\\sqrt{2}-1$, minimizing worst-case variance. We establish the unbiasedness of the Horvitz-Thompson estimator and derive a finite-population Central Limit Theorem that accommodates complex and unstable path and cycle structures as the population grows. Furthermore, we extend the design to many-to-one matchings, where capacity constraints fundamentally alter the structure of the disagreement set. Using graph-theoretic tools, including finding augmenting paths and Euler-tour decomposition on an auxiliary unbalanced directed graph, we construct feasible alternating path and cycle decompositions that allow the design and inference results to carry over.",
        "keywords": [
          "econ.EM",
          "stat.ME",
          "eess.SY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21036v1",
        "authors": [
          "Chonghuan Wang"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.ME",
          "eess.SY"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.464124",
      "entities": [
        "Alternating Path Randomized Design",
        "Experimental Design",
        "Matching Matching",
        "Framework",
        "Agreement",
        "MIT",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20976v1",
      "title": "The Effects of Higher Education on Midlife Depression: Quasi-Experimental Evidence from South Korea",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20976v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Higher education has expanded worldwide, with women outpacing men in many regions. While educational attainment is consistently linked to better physical health, its mental health effects - particularly for women - remain underexplored, and causal evidence is limited. We estimate the impact of college completion on depression among middle-aged women in South Korea, leveraging the 1993 higher education reform, which raised women's college attainment by 45 percentage points (pp) over the following decade. We use two nationally representative datasets to triangulate evidence, including the Korea National Health and Nutrition Examination Survey (KNHANES, 2007-2021) for physician-diagnosed depression, and the Korean Longitudinal Survey of Women and Families (KLoWF, 2007-2022) to validate findings using self-reports of depressive symptoms. We implement two-stage least squares (2SLS) with a birth-cohort instrument based on exposure to the reform (within 3 years of the cutoff in KNHANES and within 1 to 3 years in KLoWF). In KNHANES, college completion lowers physician-diagnosed depression by 2.4 pp, attenuating to 1.6 pp after adjusting for income, employment, and physical health. In KLoWF, college completion improves self-reported mental health. The weekly depressive-symptoms composite declines by 17.4 pp, attenuating to 16.4 pp after covariate adjustment. Placebo tests on unaffected cohorts yield null results. This study contributes to the growing quasi-experimental literature on education and mental health with convergent evidence across clinical diagnoses and self-reported depressive symptoms in South Korea. By focusing on college education in a non-Western setting, it extends the external validity of existing findings and highlights educational policy as a potential lever to reduce the burden of midlife depression among women.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20976v1",
        "authors": [
          "Ah-Reum Lee",
          "Jacqueline M. Torres",
          "Jinkook Lee"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.464420",
      "entities": [
        "Nutrition Examination Survey",
        "Korean Longitudinal Survey",
        "Experimental Evidence",
        "Korea National Health",
        "Midlife Depression",
        "South Korea Higher",
        "Higher Education",
        "South Korea",
        "Policy",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20853v1",
      "title": "A Smoothed GMM for Dynamic Quantile Preferences Estimation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20853v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "This paper suggests methods for estimation of the $τ$-quantile, $τ\\in(0,1)$, as a parameter along with the other finite-dimensional parameters identified by general conditional quantile restrictions. We employ a generalized method of moments framework allowing for non-linearities and dependent data, where moment functions are smoothed to aid both computation and tractability. Consistency and asymptotic normality of the estimators are established under weak assumptions. Simulations illustrate the finite-sample properties of the methods. An empirical application using a quantile intertemporal consumption model with multiple assets estimates the risk attitude, which is captured by $τ$, together with the elasticity of intertemporal substitution.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20853v1",
        "authors": [
          "Xin Liu",
          "Luciano de Castro",
          "Antonio F. Galvao"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.464761",
      "entities": [
        "Dynamic Quantile Preferences Estimation",
        "Framework",
        "Act",
        "GMM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20912v1",
      "title": "Clear Messages, Ambiguous Audiences: Measuring Interpretability in Political Communication",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20912v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Text-based measurement in political research often treats classi6ication disagreement as random noise. We examine this assumption using con6idence-weighted human annotations of 5,000 social media messages by U.S. politicians. We 6ind that political communication is generally highly legible, with mean con6idence exceeding 0.99 across message type, partisan bias, and audience classi6ications. However, systematic variation concentrates in the constituency category, which exhibits a 1.79 percentage point penalty in audience classi6ication con6idence. Given the high baseline of agreement, this penalty represents a sharp relative increase in interpretive uncertainty. Within messages, intent remains clear while audience targeting becomes ambiguous. These patterns persist with politician 6ixed effects, suggesting that measurement error in political text is structured by strategic incentives rather than idiosyncratic coder error.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20912v1",
        "authors": [
          "Krishna Sharma",
          "Khemraj Bhatt"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.464922",
      "entities": [
        "Political Communication Text",
        "Measuring Interpretability",
        "Ambiguous Audiences",
        "Clear Messages",
        "Agreement",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20724v1",
      "title": "Pricing Catastrophe: How Extreme Political Shocks Reprice Sovereign Risk, Beliefs, and Growth Expectations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20724v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Extreme political shocks may reshape economies not only through contemporaneous disruption but by altering beliefs about the distribution of future states. We study how such belief ruptures affect the cost of capital, expectations, and macroeconomic dynamics, using the October 7, 2023 attack on Israel as a precisely timed shock. Leveraging monthly data from 2008 to 2025 and a donor pool of advanced economies, we estimate counterfactual paths using a matrix completion design with rolling-window cross-validation and placebo-based inference, corroborated by synthetic difference-in-differences. We document three core findings. First, long-horizon sovereign risk of Israel is persistently repriced. Ten-year yields and spreads relative to the United States rise sharply and remain elevated. Second, household welfare beliefs deteriorate durably, as reflected in consumer confidence. Third, medium-run momentum improves, captured by a strong rise in the OECD composite leading indicator. These patterns reveal risk-growth decoupling where tail-risk premia rise even as medium-horizon activity expectations strengthen. Our results highlight belief-driven channels as a central mechanism through which extreme ruptures shape macro-financial outcomes.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20724v1",
        "authors": [
          "Riste Ichev",
          "Rok Spruk"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.465128",
      "entities": [
        "How Extreme Political Shocks",
        "Growth Expectations Extreme",
        "Reprice Sovereign Risk",
        "Pricing Catastrophe",
        "United States",
        "OECD",
        "Wind",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20487v2",
      "title": "Normative Equivalence in Human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20487v2",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.",
        "keywords": [
          "cs.GT",
          "cs.AI",
          "econ.GN",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20487v2",
        "authors": [
          "Nico Mutzner",
          "Taha Yasseri",
          "Heiko Rauhut"
        ],
        "arxiv_categories": [
          "cs.GT",
          "cs.AI",
          "econ.GN",
          "cs.HC"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.465394",
      "entities": [
        "Artificial Intelligence",
        "Normative Equivalence",
        "Drives Cooperation",
        "Public Goods Game",
        "Not Identity",
        "Intel",
        "Act",
        "PGG",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20469v1",
      "title": "The realized empirical distribution function of stochastic variance with application to goodness-of-fit testing",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20469v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "We propose a nonparametric estimator of the empirical distribution function (EDF) of the latent spot variance of the log-price of a financial asset. We show that over a fixed time span our realized EDF (or REDF) -- inferred from noisy high-frequency data -- is consistent as the mesh of the observation grid goes to zero. In a double-asymptotic framework, with time also increasing to infinity, the REDF converges to the cumulative distribution function of volatility, if it exists. We exploit these results to construct some new goodness-of-fit tests for stochastic volatility models. In a Monte Carlo study, the REDF is found to be accurate over the entire support of volatility. This leads to goodness-of-fit tests that are both correctly sized and relatively powerful against common alternatives. In an empirical application, we recover the REDF from stock market high-frequency data. We inspect the goodness-of-fit of several two-parameter marginal distributions that are inherent in standard stochastic volatility models. The inverse Gaussian offers the best overall description of random equity variation, but the fit is less than perfect. This suggests an extra parameter (as available in, e.g., the generalized inverse Gaussian) is required to model stochastic variance.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20469v1",
        "authors": [
          "Kim Christensen",
          "Martin Thyrsgaard",
          "Bezirgen Veliyev"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.465626",
      "entities": [
        "Monte Carlo",
        "Framework",
        "Standard",
        "REDF",
        "EDF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20463v1",
      "title": "Realized range-based estimation of integrated variance",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20463v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "We provide a set of probabilistic laws for estimating the quadratic variation of continuous semimartingales with realized range-based variance -- a statistic that replaces every squared return of realized variance with a normalized squared range. If the entire sample path of the process is available, and under a set of weak conditions, our statistic is consistent and has a mixed Gaussian limit, whose precision is five times greater than that of realized variance. In practice, of course, inference is drawn from discrete data and true ranges are unobserved, leading to downward bias. We solve this problem to get a consistent, mixed normal estimator, irrespective of non-trading effects. This estimator has varying degrees of efficiency over realized variance, depending on how many observations that are used to construct the high-low. The methodology is applied to TAQ data and compared with realized variance. Our findings suggest that the empirical path of quadratic variation is also estimated better with the realized range-based variance.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20463v1",
        "authors": [
          "Kim Christensen",
          "Mark Podolskij"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.465794",
      "entities": [
        "Act",
        "WHO",
        "MIT",
        "TAQ",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20452v1",
      "title": "Manipulation in Prediction Markets: An Agent-based Modeling Experiment",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20452v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Prediction markets mobilize financial incentives to forecast binary event outcomes through the aggregation of dispersed beliefs and heterogeneous information. Their growing popularity and demonstrated predictive accuracy in political elections have raised speculation and concern regarding their susceptibility to manipulation and the potential consequences for democratic processes. Using agent-based simulations combined with an analytic characterization of price dynamics, we study how high-budget agents can introduce price distortions in prediction markets. We explore the persistence and stability of these distortions in the presence of herding or stubborn agents, and analyze how agent expertise affects market-price variance. Firstly we propose an agent-based model of a prediction market in which bettors with heterogeneous expertise, noisy private information, variable learning rates and budgets observe the evolution of public opinion on a binary election outcome to inform their betting strategies in the market. The model exhibits stability across a broad parameter space, with complex agent behaviors and price interactions producing self-regulatory price discovery. Second, using this simulation framework, we investigate the conditions under which a highly resourced minority, or ''whale'' agent, with a biased valuation can distort the market price, and for how long. We find that biased whales can temporarily shift prices, with the magnitude and duration of distortion increasing when non-whale bettors exhibit herding behavior and slow learning. Our theoretical analysis corroborates these results, showing that whales can shift prices proportionally to their share of market capital, with distortion duration depending on non-whale learning rates and herding intensity.",
        "keywords": [
          "econ.GN",
          "physics.soc-ph",
          "q-fin.TR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20452v1",
        "authors": [
          "Bridget Smart",
          "Ebba Mark",
          "Anne Bastian"
        ],
        "arxiv_categories": [
          "econ.GN",
          "physics.soc-ph",
          "q-fin.TR"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.466073",
      "entities": [
        "Modeling Experiment Prediction",
        "Prediction Markets",
        "Framework",
        "An Agent",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20285v1",
      "title": "Bank Runs With and Without Bank Failure",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20285v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "We study the causes and consequences of bank runs using a novel dataset on bank runs in the United States from 1863 to 1934. Applying natural language processing to historical newspapers, we identify 4,049 runs on individual banks. Runs are considerably more likely in weak banks but also occur in strong banks, especially in response to negative news about the real economy or the broader banking system. However, runs typically only result in failure for banks with weak fundamentals. Strong banks survive runs through various mechanisms, including interbank cooperation, equity injections, public signals of strength, and suspension of convertibility. At the local level, bank failures (with and without runs) translate into substantially larger declines in deposits and lending than runs without failures. Our findings suggest that poor bank fundamentals are necessary for bank runs to translate into failure and for bank distress to generate severe economic consequences.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20285v1",
        "authors": [
          "Sergio Correia",
          "Stephan Luck",
          "Emil Verner"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.466230",
      "entities": [
        "Without Bank Failure We",
        "Bank Runs With",
        "United States",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20238v1",
      "title": "Large Language Models Polarize Ideologically but Moderate Affectively in Online Political Discourse",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20238v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "The emergence of large language models (LLMs) is reshaping how people engage in political discourse online. We examine how the release of ChatGPT altered ideological and emotional patterns in the largest political forum on Reddit. Analysis of millions of comments shows that ChatGPT intensified ideological polarization: liberals became more liberal, and conservatives more conservative. This shift does not stem from the creation of more persuasive or ideologically extreme original content using ChatGPT. Instead, it originates from the tendency of ChatGPT-generated comments to echo and reinforce the viewpoint of original posts, a pattern consistent with algorithmic sycophancy. Yet, despite growing ideological divides, affective polarization, measured by hostility and toxicity, declined. These findings reveal that LLMs can simultaneously deepen ideological separation and foster more civil exchanges, challenging the long-standing assumption that extremity and incivility necessarily move together.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20238v1",
        "authors": [
          "Gavin Wang",
          "Srinaath Anbudurai",
          "Oliver Sun"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.466400",
      "entities": [
        "Large Language Models Polarize",
        "Moderate Affectively",
        "ChatGPT",
        "MIT",
        "LLM",
        "DOE",
        "GPT",
        "EPA"
      ]
    },
    {
      "id": "arxiv-2601.20197v1",
      "title": "Bias-Reduced Estimation of Finite Mixtures: An Application to Latent Group Structures in Panel Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20197v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Finite mixture models are widely used in econometric analyses to capture unobserved heterogeneity. This paper shows that maximum likelihood estimation of finite mixtures of parametric densities can suffer from substantial finite-sample bias in all parameters under mild regularity conditions. The bias arises from the influence of outliers in component densities with unbounded or large support and increases with the degree of overlap among mixture components. I show that maximizing the classification-mixture likelihood function, equipped with a consistent classifier, yields parameter estimates that are less biased than those obtained by standard maximum likelihood estimation (MLE). I then derive the asymptotic distribution of the resulting estimator and provide conditions under which oracle efficiency is achieved. Monte Carlo simulations show that conventional mixture MLE exhibits pronounced finite-sample bias, which diminishes as the sample size or the statistical distance between component densities tends to infinity. The simulations further show that the proposed estimation strategy generally outperforms standard MLE in finite samples in terms of both bias and mean squared errors under relatively weak assumptions. An empirical application to latent group panel structures using health administrative data shows that the proposed approach reduces out-of-sample prediction error by approximately 17.6% relative to the best results obtained from standard MLE procedures.",
        "keywords": [
          "stat.CO",
          "econ.EM",
          "stat.ME",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20197v1",
        "authors": [
          "Raphaël Langevin"
        ],
        "arxiv_categories": [
          "stat.CO",
          "econ.EM",
          "stat.ME",
          "cs.LG"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.466637",
      "entities": [
        "Latent Group Structures",
        "Reduced Estimation",
        "Panel Data Finite",
        "Finite Mixtures",
        "An Application",
        "Monte Carlo",
        "Standard",
        "Oracle",
        "NIST",
        "MLE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20169v1",
      "title": "United in Currency, Divided in Growth: Dynamic Effects of Euro Adoption",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20169v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Does euro adoption affect long-run economic growth? Existing evidence is mixed, reflecting limited treated countries, long horizons that challenge inference, and heterogeneity across member states. We estimate causal dynamic and heterogeneous treatment effects using Causal Forests with Fixed Effects (CFFE), a machine-learning approach that combines causal forests with two-way fixed effects. Under a conditional parallel-trends assumption, we find that euro adoption reduced annual GDP growth by 0.3-0.4 percentage points on average. Effects emerge shortly after adoption and stabilize after roughly a decade. Average effects mask substantial heterogeneity. Countries with lower initial GDP per capita experience larger and more persistent growth shortfalls than core economies. Weaker consumption and productivity growth contribute to the overall effect, while improvements in net exports partially offset these declines. A two-country New Keynesian DSGE model with hysteresis generates qualitatively similar patterns: one-size-fits-all monetary policy and scarring mechanisms produce larger output losses under monetary union than under flexible exchange rates. By jointly estimating dynamic and heterogeneous treatment effects, the analysis highlights the importance of country characteristics in assessing the long-run consequences of monetary union.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20169v1",
        "authors": [
          "Harry Aytug"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.466852",
      "entities": [
        "Euro Adoption Does",
        "Dynamic Effects",
        "Causal Forests",
        "New Keynesian",
        "Fixed Effects",
        "Policy",
        "DSGE",
        "CFFE",
        "Act",
        "MIT",
        "GDP",
        "DOE",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20018v1",
      "title": "Decoupling and randomization for double-indexed permutation statistics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20018v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "This paper introduces a version of decoupling and randomization to establish concentration inequalities for double-indexed permutation statistics. The results yield, among other applications, a new combinatorial Hanson-Wright inequality and a new combinatorial Bennett inequality. Several illustrative examples from rank-based statistics, graph-based statistics, and causal inference are also provided.",
        "keywords": [
          "math.ST",
          "econ.EM",
          "math.PR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20018v1",
        "authors": [
          "Mingxuan Zou",
          "Jingfan Xu",
          "Peng Ding"
        ],
        "arxiv_categories": [
          "math.ST",
          "econ.EM",
          "math.PR"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.466934"
    },
    {
      "id": "arxiv-2601.19886v1",
      "title": "AI Cap-and-Trade: Efficiency Incentives for Accessibility and Sustainability",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19886v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "The race for artificial intelligence (AI) dominance often prioritizes scale over efficiency. Hyper-scaling is the common industry approach: larger models, more data, and as many computational resources as possible. Using more resources is a simpler path to improved AI performance. Thus, efficiency has been de-emphasized. Consequently, the need for costly computational resources has marginalized academics and smaller companies. Simultaneously, increased energy expenditure, due to growing AI use, has led to mounting environmental costs. In response to accessibility and sustainability concerns, we argue for research into, and implementation of, market-based methods that incentivize AI efficiency. We believe that incentivizing efficient operations and approaches will reduce emissions while opening new opportunities for academics and smaller companies. As a call to action, we propose a cap-and-trade system for AI. Our system provably reduces computations for AI deployment, thereby lowering emissions and monetizing efficiency to the benefit of of academics and smaller companies.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "econ.GN",
          "cs.GT"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19886v1",
        "authors": [
          "Marco Bornstein",
          "Amrit Singh Bedi"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "econ.GN",
          "cs.GT"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:44.467113",
      "entities": [
        "Artificial Intelligence",
        "Efficiency Incentives",
        "Intel",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22111v1",
      "title": "Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22111v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectories, limiting full wind-field recovery. This study presents a framework for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm. A synthetic turbulence environment and high-fidelity multirotor simulation are used to generate training and evaluation data. Local wind components are estimated from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM) and assimilated into a physics-informed neural network (PINN) to reconstruct a continuous wind field in space and time. For local wind estimation, the bidirectional LSTM achieves root-mean-square errors (RMSE) of 0.064 and 0.062 m/s for the north and east components in low-wind conditions, increasing to 0.122 to 0.129 m/s under moderate winds and 0.271 to 0.273 m/s in high-wind conditions, while the vertical component exhibits higher error, with RMSE values of 0.029 to 0.091 m/s. The physics-informed reconstruction recovers the dominant spatial and temporal structure of the wind field up to 1000 m altitude while preserving mean flow direction and vertical shear. Under moderate wind conditions, the reconstructed mean wind field achieves an overall RMSE between 0.118 and 0.154 m/s across evaluated UAS configurations, with the lowest error obtained using a five-UAS swarm. These results demonstrate that coordinated UAS measurements enable accurate and scalable four-dimensional wind-field reconstruction without dedicated wind sensors or fixed infrastructure.",
        "keywords": [
          "eess.SY",
          "physics.ao-ph",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22111v1",
        "authors": [
          "Abdullah Tasim",
          "Wei Sun"
        ],
        "arxiv_categories": [
          "eess.SY",
          "physics.ao-ph",
          "cs.LG"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.442767",
      "entities": [
        "Synthetic Turbulent Environment Accurate",
        "Dimensional Atmospheric Wind Fields",
        "Physics Informed Reconstruction",
        "Swarm Observations",
        "Neural Network",
        "Using Multi",
        "Framework",
        "RMSE",
        "PINN",
        "Wind",
        "LSTM",
        "MIT",
        "UAS",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2601.21913v1",
      "title": "Rapid estimation of global sea surface temperatures from sparse streaming in situ observations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21913v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Reconstructing high-resolution sea surface temperatures (SST) from staggered SST measurements is essential for weather forecasting and climate projections. However, when SST measurements are sparse, the resulting inferred SST fields are rather inaccurate. Here, we demonstrate the ability of Sparse Discrete Empirical Interpolation Method (S-DEIM) to reconstruct the high-resolution SST field from sparse in situ observations, without using a model. The S-DEIM estimate consists of two terms, one computed from instantaneous in situ observations using empirical interpolation, and the other learned from the historical time series of observations using recurrent neural networks (RNNs). We train the RNNs using the National Oceanic and Atmospheric Administration's weekly high-resolution SST dataset spanning the years 1989-2021 which constitutes the training data. Subsequently, we examine the performance of S-DEIM on the test data, comprising January 2022 to January 2023. For this test data, S-DEIM infers the high-resolution SST from 100 in situ observations, constituting only 0.2% of the high-resolution spatial grid. We show that the resulting S-DEIM reconstructions are about 40% more accurate than earlier empirical interpolation methods, such as DEIM and Q-DEIM. Furthermore, 91% of S-DEIM estimates fall within $\\pm 1^\\circ$C of the true SST. We also demonstrate that S-DEIM is robust with respect to sensor placement: even when the sensors are distributed randomly, S-DEIM reconstruction error deteriorates only by 1-2%. S-DEIM is also computationally efficient. Training the RNN, which is performed only once offline, takes approximately one minute. Once trained, the S-DEIM reconstructions are computed in less than a second. As such, S-DEIM can be used for rapid SST reconstruction from sparse streaming observational data in real time.",
        "keywords": [
          "math.NA",
          "physics.ao-ph",
          "math.DS"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21913v1",
        "authors": [
          "Cassidy All",
          "Kevin Ho",
          "Maya Magnuski"
        ],
        "arxiv_categories": [
          "math.NA",
          "physics.ao-ph",
          "math.DS"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.443072",
      "entities": [
        "Sparse Discrete Empirical Interpolation",
        "Atmospheric Administration",
        "National Oceanic",
        "Neural Network",
        "DEIM",
        "NIST",
        "RNN",
        "SST",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2601.21890v1",
      "title": "Reddy: An open-source toolbox for analyzing eddy-covariance measurements in heterogeneous environments",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21890v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Land-atmosphere exchange processes are determined by turbulent fluxes, which can be derived from eddy-covariance measurements. This method was established to quantify ecosystem-scale vertical atmosphere-vegetation exchange processes, but is also used to validate atmospheric turbulence theories with the ultimate aim to improve the representation of turbulence in numerical models. While the focus has long been on turbulence over idealized, homogeneous and flat surfaces, recent scientific developments are shifting towards investigating turbulent exchange processes in complex heterogeneous environments under non-idealized conditions, which pose particular challenges, e.g. advective fluxes between different surface types or non-stationarity of nighttime turbulence. This requires to rethink standard post-processing routines for determining turbulent fluxes from the high-frequency sonic and gas analyzer measurements. Here, we introduce the open-source R-package 'Reddy', which provides modular-built functions for post-processing, analysis and visualization of eddy-covariance measurements, including investigating spectra, coherent structures, anisotropy, flux footprints and surface energy balance closure. The 'Reddy' package is accompanied by a detailed documentation and a set of jupyter notebooks introducing new users hands-on to eddy-covariance data analysis. We showcase 'Reddy' based on measurements from three different sites in Norway: A case study during strong stratification over alpine tundra, for determining suitable averaging times during ice-cover transition at a boreal lake, and for fitting flux-variance relations for a permafrost peatland. 'Reddy' serves as extension of previously developed software packages, paving the way towards holistic turbulence data analysis in heterogeneous real-world environments.",
        "keywords": [
          "stat.ME",
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21890v1",
        "authors": [
          "Laura Mack",
          "Norbert Pirk"
        ],
        "arxiv_categories": [
          "stat.ME",
          "physics.ao-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.443356",
      "entities": [
        "Standard",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2601.21743v1",
      "title": "Impact of behavioral heterogeneity on epidemic outcome and its mapping into effective network topologies",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21743v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Human behavior plays a critical role in shaping epidemic trajectories. During health crises, people respond in diverse ways in terms of self-protection and adherence to recommended measures, largely reflecting differences in how individuals assess risk. This behavioral variability induces effective heterogeneity into key epidemic parameters, such as infectivity and susceptibility. We introduce a minimal extension of the susceptible-infected-removed~(SIR) model, denoted HeSIR, that captures these effects through a simple bimodal scheme, where individuals may have higher or lower transmission--related traits. We derive a closed-form expression for the epidemic threshold in terms of the model parameters, and the network's degree distribution and homophily, defined as the tendency of like--risk individuals to preferentially interact. We identify a resurgence regime just beyond the classical threshold, where the number of infected individuals may initially decline before surging into large-scale transmission. Through simulations on homogeneous and heterogeneous network topologies we corroborate the analytical results and highlight how variations in susceptibility and infectivity influence the epidemic dynamics. We further show that, under suitable assumptions, the HeSIR model maps onto a standard SIR process on an appropriately modified contact network, providing a unified interpretation in terms of structural connectivity. Our findings quantify the effect of heterogeneous behavioral responses, especially in the presence of homophily, and caution against underestimating epidemic potential in fragmented populations, which may undermine timely containment efforts. The results also extend to heterogeneity arising from biological or other non-behavioral sources.",
        "keywords": [
          "q-bio.PE",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21743v1",
        "authors": [
          "Fabio Mazza",
          "Gabriele Ricci",
          "Francesca Colaiori"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.443631",
      "entities": [
        "Standard",
        "Act",
        "SIR",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21516v1",
      "title": "PINN-based short-term forecasting of fault slip evolution during the 2010 slow slip event in the Bungo Channel, Japan",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21516v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Monitoring and forecasting fault slip evolution are fundamental for understanding earthquake cycles and assessing future seismic hazards. This study proposes a physics-based data assimilation framework that integrates geodetic observations with fault mechanics introducing spatial heterogeneity in frictional properties, with a particular focus on short-term fault slip forecasting. The proposed method employs physics-informed neural networks (PINNs) to calculate fault slip evolutions and to optimize the spatial distribution of frictional properties and is applied to the 2010 slow slip event beneath the Bungo Channel, southwest Japan, by changing the data period to be assimilated. When only the initial phase of slip acceleration is assimilated, a velocity-weakening frictional region is inferred beneath southwest Shikoku, corresponding to the initial nucleation are of the slow slip event. Out results demonstrate that the PINN-based data assimilation framework successfully forecasts slow transient slip even when only slip acceleration data are assimilated, whereas forecasts based on frictionally homogeneous models result in unstable fast slip. This difference can be interpreted as a consequence of introducing frictional heterogeneity, which allows both the characteristic size of the slipping region and the critical nucleation size to be variable, leading to stable slip evolution consistent with observations. When longer observation periods are assimilated, a velocity-strengthening region emerges around the slip-weakening patch, progressively restricting the direction of slip propagation. This velocity-strengthening region is interpreted as a mechanical constraint imposed by fault physics, linking the slip regions required to reproduce the observed geodetic time series. The results highlight the capability of PINN-based data assimilation incorporating geodetic observations and fault mechanics.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21516v1",
        "authors": [
          "Masayuki Kano",
          "Rikuto Fukushima"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.443928",
      "entities": [
        "Japan Monitoring",
        "Neural Network",
        "Bungo Channel",
        "Framework",
        "PINN",
        "Act",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21480v1",
      "title": "Long-term evolution of regulatory DNA sequences. Part 2: Theory and future challenges",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21480v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Promoters and enhancers are cis-regulatory elements (CREs), DNA sequences that bind transcription factor (TF) proteins to up- or down-regulate target genes. Decades-long efforts yielded TF-DNA interaction models that predict how strongly an individual TF binds arbitrary DNA sequences and how individual binding events on the CRE combine to affect gene expression. These insights can be synthesized into a global, biophysically-realistic, and quantitative genotype-phenotype (GP) map for gene regulation, a \"holy grail\" for the application of evolutionary theory. A global map provides a rare opportunity to simulate long-term evolution of regulatory sequences and pose several fundamental questions: How long does it take to evolve CREs de novo? How many non-trivial regulatory functions exist in sequence space? How connected are they? For which regulatory architecture is CRE evolution most rapid and evolvable? In this article, the second of a two-part series, we review the application of evolutionary concepts - epistasis, robustness, evolvability, tunability, plasticity, and bet-hedging - to the evolution of gene regulatory sequences. We then evaluate the potential for a unifying theory for the evolution of regulatory sequences, and identify key open challenges.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21480v1",
        "authors": [
          "Elia Mascolo",
          "Réka Borbély",
          "Noa Ottilie Borst"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.444135",
      "entities": [
        "Regulation",
        "Act",
        "DOE",
        "DNA",
        "CRE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21079v1",
      "title": "The quenched structured coalescent for diploid population models on finite graphs with large migrations and uneven offspring distributions",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21079v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "In this work we describe a new model for the evolution of a diploid structured population backwards in time that allows for large migrations and uneven offspring distributions. The model generalizes both the mean-field model of Birkner et al. [\\textit{Electron. J. Probab.} 23: 1-44 (2018)] and the haploid structured model of Möhle [\\textit{Theor. Popul. Biol.} 2024 Apr:156:103-116]. We show convergence, with mild conditions on the joint distribution of offspring frequencies and migrations, of gene genealogies conditional on the pedigree to a time-inhomogeneous coalescent process driven by a Poisson point process $Ψ$ that records the timing and scale of large migrations and uneven offspring distributions. This quenched scaling limit demonstrates a significant difference in the predictions of the classical annealed theory of structured coalescent processes. In particular, the annealed and quenched scaling limits coincide if and only if these large migrations and uneven offspring distributions are absent. The proof proceeds by the method of moments and utilizes coupling techniques from the theory of random walks in random environments. Several examples are given and their quenched scaling limits established.",
        "keywords": [
          "q-bio.PE",
          "math.PR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21079v1",
        "authors": [
          "Maximillian Newman"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "math.PR"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.444661",
      "entities": [
        "MIT",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21004v1",
      "title": "A Tolerance-Based Framework for Spatio-Temporal Forecast Validation Using the gamma-Index",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21004v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Classical field forecast evaluation relies mainly on local scores such as RMSE or MAE. These metrics severely over-penalize small spatial or temporal displacements of coherent structures, a limitation known as the double-penalty issue and common to many forecasting domains. The present paper introduces a tolerance-based framework built on the three-dimensional gamma index, initially designed for medical dose verification, as a unified acceptance criterion for gridded forecasts. The method embeds explicit margins in space (DTA), time (TTA), and intensity (IDT), and evaluates whether predictions agree with observations within predefined physical bounds rather than through pixel-wise differences only. A synthetic illustration is first used to show why conventional metrics can misrepresent usable forecasts. The approach is then applied to satellite-derived SSI fields to demonstrate operational behaviour on a real dataset. Results confirm that the gamma criterion preserves structural consistency under minor positional noise while isolating physically significant discrepancies. The formulation is generic and can be implemented for any gridded variable provided meaningful tolerances are defined, offering a pragmatic complement to existing spatial verification tools in general forecasting workflows.",
        "keywords": [
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21004v1",
        "authors": [
          "Cyril Voyant"
        ],
        "arxiv_categories": [
          "physics.ao-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.444885",
      "entities": [
        "Temporal Forecast Validation Using",
        "Index Classical",
        "Based Framework",
        "Satellite",
        "Framework",
        "RMSE",
        "MIT",
        "TTA",
        "IDT",
        "EPA",
        "MAE",
        "DTA",
        "SSI",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20981v1",
      "title": "Diversifying Toxicity Search in Large Language Models Through Speciation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20981v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Evolutionary prompt search is a practical black-box approach for red teaming large language models (LLMs), but existing methods often collapse onto a small family of high-performing prompts, limiting coverage of distinct failure modes. We present a speciated quality-diversity (QD) extension of ToxSearch that maintains multiple high-toxicity prompt niches in parallel rather than optimizing a single best prompt. ToxSearch-S introduces unsupervised prompt speciation via a search methodology that maintains capacity-limited species with exemplar leaders, a reserve pool for outliers and emerging niches, and species-aware parent selection that trades off within-niche exploitation and cross-niche exploration. ToxSearch-S is found to reach higher peak toxicity ($\\approx 0.73$ vs.\\ $\\approx 0.47$) and a extreme heavier tail (top-10 median $0.66$ vs.\\ $0.45$) than the baseline, while maintaining comparable performance on moderately toxic prompts. Speciation also yields broader semantic coverage under a topic-as-species analysis (higher effective topic diversity $N_1$ and larger unique topic coverage $K$). Finally, species formed are well-separated in embedding space (mean separation ratio $\\approx 1.93$) and exhibit distinct toxicity distributions, indicating that speciation partitions the adversarial space into behaviorally differentiated niches rather than superficial lexical variants. This suggests our approach uncovers a wider range of attack strategies.",
        "keywords": [
          "q-bio.PE",
          "cs.NE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20981v1",
        "authors": [
          "Onkar Shelar",
          "Travis Desell"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "cs.NE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.445110",
      "entities": [
        "Large Language Models Through",
        "Diversifying Toxicity Search",
        "Speciation Evolutionary",
        "Act",
        "MIT",
        "LLM",
        "EPA",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20771v1",
      "title": "Cross-Country Learning for National Infectious Disease Forecasting Using European Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20771v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Accurate forecasting of infectious disease incidence is critical for public health planning and timely intervention. While most data-driven forecasting approaches rely primarily on historical data from a single country, such data are often limited in length and variability, restricting the performance of machine learning (ML) models. In this work, we investigate a cross-country learning approach for infectious disease forecasting, in which a single model is trained on time series data from multiple countries and evaluated on a country of interest. This setting enables the model to exploit shared epidemic dynamics across countries and to benefit from an enlarged training set. We examine this approach through a case study on COVID-19 case forecasting in Cyprus, using surveillance data from European countries. We evaluate multiple ML models and analyse the impact of the lookback window length and cross-country `data augmentation' on multi-step forecasting performance. Our results show that incorporating data from other countries can lead to consistent improvements over models trained solely on national data. Although the empirical focus is on Cyprus and COVID-19, the proposed framework and findings are applicable to infectious disease forecasting more broadly, particularly in settings with limited national historical data.",
        "keywords": [
          "q-bio.PE",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20771v1",
        "authors": [
          "Zacharias Komodromos",
          "Kleanthis Malialis",
          "Artemis Kontou"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "cs.LG"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.445325",
      "entities": [
        "National Infectious Disease Forecasting",
        "Using European Data Accurate",
        "Country Learning",
        "Machine Learning",
        "Framework",
        "COVID-19",
        "COVID",
        "Wind",
        "Act",
        "MIT",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20670v1",
      "title": "Noise-induced excitability: bloom, bust and extirpation in autotoxic population dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20670v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Species populations often modify their environment as they grow. When environmental feedback operates more slowly than population growth, the system can undergo boom-bust dynamics, where the population overshoots its carrying capacity and subsequently collapses. In extreme cases, this collapse leads to total extinction. While deterministic models typically fail to capture these finite-time extinction events, we propose a stochastic framework, derived from an individual-based model, to describe boom-bust-extirpation dynamics. We identify a noise-driven, threshold-like behavior where, depending on initial conditions, the population either undergoes a \"boom\" or is extirpated before the expansion occurs. Furthermore, we characterize a transition between an excitable regime, where most trajectories are captured by the absorbing state immediately after the first bust, and a persistent regime, where most populations reach a metastable state. We show that this transition is governed by the diffusion strength and the ratio of environmental-to-population timescales. This framework provides a theoretical basis for understanding irreversible transitions in invasive species, plant succession, and microbial dynamics.",
        "keywords": [
          "nlin.AO",
          "q-bio.PE",
          "cond-mat.stat-mech"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20670v1",
        "authors": [
          "Pablo Moreno-Spiegelberg",
          "Javier Aguilar"
        ],
        "arxiv_categories": [
          "nlin.AO",
          "q-bio.PE",
          "cond-mat.stat-mech"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.445531",
      "entities": [
        "Framework",
        "Fusion",
        "Meta",
        "NIST",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20416v1",
      "title": "Promotion of cooperation in deme-structured populations with growth-merging dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20416v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "The spatial structure of populations may promote the emergence and maintenance of cooperation. Cooperation in the prisoner's dilemma is favored under specific update rules in evolutionary graph theory models with one individual per node of a graph, but this effect vanishes in models with well-mixed demes connected by migrations under soft selection. In contrast, experiments and models involving cycles of growth, merging and dilution have shown that spatial structure can favor cooperation. Here, we reconcile these findings by studying deme-structured populations under growth-merging-dilution dynamics, corresponding to a clique (fully connected graph) under hard selection. We obtain analytical conditions for the cooperator fraction to increase during deterministic logistic growth, and to increase on average under dilution-growth-merging cycles, in the weak selection regime. Furthermore, we analytically express the fixation probability of cooperators under weak selection, yielding a criterion for cooperative mutants to have a higher fixation probability than neutral ones. Finally, numerical simulations show that stochastic growth further promotes cooperation. Overall, hard selection is essential for cooperation to be promoted in deme-structured populations.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20416v1",
        "authors": [
          "Damien Ribière",
          "Alia Abbara",
          "Anne-Florence Bitbol"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.445730",
      "entities": [
        "NIST",
        "Act",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20342v1",
      "title": "StormDiT: A generative AI model bridges the 2-6 hour 'gray zone' in precipitation nowcasting",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20342v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Accurate short-term warnings for extreme precipitation are critical for global disaster mitigation but are hindered by a persistent predictability barrier at the 2-6 hour horizon -- the \"nowcasting gray zone.\" In this window, traditional observation-based extrapolation fails due to error accumulation, while numerical weather prediction is computationally too slow to resolve storm-scale dynamics. Recent generative AI approaches attempt to bridge this gap by decomposing precipitation into separate deterministic advection and stochastic diffusion components. However, this decomposition can sever fundamental causal links between entangled atmospheric processes, such as the dynamic initiation of convection triggered by boundary advection. Here we present StormDiT, a unified generative model that treats weather evolution as a holistic spatiotemporal problem, learning the coupled physics of the gray zone without human-imposed structural priors. Trained on a massive dataset of 7,720 precipitation events from China, our model achieves a breakthrough in long-horizon stability. On a heavy-rainfall test set, it maintains skillful prediction for strong convection ($\\ge$ 35 dBZ) with a Critical Success Index (CSI) near 0.2 across the full 6-hour forecast at 6-minute resolution. Crucially, the model exhibits superior probabilistic calibration, accurately quantifying operational risks. On the public SEVIR benchmark, our unified paradigm more than doubles the state-of-the-art 1-hour performance for heavy rain and establishes the first robust baseline for 3-hour forecasting. Furthermore, interpretability analysis reveals that the model attends to non-local physical precursors, such as outflow boundaries, explicitly validating its emergent understanding of convective organization.",
        "keywords": [
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20342v1",
        "authors": [
          "Haofei Sun",
          "Yunfan Yang",
          "Wei Han"
        ],
        "arxiv_categories": [
          "physics.ao-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.446001",
      "entities": [
        "Critical Success Index",
        "Fusion",
        "SEVIR",
        "NIST",
        "Wind",
        "MIT",
        "CSI",
        "IoT",
        "EPA",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20182v1",
      "title": "Atmospheric Muon Measurements Near Tornadic and Non-Tornadic Storms in the US Central Plains",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20182v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Tornadoes and other severe weather hazards affect thousands of people every year. Despite this, the details surrounding tornadic processes including formation, decay, and longevity are not well understood, partially due to limitations of available instrumentation. Measurements of atmospheric pressure within tornadic systems currently rely almost entirely on in-situ instrumentation, and no existing techniques can provide two-dimensional spatial information of the atmospheric density field. Atmospheric muons may hold a solution to this problem: muons are attenuated by matter, and tornadic storms are large regions of low atmospheric density, suggesting that tornadic storms induce a directional perturbation on the atmospheric muon flux. Measurements of this perturbation could then be used to infer the density field associated with severe weather. Simulations of these systems indicate that a robust measurement of the atmospheric density field would require a relatively large muon detector, however smaller detectors may be able to detect ambient muon flux perturbations if the storm is large and intense enough. This paper presents results from a pilot field study that measured the atmospheric muon flux near tornadic storms during May 2025, including directional measurements of the muon flux near tornadic mesocyclones and a measurement of the muon flux near the base of a forming tornado.",
        "keywords": [
          "physics.ao-ph",
          "hep-ex"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20182v1",
        "authors": [
          "William Luszczak",
          "Jana Houser",
          "Matt Kauer"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "hep-ex"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.446223",
      "entities": [
        "Atmospheric Muon Measurements Near",
        "Central Plains Tornadoes",
        "Tornadic Storms",
        "MIT",
        "DOE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20136v1",
      "title": "Physics-informed deep learning links geodetic data and fault friction",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20136v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "Fault slip modeling, based on laboratory-derived friction laws, has significantly enhanced our understanding of fault mechanics. Agreement between model predictions and observations supports the hypothesis that observed slip diversity, including fast earthquakes and slow transient slips (Slow Slip Events; SSEs), originates from frictional heterogeneity. However, quantitative assessments of frictional heterogeneity from geodetic observations while fully incorporating fault mechanics are lacking due to the difficulties of high-dimensional optimization. In this study, we aim to address this gap using Physics-Informed Neural Networks (PINNs) to link frictional heterogeneity with geodetic observations. PINNs employ a neural network to represent the spatially variable frictional properties, making their estimation feasible. Targeting the 2010 Bungo SSE in southwest Japan, our estimation reveals heterogeneous friction coinciding with localized SSE nucleation in southwest Shikoku, and subsequent westward propagation. The calculated fault slip of SSE successfully reproduces the spatio-temporal pattern of observed surface displacements. This PINN-based inversion provides a mechanically consistent fault slip model validated through quantitative comparison with observations. Furthermore, we predict the future fault slip evolution, demonstrating the importance of assimilating observations spanning multiple SSE cycles. Our results demonstrate the potential of PINN for advancing understanding of fault mechanics and enabling physics-based fault slip forecasting.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20136v1",
        "authors": [
          "Rikuto Fukushima",
          "Masayuki Kano",
          "Kazuro Hirahara"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.446461",
      "entities": [
        "Informed Neural Networks",
        "Slow Slip Events",
        "Neural Network",
        "Deep Learning",
        "Laboratory",
        "Agreement",
        "PINN",
        "SSE",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20123v1",
      "title": "Optimal illness policy for an unethical daycare center",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20123v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "While businesses are typically more profitable if their workers and communities are minimally exposed to diseases, the same is not true for daycare centers. Here it is shown that a daycare center could maximize its profits by maintaining a population of sick children within the center, with the intention to infect more children who then do not attend. Through a modification of the Susceptible-Infected-Recovered (SIR) model for disease spread we find the optimal number of sick children who should be kept within the center to maximize profits. We show that as disease infectiousness increases, the optimal attendance rate of sick children approaches zero, while the potential profit increases.",
        "keywords": [
          "q-bio.PE",
          "math.DS"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20123v1",
        "authors": [
          "Lauren D Smith"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "math.DS"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.446590",
      "entities": [
        "Policy",
        "WHO",
        "SIR",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20049v1",
      "title": "Evolving beyond collapse: An adaptive particle batch smoother for cryospheric data assimilation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20049v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "We present a new adaptive particle-based data assimilation scheme for cryospheric applications that leverages promising developments in importance sampling. The proposed approach seeks to combine some of the advantages of two widely used classes of schemes: particle methods and iterative ensemble Kalman methods. Specifically, it extends the PBS that is commonly used in cryospheric data assimilation, with the AMIS algorithm. This adaptive formulation transforms the PBS into an iterative scheme with improved resilience against ensemble collapse and the ability to implement early-stopping strategies. As such, computational cost is automatically adapted to the complexity of the problem at hand, even down to the grid-cell and water year level in distributed multiyear simulations. In homage to the schemes that it builds on, we coin this new algorithm the Adaptive Particle Batch Smoother (AdaPBS) and we test it across a range of scenarios. First, we conducted an intercomparison of some of the most commonly used cryospheric data assimilation algorithms using MCMC simulation as a costly gold-standard benchmark in a simplified temperature index model assimilating snow depth observations. We further evaluated AdaPBS by assimilating snow depth observations from the ESMSnowMIP project at 6 different sites spanning 3 continents, using an ensemble of simulations generated with the more complex FSM2. Our results demonstrate that AdaPBS is a robust and reliable tool, outperforming or at least matching the performance of other commonly used algorithms and successfully handling complex cases with dense observational datasets. All experiments were carried out using the open-source MuSA toolbox, which now includes AdaPBS and MCMC among the growing list of available cryospheric data assimilation methods.",
        "keywords": [
          "physics.ao-ph",
          "physics.geo-ph",
          "physics.data-an"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20049v1",
        "authors": [
          "Kristoffer Aalstad",
          "Esteban Alonso-González",
          "Norbert Pirk"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "physics.geo-ph",
          "physics.data-an"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.446875",
      "entities": [
        "Adaptive Particle Batch Smoother",
        "Standard",
        "MCMC",
        "AMIS",
        "PBS",
        "NSF",
        "AI"
      ]
    },
    {
      "id": "arxiv-2601.19681v1",
      "title": "Long-term evolution of regulatory DNA sequences. Part 1: Simulations on global, biophysically-realistic genotype-phenotype maps",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19681v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "Promoters and enhancers are cis-regulatory elements (CREs), DNA sequences that bind transcription factor (TF) proteins to up- or down-regulate target genes. Decades-long efforts yielded TF-DNA interaction models that predict how strongly an individual TF binds arbitrary DNA sequences and how individual binding events on the CRE combine to affect gene expression. These insights can be synthesized into a global, biophysically-realistic, and quantitative genotype-phenotype (GP) map for gene regulation, a \"holy grail\" for the application of evolutionary theory. A global map provides a rare opportunity to simulate long-term evolution of regulatory sequences and pose several fundamental questions: How long does it take to evolve CREs de novo? How many non-trivial regulatory functions exist in sequence space? How connected are they? For which regulatory architecture is CRE evolution most rapid and evolvable? In this article, the first of a two-part series, we briefly review the pertinent modeling and simulation efforts for a unique system that enables close, quantitative, and mechanistic links between biophysics, as well as systems, synthetic, and evolutionary biology.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19681v1",
        "authors": [
          "Elia Mascolo",
          "Réka Borbély",
          "Santiago Herrera-Álvarez"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.447072",
      "entities": [
        "Regulation",
        "NIST",
        "Act",
        "DOE",
        "DNA",
        "CRE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.19298v1",
      "title": "Spread/Error relationship and spatial error structure of precipitation ensemble nowcasting: Comparison of STEPS and generative AI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19298v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "The predictability of the generative AI-based nowcasting model LDCast (trained on another region) is evaluated over Belgium, together with the pysteps implementation of the nowcasting algorithm STEPS. STEPS and LDCast are slightly underdispersive, but the ensemble spread provides an estimation of the error at almost all scales. Both models adapt the properties of their ensembles to the type of event, either convective or stratiform. The spatial scores of the STEPS and LDCast ensembles are compared with those of surrogate ensembles having some key properties, revealing that both STEPS and LDCast have very little ability to spatially localise the ensemble mean error vector through their ensemble members. This suggests that the content of STEPS and LDCast ensembles is informative in terms of statistics, but not in terms of dynamics.",
        "keywords": [
          "physics.geo-ph",
          "physics.ao-ph",
          "nlin.CD"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19298v1",
        "authors": [
          "Martin Bonte",
          "Lesley De Cruz",
          "Fabian Debal"
        ],
        "arxiv_categories": [
          "physics.geo-ph",
          "physics.ao-ph",
          "nlin.CD"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.447235",
      "entities": [
        "STEPS",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2601.19289v1",
      "title": "Continued activity of the 25th cycle: largest in 20 years. Ground-level enhancement and Forbush decrease",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19289v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "After a very calm 24th solar activity cycle, the 25th cycle has already seen several interesting events. A Ground Level Enhancement GLE77 was observed on 11 November 2025 following an X5.1 class solar flare. A strong Forbush decrease occurred on 19 and 20 January 2026 during one of the most intense geomagnetic storms of Solar Cycle 25. Events were recorded coherently by the global neutron monitor network and by SEVAN detectors at multiple altitudes. Using spectrometric capabilities, we reconstruct energy spectra of missing neutrons and muons during the FD and compare them with corresponding spectra measured during GLE77. The analysis demonstrates that FD and GLE signatures are intrinsically asymmetric. FDs selectively suppress the preexisting galactic cosmic ray population, whereas GLEs introduce an additional, harder particle component. Neutron and muon channels exhibit markedly different spectral behavior, particularly at higher deposited energies, reflecting their sensitivity to different primary energy ranges. These results show that combined NM and SEVAN observations provide robust, complementary diagnostics of rigidity dependent cosmic ray modulation during extreme heliospheric disturbances.",
        "keywords": [
          "astro-ph.SR",
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19289v1",
        "authors": [
          "B. Sargsyan",
          "A. Chilingarian"
        ],
        "arxiv_categories": [
          "astro-ph.SR",
          "physics.ao-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-01-30T18:50:47.447452",
      "entities": [
        "Ground Level Enhancement",
        "Solar Cycle",
        "Solar",
        "SEVAN",
        "Act",
        "GLE",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22104v1",
      "title": "Social Media Data for Population Mapping: A Bayesian Approach to Address Representativeness and Privacy Challenges",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22104v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Accurate and timely population data are essential for disaster response and humanitarian planning, but traditional censuses often cannot capture rapid demographic changes. Social media data offer a promising alternative for dynamic population monitoring, but their representativeness remains poorly understood and stringent privacy requirements limit their reliability. Here, we address these limitations in the context of the Philippines by calibrating Facebook user counts with the country's 2020 census figures. First, we find that differential privacy techniques commonly applied to social media-based population datasets disproportionately mask low-population areas. To address this, we propose a Bayesian imputation approach to recover missing values, restoring data coverage for $5.5\\%$ of rural areas. Further, using the imputed social media data and leveraging predictors such as urbanisation level, demographic composition, and socio-economic status, we develop a statistical model for the proportion of Facebook users in each municipality, which links observed Facebook user numbers to the true population levels. Out-of-sample validation demonstrates strong result generalisability, with errors as low as ${\\approx}18\\%$ and ${\\approx}24\\%$ for urban and rural Facebook user proportions, respectively. We further demonstrate that accounting for overdispersion and spatial correlations in the data is crucial to obtain accurate estimates and appropriate credible intervals. Crucially, as predictors change over time, the models can be used to regularly update the population predictions, providing a dynamic complement to census-based estimates. These results have direct implications for humanitarian response in disaster-prone regions and offer a general framework for using biased social media signals to generate reliable and timely population data.",
        "keywords": [
          "stat.ME",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22104v1",
        "authors": [
          "Paolo Andrich",
          "Shengjie Lai",
          "Halim Jun"
        ],
        "arxiv_categories": [
          "stat.ME",
          "stat.AP"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.349364",
      "entities": [
        "Privacy Challenges Accurate",
        "Address Representativeness",
        "Population Mapping",
        "Bayesian Approach",
        "Social Media Data",
        "Framework",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22093v1",
      "title": "Investigating Associational Biases in Inter-Model Communication of Large Generative Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22093v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22093v1",
        "authors": [
          "Fethiye Irmak Dogan",
          "Yuval Weiss",
          "Kajal Patel"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.349616",
      "entities": [
        "Investigating Associational Biases",
        "Large Generative Models Social",
        "Model Communication",
        "PHASE",
        "Act",
        "RAF",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22082v1",
      "title": "Auditorily Embodied Conversational Agents: Effects of Spatialization and Situated Audio Cues on Presence and Social Perception",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22082v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Embodiment can enhance conversational agents, such as increasing their perceived presence. This is typically achieved through visual representations of a virtual body; however, visual modalities are not always available, such as when users interact with agents using headphones or display-less glasses. In this work, we explore auditory embodiment. By introducing auditory cues of bodily presence - through spatially localized voice and situated Foley audio from environmental interactions - we investigate how audio alone can convey embodiment and influence perceptions of a conversational agent. We conducted a 2 (spatialization: monaural vs. spatialized) x 2 (Foley: none vs. Foley) within-subjects study, where participants (n=24) engaged in conversations with agents. Our results show that spatialization and Foley increase co-presence, but reduce users' perceptions of the agent's attention and other social attributes.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22082v1",
        "authors": [
          "Yi Fei Cheng",
          "Jarod Bloch",
          "Alexander Wang"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.349783",
      "entities": [
        "Auditorily Embodied Conversational Agents",
        "Social Perception Embodiment",
        "Situated Audio Cues",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2601.22081v1",
      "title": "Accessibility-Driven Information Transformations in Mixed-Visual Ability Work Teams",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22081v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Blind and low-vision (BLV) employees in mixed-visual ability teams often encounter information (e.g., PDFs, diagrams) in inaccessible formats. To enable teamwork, teams must transform these representations by modifying or re-creating them into accessible forms. However, these transformations are frequently overlooked, lack infrastructural support, and cause additional labour. To design systems that move beyond one-off accommodations to effective mixed-ability collaboration, we need a deeper understanding of the representations, their transformations and how they occur. We conducted a week-long diary study with follow-up interviews with 23 BLV and sighted professionals from five legal, non-profit, and consulting teams, documenting 36 transformation cases. Our analysis characterizes how teams perform representational transformations for accessibility: how they are triggered proactively or reactively, how they simplify or enhance, and four common patterns in which workers coordinate with each other to address representational incompatibility. Our findings uncover opportunities for designing systems that can better support mixed-visual ability work.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22081v1",
        "authors": [
          "Yichun Zhao",
          "Miguel A. Nacenta",
          "Mahadeo A. Sukhai"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.350044",
      "entities": [
        "Driven Information Transformations",
        "Visual Ability Work Teams",
        "Act",
        "NSF",
        "BLV",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22013v1",
      "title": "Vidmento: Creating Video Stories Through Context-Aware Expansion With Generative Video",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22013v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Video storytelling is often constrained by available material, limiting creative expression and leaving undesired narrative gaps. Generative video offers a new way to address these limitations by augmenting captured media with tailored visuals. To explore this potential, we interviewed eight video creators to identify opportunities and challenges in integrating generative video into their workflows. Building on these insights and established filmmaking principles, we developed Vidmento, a tool for authoring hybrid video stories that combine captured and generated media through context-aware expansion. Vidmento surfaces opportunities for story development, generates clips that blend stylistically and narratively with surrounding media, and provides controls for refinement. In a study with 12 creators, Vidmento supported narrative development and exploration by systematically expanding initial materials with generative media, enabling expressive video storytelling aligned with creative intent. We highlight how creators bridge story gaps with generative content and where they find this blending capability most valuable.",
        "keywords": [
          "cs.HC",
          "cs.AI",
          "cs.MM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22013v1",
        "authors": [
          "Catherine Yeh",
          "Anh Truong",
          "Mira Dontcheva"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.AI",
          "cs.MM"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.350300",
      "entities": [
        "Aware Expansion With Generative",
        "Creating Video Stories Through",
        "Video Video",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21977v1",
      "title": "From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21977v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based \"particles\" rather than cognitive \"agents\". To bridge this, we introduce \\textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \\textbf{Cognitive Friction} ($C_f$) it is possible to reveal \"Phantom Affordances\", i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21977v1",
        "authors": [
          "Javier Argota Sánchez-Vaquerizo",
          "Luis Borunda Monsivais"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.350566",
      "entities": [
        "Agentic Environmental Simulations",
        "Spatial Simulation Traditional",
        "Computational Fluid Dynamics",
        "Episodic Spatial Reasoning",
        "Phantom Affordances",
        "Cognitive Friction",
        "Large Multimodal",
        "From Particles",
        "Framework",
        "NIST",
        "Act",
        "IoT",
        "HCI",
        "AI"
      ]
    },
    {
      "id": "arxiv-2601.21965v1",
      "title": "Cognitive Load Estimation Using Brain Foundation Models and Interpretability for BCIs",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21965v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Accurately monitoring cognitive load in real time is critical for Brain-Computer Interfaces (BCIs) that adapt to user engagement and support personalized learning. Electroencephalography (EEG) offers a non-invasive, cost-effective modality for capturing neural activity, though traditional methods often struggle with cross-subject variability and task-specific preprocessing. We propose leveraging Brain Foundation Models (BFMs), large pre-trained neural networks, to extract generalizable EEG features for cognitive load estimation. We adapt BFMs for long-term EEG monitoring and show that fine-tuning a small subset of layers yields improved accuracy over the state-of-the-art. Despite their scale, BFMs allow for real-time inference with a longer context window. To address often-overlooked interpretability challenges, we apply Partition SHAP (SHapley Additive exPlanations) to quantify feature importance. Our findings reveal consistent emphasis on prefrontal regions linked to cognitive control, while longitudinal trends suggest learning progression. These results position BFMs as efficient and interpretable tools for continuous cognitive load monitoring in real-world BCIs.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21965v1",
        "authors": [
          "Deeksha M. Shama",
          "Dimitra Emmanouilidou",
          "Ivan J. Tashev"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.350838",
      "entities": [
        "Cognitive Load Estimation Using",
        "Brain Foundation Models",
        "Computer Interfaces",
        "Neural Network",
        "SHAP",
        "Wind",
        "Act",
        "EEG",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21963v1",
      "title": "Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21963v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Generative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Language Models (LLMs) and multimodal systems. Central to this work are our practical contributions: JudgeGPT, a platform for evaluating human perception of AI-generated news, and RogueGPT, a controlled stimulus generation engine for research. Together, these tools form an experimental pipeline for studying how humans perceive and detect AI-generated misinformation. Our findings show that detection capabilities have improved, but the competition between generation and detection continues. We discuss mitigation strategies including LLM-based detection, inoculation approaches, and the dual-use nature of generative AI. This work contributes to research addressing the adverse impacts of AI on information quality.",
        "keywords": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21963v1",
        "authors": [
          "Alexander Loth",
          "Martin Kappes",
          "Marc-Oliver Pahl"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.351078",
      "entities": [
        "Digital Ecosystems Generative",
        "Industrialized Deception",
        "Generated Misinformation",
        "Large Language Models",
        "Act",
        "MIT",
        "LLM",
        "GPT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21961v1",
      "title": "How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21961v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.",
        "keywords": [
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21961v1",
        "authors": [
          "Kuai Yu",
          "Naicheng Yu",
          "Han Wang"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.351446",
      "entities": [
        "Visual Attributes Influence Web",
        "User Interface Design Factors",
        "Comprehensive Evaluation",
        "Visual Attribute Factors",
        "Target Mention Rate",
        "Target Click Rate",
        "Act",
        "MIT",
        "VAF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21920v1",
      "title": "From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21920v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "In the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI's dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust'': the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21920v1",
        "authors": [
          "Upol Ehsan",
          "Samir Passi",
          "Koustuv Saha"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.351750",
      "entities": [
        "Addressing Asymptomatic",
        "Amplifier Paradox",
        "Dignified Human",
        "Interaction In",
        "From Future",
        "Framework",
        "Act",
        "IoT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21900v1",
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21900v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the \"locality hypothesis\", suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.MM",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21900v1",
        "authors": [
          "Chuancheng Shi",
          "Shangze Li",
          "Wenjun Lu"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.MM",
          "cs.CV"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.352041",
      "entities": [
        "Level Intervention Despite",
        "Large Foundation Models",
        "Robust Safety",
        "Framework",
        "Act",
        "FIS",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21839v1",
      "title": "Test-Time Compute Games",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21839v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Test-time compute has emerged as a promising strategy to enhance the reasoning abilities of large language models (LLMs). However, this strategy has in turn increased how much users pay cloud-based providers offering LLM-as-a-service, since providers charge users for the amount of test-time compute they use to generate an output. In our work, we show that the market of LLM-as-a-service is socially inefficient: providers have a financial incentive to increase the amount of test-time compute, even if this increase contributes little to the quality of the outputs. To address this inefficiency, we introduce a reverse second-price auction mechanism where providers bid their offered price and (expected) quality for the opportunity to serve a user, and users pay proportionally to the marginal value generated by the winning provider relative to the second-highest bidder. To illustrate and complement our theoretical results, we conduct experiments with multiple instruct models from the $\\texttt{Llama}$ and $\\texttt{Qwen}$ families, as well as reasoning models distilled from $\\texttt{DeepSeek-R1}$, on math and science benchmark datasets.",
        "keywords": [
          "cs.CY",
          "cs.GT",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21839v1",
        "authors": [
          "Ander Artola Velasco",
          "Dimitrios Rontogiannis",
          "Stratis Tsirtsis"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.GT",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.352289",
      "entities": [
        "Time Compute Games Test",
        "LLM",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21837v1",
      "title": "Trustworthy Intelligent Education: A Systematic Perspective on Progress, Challenges, and Future Directions",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21837v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "In recent years, trustworthiness has garnered increasing attention and exploration in the field of intelligent education, due to the inherent sensitivity of educational scenarios, such as involving minors and vulnerable groups, highly personalized learning data, and high-stakes educational outcomes. However, existing research either focuses on task-specific trustworthy methods without a holistic view of trustworthy intelligent education, or provides survey-level discussions that remain high-level and fragmented, lacking a clear and systematic categorization. To address these limitations, in this paper, we present a systematic and structured review of trustworthy intelligent education. Specifically, We first organize intelligent education into five representative task categories: learner ability assessment, learning resource recommendation, learning analytics, educational content understanding, and instructional assistance. Building on this task landscape, we review existing studies from five trustworthiness perspectives, including safety and privacy, robustness, fairness, explainability, and sustainability, and summarize and categorize the research methodologies and solution strategies therein. Finally, we summarize key challenges and discuss future research directions. This survey aims to provide a coherent reference framework and facilitate a clearer understanding of trustworthiness in intelligent education.",
        "keywords": [
          "cs.IR",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21837v1",
        "authors": [
          "Xiaoshan Yu",
          "Shangshang Yang",
          "Ziwen Wang"
        ],
        "arxiv_categories": [
          "cs.IR",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.352606",
      "entities": [
        "Trustworthy Intelligent Education",
        "Systematic Perspective",
        "Future Directions In",
        "Framework",
        "Intel",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21815v1",
      "title": "Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21815v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Understanding how media rhetoric shapes audience engagement is crucial in the attention economy. This study examines how moral emotional framing by mainstream news channels on YouTube influences user behavior across Korea and the United States. To capture the platform's multimodal nature, combining thumbnail images and video titles, we develop a multimodal moral emotion classifier by fine tuning a vision language model. The model is trained on human annotated multimodal datasets in both languages and applied to approximately 400,000 videos from major news outlets. We analyze engagement levels including views, likes, and comments, representing increasing degrees of commitment. The results show that other condemning rhetoric expressions of moral outrage that criticize others morally consistently increase all forms of engagement across cultures, with effects ranging from passive viewing to active commenting. These findings suggest that moral outrage is a particularly effective emotional strategy, attracting not only attention but also active participation. We discuss concerns about the potential misuse of other condemning rhetoric, as such practices may deepen polarization by reinforcing in group and out group divisions. To facilitate future research and ensure reproducibility, we publicly release our Korean and English multimodal moral emotion classifiers.",
        "keywords": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21815v1",
        "authors": [
          "Seongchan Park",
          "Jaehong Kim",
          "Hyeonseung Kim"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.352942",
      "entities": [
        "Moral Outrage Shapes Commitments",
        "Multimodal Moral Emotions",
        "Beyond Attention",
        "United States",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21791v1",
      "title": "Preliminary Results of a Scoping Review on Assistive Technologies for Adults with ADHD",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21791v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Attention Deficit Hyperactivity Disorder (ADHD), characterized by inattention, hyperactivity, and impulsivity, is prevalent in the adult population. Long perceived and treated as a childhood condition, ADHD and its characteristics nonetheless impact a significant portion of adults today. In contrast to children with ADHD, adults with ADHD face unique challenges in the workplace and in higher education. In this work-in-progress paper, we present a scoping review as a foundation to understand and explore existing technology-based approaches to support adults with ADHD. In total, our search returned 3,538 papers upon which we selected, based on PRISMA-ScR, a total of 46 papers for in-depth analysis. Our initial findings highlight that most papers take on a therapeutic or intervention perspective instead of a more positive support perspective. Our analysis also found a tremendous increase in recent papers on the topic, which highlights that more and more researchers are becoming aware of the need to address ADHD with adults. For the future, we aim to further analyze the corpus and identify research gaps and potentials for further development of ADHD assistive technologies.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21791v1",
        "authors": [
          "Valerie Tan",
          "Luisa Jost",
          "Jens Gerken"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.353226",
      "entities": [
        "Attention Deficit Hyperactivity Disorder",
        "Assistive Technologies",
        "Preliminary Results",
        "Scoping Review",
        "PRISMA",
        "ADHD",
        "Act",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21650v1",
      "title": "When Life Gives You AI, Will You Turn It Into A Market for Lemons? Understanding How Information Asymmetries About AI System Capabilities Affect Market Outcomes and Adoption",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21650v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "AI consumer markets are characterized by severe buyer-supplier market asymmetries. Complex AI systems can appear highly accurate while making costly errors or embedding hidden defects. While there have been regulatory efforts surrounding different forms of disclosure, large information gaps remain. This paper provides the first experimental evidence on the important role of information asymmetries and disclosure designs in shaping user adoption of AI systems. We systematically vary the density of low-quality AI systems and the depth of disclosure requirements in a simulated AI product market to gauge how people react to the risk of accidentally relying on a low-quality AI system. Then, we compare participants' choices to a rational Bayesian model, analyzing the degree to which partial information disclosure can improve AI adoption. Our results underscore the deleterious effects of information asymmetries on AI adoption, but also highlight the potential of partial disclosure designs to improve the overall efficiency of human decision-making.",
        "keywords": [
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21650v1",
        "authors": [
          "Alexander Erlei",
          "Federico Cau",
          "Radoslav Georgiev"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.353488",
      "entities": [
        "Understanding How Information Asymmetries",
        "System Capabilities Affect Market",
        "When Life Gives You",
        "Will You Turn It",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21648v1",
      "title": "CAF-Mamba: Mamba-Based Cross-Modal Adaptive Attention Fusion for Multimodal Depression Detection",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21648v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Depression is a prevalent mental health disorder that severely impairs daily functioning and quality of life. While recent deep learning approaches for depression detection have shown promise, most rely on limited feature types, overlook explicit cross-modal interactions, and employ simple concatenation or static weighting for fusion. To overcome these limitations, we propose CAF-Mamba, a novel Mamba-based cross-modal adaptive attention fusion framework. CAF-Mamba not only captures cross-modal interactions explicitly and implicitly, but also dynamically adjusts modality contributions through a modality-wise attention mechanism, enabling more effective multimodal fusion. Experiments on two in-the-wild benchmark datasets, LMVD and D-Vlog, demonstrate that CAF-Mamba consistently outperforms existing methods and achieves state-of-the-art performance.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21648v1",
        "authors": [
          "Bowen Zhou",
          "Marc-André Fiedler",
          "Ayoub Al-Hamadi"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.CV"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.353695",
      "entities": [
        "Multimodal Depression Detection Depression",
        "Modal Adaptive Attention Fusion",
        "Deep Learning",
        "Based Cross",
        "Framework",
        "Fusion",
        "LMVD",
        "Act",
        "CAF",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21631v1",
      "title": "Turning Language Model Training from Black Box into a Sandbox",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21631v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Most classroom engagements with generative AI focus on prompting pre-trained models, leaving the role of training data and model mechanics opaque. We developed a browser-based tool that allows students to train a small transformer language model entirely on their own device, making the training process visible. In a CS1 course, 162 students completed pre- and post-test explanations of why language models sometimes produce incorrect or strange output. After a brief hands-on training activity, students' explanations shifted significantly from anthropomorphic and misconceived accounts toward data- and model-based reasoning. The results suggest that enabling learners to directly observe training can support conceptual understanding of the data-driven nature of language models and model training, even within a short intervention. For K-12 AI literacy and AI education research, the study findings suggest that enabling students to train - and not only prompt - language models can shift how they think about AI.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21631v1",
        "authors": [
          "Nicolas Pope",
          "Matti Tedre"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.353940",
      "entities": [
        "Turning Language Model Training",
        "Sandbox Most",
        "Transformer",
        "Black Box",
        "K-12",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21605v1",
      "title": "Age Matters: Analyzing Age-Related Discussions in App Reviews",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21605v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "In recent years, mobile applications have become indispensable tools for managing various aspects of life. From enhancing productivity to providing personalized entertainment, mobile apps have revolutionized people's daily routines. Despite this rapid growth and popularity, gaps remain in how these apps address the needs of users from different age groups. Users of varying ages face distinct challenges when interacting with mobile apps, from younger users dealing with inappropriate content to older users having difficulty with usability due to age-related vision and cognition impairments. Although there have been initiatives to create age-inclusive apps, a limited understanding of user perspectives on age-related issues may hinder developers from recognizing specific challenges and implementing effective solutions. In this study, we explore age discussions in app reviews to gain insights into how mobile apps should cater to users across different age groups.We manually curated a dataset of 4,163 app reviews from the Google Play Store and identified 1,429 age-related reviews and 2,734 non-age-related reviews. We employed eight machine learning, deep learning, and large language models to automatically detect age discussions, with RoBERTa performing the best, achieving a precision of 92.46%. Additionally, a qualitative analysis of the 1,429 age-related reviews uncovers six dominant themes reflecting user concerns.",
        "keywords": [
          "cs.SE",
          "cs.HC",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21605v1",
        "authors": [
          "Shashiwadana Nirmania",
          "Garima Sharma",
          "Hourieh Khalajzadeh"
        ],
        "arxiv_categories": [
          "cs.SE",
          "cs.HC",
          "cs.LG"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.354250",
      "entities": [
        "Related Discussions",
        "Google Play Store",
        "Machine Learning",
        "App Reviews In",
        "Analyzing Age",
        "Deep Learning",
        "Age Matters",
        "Google",
        "BERT",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21521v1",
      "title": "A Unified SPD Token Transformer Framework for EEG Classification: Systematic Comparison of Geometric Embeddings",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21521v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Spatial covariance matrices of EEG signals are Symmetric Positive Definite (SPD) and lie on a Riemannian manifold, yet the theoretical connection between embedding geometry and optimization dynamics remains unexplored. We provide a formal analysis linking embedding choice to gradient conditioning and numerical stability for SPD manifolds, establishing three theoretical results: (1) BWSPD's $\\sqrtκ$ gradient conditioning (vs $κ$ for Log-Euclidean) via Daleckii-Kreĭn matrices provides better gradient conditioning on high-dimensional inputs ($d \\geq 22$), with this advantage reducing on low-dimensional inputs ($d \\leq 8$) where eigendecomposition overhead dominates; (2) Embedding-Space Batch Normalization (BN-Embed) approximates Riemannian normalization up to $O(\\varepsilon^2)$ error, yielding $+26\\%$ accuracy on 56-channel ERP data but negligible effect on 8-channel SSVEP data, matching the channel-count-dependent prediction; (3) bi-Lipschitz bounds prove BWSPD tokens preserve manifold distances with distortion governed solely by the condition ratio $κ$. We validate these predictions via a unified Transformer framework comparing BWSPD, Log-Euclidean, and Euclidean embeddings within identical architecture across 1,500+ runs on three EEG paradigms (motor imagery, ERP, SSVEP; 36 subjects). Our Log-Euclidean Transformer achieves state-of-the-art performance on all datasets, substantially outperforming classical Riemannian classifiers and recent SPD baselines, while BWSPD offers competitive accuracy with similar training time.",
        "keywords": [
          "cs.HC",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21521v1",
        "authors": [
          "Chi-Sheng Chen",
          "En-Jui Kuo",
          "Guan-Ying Chen"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.LG"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-01-30T18:50:50.355082",
      "entities": [
        "Geometric Embeddings Spatial",
        "Symmetric Positive Definite",
        "Token Transformer Framework",
        "Space Batch Normalization",
        "Systematic Comparison",
        "Euclidean Transformer",
        "Transformer",
        "Framework",
        "Our Log",
        "BWSPD",
        "SSVEP",
        "NSF",
        "SPD",
        "ERP",
        "EEG"
      ]
    },
    {
      "id": "arxiv-2601.22093v1",
      "title": "Investigating Associational Biases in Inter-Model Communication of Large Generative Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22093v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22093v1",
        "authors": [
          "Fethiye Irmak Dogan",
          "Yuval Weiss",
          "Kajal Patel"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.726443",
      "entities": [
        "Investigating Associational Biases",
        "Large Generative Models Social",
        "Model Communication",
        "PHASE",
        "Act",
        "RAF",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21977v1",
      "title": "From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21977v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based \"particles\" rather than cognitive \"agents\". To bridge this, we introduce \\textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \\textbf{Cognitive Friction} ($C_f$) it is possible to reveal \"Phantom Affordances\", i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21977v1",
        "authors": [
          "Javier Argota Sánchez-Vaquerizo",
          "Luis Borunda Monsivais"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.726667",
      "entities": [
        "Agentic Environmental Simulations",
        "Spatial Simulation Traditional",
        "Computational Fluid Dynamics",
        "Episodic Spatial Reasoning",
        "Phantom Affordances",
        "Cognitive Friction",
        "Large Multimodal",
        "From Particles",
        "Framework",
        "NIST",
        "Act",
        "IoT",
        "HCI",
        "AI"
      ]
    },
    {
      "id": "arxiv-2601.21963v1",
      "title": "Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21963v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Generative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Language Models (LLMs) and multimodal systems. Central to this work are our practical contributions: JudgeGPT, a platform for evaluating human perception of AI-generated news, and RogueGPT, a controlled stimulus generation engine for research. Together, these tools form an experimental pipeline for studying how humans perceive and detect AI-generated misinformation. Our findings show that detection capabilities have improved, but the competition between generation and detection continues. We discuss mitigation strategies including LLM-based detection, inoculation approaches, and the dual-use nature of generative AI. This work contributes to research addressing the adverse impacts of AI on information quality.",
        "keywords": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21963v1",
        "authors": [
          "Alexander Loth",
          "Martin Kappes",
          "Marc-Oliver Pahl"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.726858",
      "entities": [
        "Digital Ecosystems Generative",
        "Industrialized Deception",
        "Generated Misinformation",
        "Large Language Models",
        "Act",
        "MIT",
        "LLM",
        "GPT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21920v1",
      "title": "From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21920v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "In the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI's dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust'': the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21920v1",
        "authors": [
          "Upol Ehsan",
          "Samir Passi",
          "Koustuv Saha"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.727104",
      "entities": [
        "Addressing Asymptomatic",
        "Amplifier Paradox",
        "Dignified Human",
        "Interaction In",
        "From Future",
        "Framework",
        "Act",
        "IoT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21900v1",
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21900v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the \"locality hypothesis\", suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.MM",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21900v1",
        "authors": [
          "Chuancheng Shi",
          "Shangze Li",
          "Wenjun Lu"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.MM",
          "cs.CV"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.727337",
      "entities": [
        "Level Intervention Despite",
        "Large Foundation Models",
        "Robust Safety",
        "Framework",
        "Act",
        "FIS",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21839v1",
      "title": "Test-Time Compute Games",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21839v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Test-time compute has emerged as a promising strategy to enhance the reasoning abilities of large language models (LLMs). However, this strategy has in turn increased how much users pay cloud-based providers offering LLM-as-a-service, since providers charge users for the amount of test-time compute they use to generate an output. In our work, we show that the market of LLM-as-a-service is socially inefficient: providers have a financial incentive to increase the amount of test-time compute, even if this increase contributes little to the quality of the outputs. To address this inefficiency, we introduce a reverse second-price auction mechanism where providers bid their offered price and (expected) quality for the opportunity to serve a user, and users pay proportionally to the marginal value generated by the winning provider relative to the second-highest bidder. To illustrate and complement our theoretical results, we conduct experiments with multiple instruct models from the $\\texttt{Llama}$ and $\\texttt{Qwen}$ families, as well as reasoning models distilled from $\\texttt{DeepSeek-R1}$, on math and science benchmark datasets.",
        "keywords": [
          "cs.CY",
          "cs.GT",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21839v1",
        "authors": [
          "Ander Artola Velasco",
          "Dimitrios Rontogiannis",
          "Stratis Tsirtsis"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.GT",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.727537",
      "entities": [
        "Time Compute Games Test",
        "LLM",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21837v1",
      "title": "Trustworthy Intelligent Education: A Systematic Perspective on Progress, Challenges, and Future Directions",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21837v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "In recent years, trustworthiness has garnered increasing attention and exploration in the field of intelligent education, due to the inherent sensitivity of educational scenarios, such as involving minors and vulnerable groups, highly personalized learning data, and high-stakes educational outcomes. However, existing research either focuses on task-specific trustworthy methods without a holistic view of trustworthy intelligent education, or provides survey-level discussions that remain high-level and fragmented, lacking a clear and systematic categorization. To address these limitations, in this paper, we present a systematic and structured review of trustworthy intelligent education. Specifically, We first organize intelligent education into five representative task categories: learner ability assessment, learning resource recommendation, learning analytics, educational content understanding, and instructional assistance. Building on this task landscape, we review existing studies from five trustworthiness perspectives, including safety and privacy, robustness, fairness, explainability, and sustainability, and summarize and categorize the research methodologies and solution strategies therein. Finally, we summarize key challenges and discuss future research directions. This survey aims to provide a coherent reference framework and facilitate a clearer understanding of trustworthiness in intelligent education.",
        "keywords": [
          "cs.IR",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21837v1",
        "authors": [
          "Xiaoshan Yu",
          "Shangshang Yang",
          "Ziwen Wang"
        ],
        "arxiv_categories": [
          "cs.IR",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.727791",
      "entities": [
        "Trustworthy Intelligent Education",
        "Systematic Perspective",
        "Future Directions In",
        "Framework",
        "Intel",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21815v1",
      "title": "Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21815v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Understanding how media rhetoric shapes audience engagement is crucial in the attention economy. This study examines how moral emotional framing by mainstream news channels on YouTube influences user behavior across Korea and the United States. To capture the platform's multimodal nature, combining thumbnail images and video titles, we develop a multimodal moral emotion classifier by fine tuning a vision language model. The model is trained on human annotated multimodal datasets in both languages and applied to approximately 400,000 videos from major news outlets. We analyze engagement levels including views, likes, and comments, representing increasing degrees of commitment. The results show that other condemning rhetoric expressions of moral outrage that criticize others morally consistently increase all forms of engagement across cultures, with effects ranging from passive viewing to active commenting. These findings suggest that moral outrage is a particularly effective emotional strategy, attracting not only attention but also active participation. We discuss concerns about the potential misuse of other condemning rhetoric, as such practices may deepen polarization by reinforcing in group and out group divisions. To facilitate future research and ensure reproducibility, we publicly release our Korean and English multimodal moral emotion classifiers.",
        "keywords": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21815v1",
        "authors": [
          "Seongchan Park",
          "Jaehong Kim",
          "Hyeonseung Kim"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.728034",
      "entities": [
        "Moral Outrage Shapes Commitments",
        "Multimodal Moral Emotions",
        "Beyond Attention",
        "United States",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21648v1",
      "title": "CAF-Mamba: Mamba-Based Cross-Modal Adaptive Attention Fusion for Multimodal Depression Detection",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21648v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Depression is a prevalent mental health disorder that severely impairs daily functioning and quality of life. While recent deep learning approaches for depression detection have shown promise, most rely on limited feature types, overlook explicit cross-modal interactions, and employ simple concatenation or static weighting for fusion. To overcome these limitations, we propose CAF-Mamba, a novel Mamba-based cross-modal adaptive attention fusion framework. CAF-Mamba not only captures cross-modal interactions explicitly and implicitly, but also dynamically adjusts modality contributions through a modality-wise attention mechanism, enabling more effective multimodal fusion. Experiments on two in-the-wild benchmark datasets, LMVD and D-Vlog, demonstrate that CAF-Mamba consistently outperforms existing methods and achieves state-of-the-art performance.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21648v1",
        "authors": [
          "Bowen Zhou",
          "Marc-André Fiedler",
          "Ayoub Al-Hamadi"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.CV"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.728201",
      "entities": [
        "Multimodal Depression Detection Depression",
        "Modal Adaptive Attention Fusion",
        "Deep Learning",
        "Based Cross",
        "Framework",
        "Fusion",
        "LMVD",
        "Act",
        "CAF",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21631v1",
      "title": "Turning Language Model Training from Black Box into a Sandbox",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21631v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Most classroom engagements with generative AI focus on prompting pre-trained models, leaving the role of training data and model mechanics opaque. We developed a browser-based tool that allows students to train a small transformer language model entirely on their own device, making the training process visible. In a CS1 course, 162 students completed pre- and post-test explanations of why language models sometimes produce incorrect or strange output. After a brief hands-on training activity, students' explanations shifted significantly from anthropomorphic and misconceived accounts toward data- and model-based reasoning. The results suggest that enabling learners to directly observe training can support conceptual understanding of the data-driven nature of language models and model training, even within a short intervention. For K-12 AI literacy and AI education research, the study findings suggest that enabling students to train - and not only prompt - language models can shift how they think about AI.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21631v1",
        "authors": [
          "Nicolas Pope",
          "Matti Tedre"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.728384",
      "entities": [
        "Turning Language Model Training",
        "Sandbox Most",
        "Transformer",
        "Black Box",
        "K-12",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21512v1",
      "title": "MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21512v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.",
        "keywords": [
          "cs.IR",
          "cs.CL",
          "cs.DB",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21512v1",
        "authors": [
          "Serry Sibaee",
          "Yasser Alhabashi",
          "Nadia Sibai"
        ],
        "arxiv_categories": [
          "cs.IR",
          "cs.CL",
          "cs.DB",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.728587",
      "entities": [
        "Unified Reverse Arabic Dictionary",
        "Domain Unified Reverse Arabic",
        "Dictionary Dataset Arabic",
        "Scale Multi",
        "Standard",
        "MURAD",
        "Meta",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21365v1",
      "title": "Small models, big threats: Characterizing safety challenges from low-compute AI models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21365v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Artificial intelligence (AI) systems are revolutionizing fields such as medicine, drug discovery, and materials science; however, many technologists and policymakers are also concerned about the technology's risks. To date, most concrete policies around AI governance have focused on managing AI risk by considering the amount of compute required to operate or build a given AI system. However, low-compute AI systems are becoming increasingly more performant - and more dangerous. Driven by agentic workflows, parameter quantization, and other model compression techniques, capabilities once only achievable on frontier-level systems have diffused into low-resource models deployable on consumer devices. In this report, we profile this trend by downloading historical benchmark performance data for over 5,000 large language models (LLMs) hosted on HuggingFace, noting the model size needed to achieve competitive LLM benchmarks has decreased by more than 10X over the past year. We then simulate the computational resources needed for an actor to launch a series of digital societal harm campaigns - such as disinformation botnets, sexual extortion schemes, voice-cloning fraud, and others - using low-compute open-source models and find nearly all studied campaigns can easily be executed on consumer-grade hardware. This position paper argues that protection measures for high-compute models leave serious security holes for their low-compute counterparts, meaning it is urgent both policymakers and technologists make greater efforts to understand and address this emerging class of threats.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21365v1",
        "authors": [
          "Prateek Puri"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.728854",
      "entities": [
        "Artificial Intelligence",
        "Policy",
        "Intel",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21212v1",
      "title": "Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21212v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21212v1",
        "authors": [
          "Xixian Yong",
          "Peilin Sun",
          "Zihe Wang"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.729114",
      "entities": [
        "Towards Customized Urban Planning",
        "Reinforcement Learning Effective",
        "Large Language Model Empowered",
        "Deep Reinforcement Learning",
        "Deep Learning",
        "Framework",
        "Policy",
        "Intel",
        "Act",
        "MIT",
        "LLM",
        "DRL",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21165v1",
      "title": "FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21165v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research. FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21165v1",
        "authors": [
          "Miles Wang",
          "Robi Lin",
          "Kat Hu"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.729345",
      "entities": [
        "Level Scientific Tasks We",
        "Perform Expert",
        "All Olympiad",
        "All Research",
        "For Research",
        "Framework",
        "Standard",
        "Act",
        "IBO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21028v1",
      "title": "\"Unlimited Realm of Exploration and Experimentation\": Methods and Motivations of AI-Generated Sexual Content Creators",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21028v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "AI-generated media is radically changing the way content is both consumed and produced on the internet, and in no place is this potentially more visible than in sexual content. AI-generated sexual content (AIG-SC) is increasingly enabled by an ecosystem of individual AI developers, specialized third-party applications, and foundation model providers. AIG-SC raises a number of concerns from old debates about the line between pornography and obscenity, to newer debates about fair use and labor displacement (in this case, of sex workers), and spurred new regulations to curb the spread of non-consensual intimate imagery (NCII) created using the same technology used to create AIG-SC. However, despite the growing prevalence of AIG-SC, little is known about its creators, their motivations, and what types of content they produce. To inform effective governance in this space, we perform an in-depth study to understand what AIG-SC creators make, along with how and why they make it. Interviews of 28 AIG-SC creators, ranging from hobbyists to entrepreneurs to those who moderate communities of hundreds of thousands of other creators, reveal a wide spectrum of motivations, including sexual exploration, creative expression, technical experimentation, and in a handful of cases, the creation of NCII.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21028v1",
        "authors": [
          "Jaron Mink",
          "Lucy Qin",
          "Elissa M. Redmiles"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.729582",
      "entities": [
        "Generated Sexual Content Creators",
        "Unlimited Realm",
        "Regulation",
        "NCII",
        "WHO",
        "MIT",
        "AIG",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20920v1",
      "title": "Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20920v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20920v1",
        "authors": [
          "Vibhhu Sharma",
          "Thorsten Joachims",
          "Sarah Dean"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.729895",
      "entities": [
        "Quantifying Interaction Effects",
        "Meta",
        "ICML",
        "ICLR",
        "Act",
        "LLM",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2601.20848v1",
      "title": "Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20848v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.",
        "keywords": [
          "cs.IR",
          "cs.CY",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20848v1",
        "authors": [
          "Weixin Chen",
          "Li Chen",
          "Yuhan Zhao"
        ],
        "arxiv_categories": [
          "cs.IR",
          "cs.CY",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.730134",
      "entities": [
        "Training Fairness Control",
        "Recommendation Despite",
        "Dynamic Fairness",
        "Train Framework",
        "Framework",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20838v1",
      "title": "Reward Models Inherit Value Biases from Pretraining",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20838v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the \"Big Two\" psychological axes, we show a robust preference of Llama RMs for \"agency\" and a corresponding robust preference of Gemma RMs for \"communion.\" This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.",
        "keywords": [
          "cs.CY",
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20838v1",
        "authors": [
          "Brian Christian",
          "Jessica A. F. Thompson",
          "Elle Michelle Yang"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.730407",
      "entities": [
        "Reward Models Inherit Value",
        "Pretraining Reward",
        "Big Two",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20792v1",
      "title": "Jurisdiction as Structural Barrier: How Privacy Policy Organization May Reduce Visibility of Substantive Disclosures",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20792v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Privacy policies are supposed to provide notice. But what if substantive information appears only where users skip it? We identify a structural pattern we call jurisdiction-siloed disclosure: information about data practices appearing in specific, actionable form only within regional compliance sections labeled \"California Residents\" or \"EU/UK Users,\" while general sections use vague or qualified language for the same practices. Our audit of 123 major companies identifies 282 potential instances across 77 companies (62.6% of this purposive sample). A conservative estimate restricted to practice categories validated against OPP-115 human annotations finds 138 instances across 54 companies (44%); post-2018 categories central to our findings await independent validation. If users skip jurisdiction-labeled sections as information foraging theory predicts, users outside regulated jurisdictions would receive less specific information about practices affecting them--a transparency failure operating through document architecture rather than omission. We propose universal substantive disclosure: practices affecting all users should appear in the main policy body, with regional sections containing only procedural rights information. This standard finds support in analogous disclosure regimes (securities, truth-in-lending, nutritional labeling) where material information must reach all affected parties. Regulators could operationalize this through the FTC's \"clear and conspicuous\" standard and GDPR transparency principles. This work is hypothesis-generating: we establish that the structural pattern exists and ground the transparency concern in behavioral theory, but direct measurement of jurisdiction-specific section skipping remains the critical validation priority. We release our methodology and annotated dataset to enable replication.",
        "keywords": [
          "cs.CL",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20792v1",
        "authors": [
          "Thomas Brackin"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.CY",
          "cs.HC"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.730695",
      "entities": [
        "How Privacy Policy Organization",
        "Substantive Disclosures Privacy",
        "May Reduce Visibility",
        "California Residents",
        "Structural Barrier",
        "Standard",
        "OPP-115",
        "Policy",
        "GDPR",
        "Act",
        "FTC",
        "OPP",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.20731v1",
      "title": "QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20731v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized \"unmarked\" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20731v1",
        "authors": [
          "Mae Sosto",
          "Delfina Sol Martinez Pandiani",
          "Laura Hollink"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T18:50:53.730884",
      "entities": [
        "Autoregressive Language Models",
        "Masked Language Models",
        "Reflect Societal Norms",
        "Large Language Models",
        "Act",
        "MIT",
        "LLM",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22099v1",
      "title": "Towards Universal Urban Patterns-of-Life Simulation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22099v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Understanding urban mobility requires models that capture how people interact with and navigate the built environment. We present a scalable, generalizable agent-based framework in which daily schedules emerge from the interplay between mandatory (e.g., work, school) and flexible (e.g., errands, food, leisure) activities, driven by evolving individual needs. The results of our model are validated against empirical patterns from the 2017 U.S. National Household Travel Survey, including activity distributions, origin-destination flows, and trip-chain length distributions. We introduce a normalized similarity metric to quantify agreement between simulated and empirical patterns. Most cities achieve scores above 0.80, demonstrating strong alignment without the need for city-specific calibration. The model scales efficiently to over 20 million agents, enabling full-population simulations of large metropolitan areas. This combination of universality and scalability enables scenario analysis for infrastructure stress testing, disaster recovery, innovation diffusion, and disease spread in urban systems.",
        "keywords": [
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22099v1",
        "authors": [
          "Sandro M. Reia",
          "Henrique F. de Arruda",
          "Shiyang Ruan"
        ],
        "arxiv_categories": [
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.520031",
      "entities": [
        "National Household Travel Survey",
        "Towards Universal Urban Patterns",
        "Life Simulation Understanding",
        "Framework",
        "Agreement",
        "Fusion",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22093v1",
      "title": "Investigating Associational Biases in Inter-Model Communication of Large Generative Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22093v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22093v1",
        "authors": [
          "Fethiye Irmak Dogan",
          "Yuval Weiss",
          "Kajal Patel"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.520318",
      "entities": [
        "Investigating Associational Biases",
        "Large Generative Models Social",
        "Model Communication",
        "PHASE",
        "Act",
        "RAF",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.22021v1",
      "title": "Post-Disaster Resource Redistribution and Cooperation Evolution Based on Two-Layer Network Evolutionary Games",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.22021v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "In the aftermath of large-scale disasters, the scarcity of resources and the paralysis of infrastructure raise severe challenges to effective post-disaster recovery. Efficient coordination between shelters and victims plays a crucial role in building community resilience, yet the evolution of two-layer behavioral feedback between these two groups through network coupling remains insufficiently understood. Here, this study develops a two-layer network to capture the cross-layer coupling between shelters and victims. The upper layer uses a post-disaster emergency resource redistribution model within the framework of the public goods game, while the lower layer adopts a cooperative evolutionary game to describe internal victim interactions. Monte Carlo simulations on scale-free networks reveal threshold effects of incentives: moderate public goods enhancement and subsidies promote cooperation, whereas excessive incentives induce free-riding. In contrast, credible and well-executed punishment effectively suppresses defection. Targeted punishment of highly connected shelters significantly enhances cooperation under resource constraints. A comparative analysis using a network generated from the actual coordinates of Beijing shelters confirms the model's generality and practical applicability. The findings highlight the importance of calibrated incentives, enforceable sanctions, and structural targeting in fostering robust cooperation across organizational and individual levels in post-disaster environments.",
        "keywords": [
          "cs.GT",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.22021v1",
        "authors": [
          "Yu Chen",
          "Genjiu Xu",
          "Sinan Feng"
        ],
        "arxiv_categories": [
          "cs.GT",
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.520546",
      "entities": [
        "Disaster Resource Redistribution",
        "Layer Network Evolutionary Games",
        "Cooperation Evolution Based",
        "Monte Carlo",
        "Framework",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21977v1",
      "title": "From Particles to Agents: Hallucination as a Metric for Cognitive Friction in Spatial Simulation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21977v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Traditional architectural simulations (e.g. Computational Fluid Dynamics, evacuation, structural analysis) model elements as deterministic physics-based \"particles\" rather than cognitive \"agents\". To bridge this, we introduce \\textbf{Agentic Environmental Simulations}, where Large Multimodal generative models actively predict the next state of spatial environments based on semantic expectation. Drawing on examples from accessibility-oriented AR pipelines and multimodal digital twins, we propose a shift from chronological time-steps to Episodic Spatial Reasoning, where simulations advance through meaningful, surprisal-triggered events. Within this framework we posit AI hallucinations as diagnostic tools. By formalizing the \\textbf{Cognitive Friction} ($C_f$) it is possible to reveal \"Phantom Affordances\", i.e. semiotic ambiguities in built space. Finally, we challenge current HCI paradigms by treating environments as dynamic cognitive partners and propose a human-centered framework of cognitive orchestration for designing AI-driven simulations that preserve autonomy, affective clarity, and cognitive integrity.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21977v1",
        "authors": [
          "Javier Argota Sánchez-Vaquerizo",
          "Luis Borunda Monsivais"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.520725",
      "entities": [
        "Agentic Environmental Simulations",
        "Spatial Simulation Traditional",
        "Computational Fluid Dynamics",
        "Episodic Spatial Reasoning",
        "Phantom Affordances",
        "Cognitive Friction",
        "Large Multimodal",
        "From Particles",
        "Framework",
        "NIST",
        "Act",
        "IoT",
        "HCI",
        "AI"
      ]
    },
    {
      "id": "arxiv-2601.21963v1",
      "title": "Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21963v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Generative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Language Models (LLMs) and multimodal systems. Central to this work are our practical contributions: JudgeGPT, a platform for evaluating human perception of AI-generated news, and RogueGPT, a controlled stimulus generation engine for research. Together, these tools form an experimental pipeline for studying how humans perceive and detect AI-generated misinformation. Our findings show that detection capabilities have improved, but the competition between generation and detection continues. We discuss mitigation strategies including LLM-based detection, inoculation approaches, and the dual-use nature of generative AI. This work contributes to research addressing the adverse impacts of AI on information quality.",
        "keywords": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21963v1",
        "authors": [
          "Alexander Loth",
          "Martin Kappes",
          "Marc-Oliver Pahl"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.520885",
      "entities": [
        "Digital Ecosystems Generative",
        "Industrialized Deception",
        "Generated Misinformation",
        "Large Language Models",
        "Act",
        "MIT",
        "LLM",
        "GPT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21920v1",
      "title": "From Future of Work to Future of Workers: Addressing Asymptomatic AI Harms for Dignified Human-AI Interaction",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21920v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "In the future of work discourse, AI is touted as the ultimate productivity amplifier. Yet, beneath the efficiency gains lie subtle erosions of human expertise and agency. This paper shifts focus from the future of work to the future of workers by navigating the AI-as-Amplifier Paradox: AI's dual role as enhancer and eroder, simultaneously strengthening performance while eroding underlying expertise. We present a year-long study on the longitudinal use of AI in a high-stakes workplace among cancer specialists. Initial operational gains hid ``intuition rust'': the gradual dulling of expert judgment. These asymptomatic effects evolved into chronic harms, such as skill atrophy and identity commoditization. Building on these findings, we offer a framework for dignified Human-AI interaction co-constructed with professional knowledge workers facing AI-induced skill erosion without traditional labor protections. The framework operationalizes sociotechnical immunity through dual-purpose mechanisms that serve institutional quality goals while building worker power to detect, contain, and recover from skill erosion, and preserve human identity. Evaluated across healthcare and software engineering, our work takes a foundational step toward dignified human-AI interaction futures by balancing productivity with the preservation of human expertise.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21920v1",
        "authors": [
          "Upol Ehsan",
          "Samir Passi",
          "Koustuv Saha"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.521100",
      "entities": [
        "Addressing Asymptomatic",
        "Amplifier Paradox",
        "Dignified Human",
        "Interaction In",
        "From Future",
        "Framework",
        "Act",
        "IoT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21900v1",
      "title": "TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21900v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the \"locality hypothesis\", suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \\textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.MM",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21900v1",
        "authors": [
          "Chuancheng Shi",
          "Shangze Li",
          "Wenjun Lu"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.MM",
          "cs.CV"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.521294",
      "entities": [
        "Level Intervention Despite",
        "Large Foundation Models",
        "Robust Safety",
        "Framework",
        "Act",
        "FIS",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21839v1",
      "title": "Test-Time Compute Games",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21839v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Test-time compute has emerged as a promising strategy to enhance the reasoning abilities of large language models (LLMs). However, this strategy has in turn increased how much users pay cloud-based providers offering LLM-as-a-service, since providers charge users for the amount of test-time compute they use to generate an output. In our work, we show that the market of LLM-as-a-service is socially inefficient: providers have a financial incentive to increase the amount of test-time compute, even if this increase contributes little to the quality of the outputs. To address this inefficiency, we introduce a reverse second-price auction mechanism where providers bid their offered price and (expected) quality for the opportunity to serve a user, and users pay proportionally to the marginal value generated by the winning provider relative to the second-highest bidder. To illustrate and complement our theoretical results, we conduct experiments with multiple instruct models from the $\\texttt{Llama}$ and $\\texttt{Qwen}$ families, as well as reasoning models distilled from $\\texttt{DeepSeek-R1}$, on math and science benchmark datasets.",
        "keywords": [
          "cs.CY",
          "cs.GT",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21839v1",
        "authors": [
          "Ander Artola Velasco",
          "Dimitrios Rontogiannis",
          "Stratis Tsirtsis"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.GT",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.521451",
      "entities": [
        "Time Compute Games Test",
        "LLM",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21837v1",
      "title": "Trustworthy Intelligent Education: A Systematic Perspective on Progress, Challenges, and Future Directions",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21837v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "In recent years, trustworthiness has garnered increasing attention and exploration in the field of intelligent education, due to the inherent sensitivity of educational scenarios, such as involving minors and vulnerable groups, highly personalized learning data, and high-stakes educational outcomes. However, existing research either focuses on task-specific trustworthy methods without a holistic view of trustworthy intelligent education, or provides survey-level discussions that remain high-level and fragmented, lacking a clear and systematic categorization. To address these limitations, in this paper, we present a systematic and structured review of trustworthy intelligent education. Specifically, We first organize intelligent education into five representative task categories: learner ability assessment, learning resource recommendation, learning analytics, educational content understanding, and instructional assistance. Building on this task landscape, we review existing studies from five trustworthiness perspectives, including safety and privacy, robustness, fairness, explainability, and sustainability, and summarize and categorize the research methodologies and solution strategies therein. Finally, we summarize key challenges and discuss future research directions. This survey aims to provide a coherent reference framework and facilitate a clearer understanding of trustworthiness in intelligent education.",
        "keywords": [
          "cs.IR",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21837v1",
        "authors": [
          "Xiaoshan Yu",
          "Shangshang Yang",
          "Ziwen Wang"
        ],
        "arxiv_categories": [
          "cs.IR",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.521651",
      "entities": [
        "Trustworthy Intelligent Education",
        "Systematic Perspective",
        "Future Directions In",
        "Framework",
        "Intel",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21815v1",
      "title": "Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21815v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Understanding how media rhetoric shapes audience engagement is crucial in the attention economy. This study examines how moral emotional framing by mainstream news channels on YouTube influences user behavior across Korea and the United States. To capture the platform's multimodal nature, combining thumbnail images and video titles, we develop a multimodal moral emotion classifier by fine tuning a vision language model. The model is trained on human annotated multimodal datasets in both languages and applied to approximately 400,000 videos from major news outlets. We analyze engagement levels including views, likes, and comments, representing increasing degrees of commitment. The results show that other condemning rhetoric expressions of moral outrage that criticize others morally consistently increase all forms of engagement across cultures, with effects ranging from passive viewing to active commenting. These findings suggest that moral outrage is a particularly effective emotional strategy, attracting not only attention but also active participation. We discuss concerns about the potential misuse of other condemning rhetoric, as such practices may deepen polarization by reinforcing in group and out group divisions. To facilitate future research and ensure reproducibility, we publicly release our Korean and English multimodal moral emotion classifiers.",
        "keywords": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21815v1",
        "authors": [
          "Seongchan Park",
          "Jaehong Kim",
          "Hyeonseung Kim"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.SI",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.521843",
      "entities": [
        "Moral Outrage Shapes Commitments",
        "Multimodal Moral Emotions",
        "Beyond Attention",
        "United States",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21783v1",
      "title": "Measuring node similarity using minimum cycles in networks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21783v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Cycles are ubiquitous in various networks such as social, biological, and technological systems, where they play a significant functional and dynamical role. This paper proposes a node similarity measure based on minimal simple cycles, referred to as cycle similarity. Specifically, the metric quantifies the similarity between two nodes by considering the minimal cycles that connect them through their neighboring nodes, with an upper bound imposed on the cycle size to ensure computational feasibility. We then systematically examine the effectiveness and applicability of this similarity measure through two fundamental tasks: link prediction and community detection. To address the scarcity of cycles in link prediction, an edge-addition correction strategy is introduced, whereby the existence of a candidate edge is hypothetically assumed before computing node similarity. Experimental results demonstrate that this correction leads to improved performance on datasets including karate, INT, PPI, and Grid. In hierarchical community detection using cycle similarity, we find that the significance of cyclic structures (reflected by Z-scores), the presence of pendant nodes with degree one, and the existence of cut vertices are the primary factors influencing the algorithm's performance.",
        "keywords": [
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21783v1",
        "authors": [
          "Bo Yang"
        ],
        "arxiv_categories": [
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.522022",
      "entities": [
        "Act",
        "INT",
        "PPI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21743v1",
      "title": "Impact of behavioral heterogeneity on epidemic outcome and its mapping into effective network topologies",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21743v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Human behavior plays a critical role in shaping epidemic trajectories. During health crises, people respond in diverse ways in terms of self-protection and adherence to recommended measures, largely reflecting differences in how individuals assess risk. This behavioral variability induces effective heterogeneity into key epidemic parameters, such as infectivity and susceptibility. We introduce a minimal extension of the susceptible-infected-removed~(SIR) model, denoted HeSIR, that captures these effects through a simple bimodal scheme, where individuals may have higher or lower transmission--related traits. We derive a closed-form expression for the epidemic threshold in terms of the model parameters, and the network's degree distribution and homophily, defined as the tendency of like--risk individuals to preferentially interact. We identify a resurgence regime just beyond the classical threshold, where the number of infected individuals may initially decline before surging into large-scale transmission. Through simulations on homogeneous and heterogeneous network topologies we corroborate the analytical results and highlight how variations in susceptibility and infectivity influence the epidemic dynamics. We further show that, under suitable assumptions, the HeSIR model maps onto a standard SIR process on an appropriately modified contact network, providing a unified interpretation in terms of structural connectivity. Our findings quantify the effect of heterogeneous behavioral responses, especially in the presence of homophily, and caution against underestimating epidemic potential in fragmented populations, which may undermine timely containment efforts. The results also extend to heterogeneity arising from biological or other non-behavioral sources.",
        "keywords": [
          "q-bio.PE",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21743v1",
        "authors": [
          "Fabio Mazza",
          "Gabriele Ricci",
          "Francesca Colaiori"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.522270",
      "entities": [
        "Standard",
        "Act",
        "SIR",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21724v1",
      "title": "A costing framework for fusion power plants",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21724v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "This paper summarizes and consolidates fusion power-plant costing work performed in support of ARPA-E from 2017 through 2024, and documents the evolution of the associated analysis framework from early capital-cost-focused studies to a standards-aligned, auditable costing capability. Early efforts applied ARIES-style cost-scaling relations to generate Nth-of-a-kind (NOAK) estimates and were calibrated through a pilot study with Bechtel and Decysive Systems to benchmark balance-of-plant (BOP) costs and validate plant-level reasonableness from an engineering, procurement, and construction (EPC) perspective. Subsequent work, informed by Lucid Catalyst studies of nuclear cost drivers, expanded the methodology to treat indirect costs explicitly and to evaluate cost-reduction pathways for non-fusion-island systems through design-for-cost practices, modularization, centralized manufacturing, and learning. As ARPA-E's fusion portfolio expanded, these methods were applied across BETHE and GAMOW concepts (and select ALPHA revisits), including enhanced treatment of tritium handling and plant integration supported by Princeton/PPPL expertise. In 2023 the capability was refactored to align with the IAEA-GEN-IV EMWG-EPRI code-of-accounts lineage, while key ARIES-derived scaling relations were replaced by bottom-up subsystem models for dominant fusion cost drivers (e.g., magnets, lasers, power supplies, and power-core components) coupled to physics-informed power balances and engineering-constrained radial builds. These developments were implemented in the spreadsheet-based Fusion Economics code (FECONs) and released as an open-source Python framework (pyFECONs), providing a transparent mapping from subsystem estimates to standardized accounts and a consistent computation of LCOE.",
        "keywords": [
          "cs.SE",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21724v1",
        "authors": [
          "Simon Woodruff"
        ],
        "arxiv_categories": [
          "cs.SE",
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.522509",
      "entities": [
        "Decysive Systems",
        "Fusion Economics",
        "Lucid Catalyst",
        "Framework",
        "Standard",
        "Nuclear",
        "Fusion",
        "ALPHA",
        "BETHE",
        "GAMOW",
        "ARIES",
        "ARPA",
        "PPPL",
        "IAEA",
        "LCOE"
      ]
    },
    {
      "id": "arxiv-2601.21648v1",
      "title": "CAF-Mamba: Mamba-Based Cross-Modal Adaptive Attention Fusion for Multimodal Depression Detection",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21648v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Depression is a prevalent mental health disorder that severely impairs daily functioning and quality of life. While recent deep learning approaches for depression detection have shown promise, most rely on limited feature types, overlook explicit cross-modal interactions, and employ simple concatenation or static weighting for fusion. To overcome these limitations, we propose CAF-Mamba, a novel Mamba-based cross-modal adaptive attention fusion framework. CAF-Mamba not only captures cross-modal interactions explicitly and implicitly, but also dynamically adjusts modality contributions through a modality-wise attention mechanism, enabling more effective multimodal fusion. Experiments on two in-the-wild benchmark datasets, LMVD and D-Vlog, demonstrate that CAF-Mamba consistently outperforms existing methods and achieves state-of-the-art performance.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21648v1",
        "authors": [
          "Bowen Zhou",
          "Marc-André Fiedler",
          "Ayoub Al-Hamadi"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.CV"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.522638",
      "entities": [
        "Multimodal Depression Detection Depression",
        "Modal Adaptive Attention Fusion",
        "Deep Learning",
        "Based Cross",
        "Framework",
        "Fusion",
        "LMVD",
        "Act",
        "CAF",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21631v1",
      "title": "Turning Language Model Training from Black Box into a Sandbox",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21631v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Most classroom engagements with generative AI focus on prompting pre-trained models, leaving the role of training data and model mechanics opaque. We developed a browser-based tool that allows students to train a small transformer language model entirely on their own device, making the training process visible. In a CS1 course, 162 students completed pre- and post-test explanations of why language models sometimes produce incorrect or strange output. After a brief hands-on training activity, students' explanations shifted significantly from anthropomorphic and misconceived accounts toward data- and model-based reasoning. The results suggest that enabling learners to directly observe training can support conceptual understanding of the data-driven nature of language models and model training, even within a short intervention. For K-12 AI literacy and AI education research, the study findings suggest that enabling students to train - and not only prompt - language models can shift how they think about AI.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21631v1",
        "authors": [
          "Nicolas Pope",
          "Matti Tedre"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.522782",
      "entities": [
        "Turning Language Model Training",
        "Sandbox Most",
        "Transformer",
        "Black Box",
        "K-12",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21512v1",
      "title": "MURAD: A Large-Scale Multi-Domain Unified Reverse Arabic Dictionary Dataset",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21512v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Arabic is a linguistically and culturally rich language with a vast vocabulary that spans scientific, religious, and literary domains. Yet, large-scale lexical datasets linking Arabic words to precise definitions remain limited. We present MURAD (Multi-domain Unified Reverse Arabic Dictionary), an open lexical dataset with 96,243 word-definition pairs. The data come from trusted reference works and educational sources. Extraction used a hybrid pipeline integrating direct text parsing, optical character recognition, and automated reconstruction. This ensures accuracy and clarity. Each record aligns a target word with its standardized Arabic definition and metadata that identifies the source domain. The dataset covers terms from linguistics, Islamic studies, mathematics, physics, psychology, and engineering. It supports computational linguistics and lexicographic research. Applications include reverse dictionary modeling, semantic retrieval, and educational tools. By releasing this resource, we aim to advance Arabic natural language processing and promote reproducible research on Arabic lexical semantics.",
        "keywords": [
          "cs.IR",
          "cs.CL",
          "cs.DB",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21512v1",
        "authors": [
          "Serry Sibaee",
          "Yasser Alhabashi",
          "Nadia Sibai"
        ],
        "arxiv_categories": [
          "cs.IR",
          "cs.CL",
          "cs.DB",
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.522941",
      "entities": [
        "Unified Reverse Arabic Dictionary",
        "Domain Unified Reverse Arabic",
        "Dictionary Dataset Arabic",
        "Scale Multi",
        "Standard",
        "MURAD",
        "Meta",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21392v1",
      "title": "Shaping the learning signal in a combined Q-learning rule to improve structured cooperation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21392v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Q-learning provides a standard reinforcement learning framework for studying cooperation by specifying how agents update action values from repeated local interactions outcomes. Although previous work has shown that reputation can promote cooperation in such systems, most models introduce reputation by modifying payoffs, encoding it directly in the state or changing partner selection, which makes it difficult to isolate the role of the learning signal itself. Here, we construct the reinforcement signal as a weighted combination of reputation and game payoffs, leaving the game and network structure unchanged. We find that increasing the weight on reputation generally promotes cooperation by consolidating clusters, but this effect is conditional on the learning dynamics. Specifically, this promoting effect vanishes in two regimes: when the learning rate is extremely small, which prevents effective information propagation and when the discount factor approaches one, as distant future expectations obscure the immediate reputational advantage. Outside these limiting cases, the efficacy of reputation in promoting cooperation is attenuated by higher learning rates but amplified by larger discount factors. These results advance the understanding of cooperative dynamics by demonstrating that cooperation can be stabilized through the reputational shaping of learning signals alone, providing critical insights into the interplay between social information and individual learning parameters.",
        "keywords": [
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21392v1",
        "authors": [
          "Chunpeng Du",
          "Zongyang Li",
          "Yali Zhang"
        ],
        "arxiv_categories": [
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.523143",
      "entities": [
        "Framework",
        "Standard",
        "Act",
        "MIT",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21365v1",
      "title": "Small models, big threats: Characterizing safety challenges from low-compute AI models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21365v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Artificial intelligence (AI) systems are revolutionizing fields such as medicine, drug discovery, and materials science; however, many technologists and policymakers are also concerned about the technology's risks. To date, most concrete policies around AI governance have focused on managing AI risk by considering the amount of compute required to operate or build a given AI system. However, low-compute AI systems are becoming increasingly more performant - and more dangerous. Driven by agentic workflows, parameter quantization, and other model compression techniques, capabilities once only achievable on frontier-level systems have diffused into low-resource models deployable on consumer devices. In this report, we profile this trend by downloading historical benchmark performance data for over 5,000 large language models (LLMs) hosted on HuggingFace, noting the model size needed to achieve competitive LLM benchmarks has decreased by more than 10X over the past year. We then simulate the computational resources needed for an actor to launch a series of digital societal harm campaigns - such as disinformation botnets, sexual extortion schemes, voice-cloning fraud, and others - using low-compute open-source models and find nearly all studied campaigns can easily be executed on consumer-grade hardware. This position paper argues that protection measures for high-compute models leave serious security holes for their low-compute counterparts, meaning it is urgent both policymakers and technologists make greater efforts to understand and address this emerging class of threats.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21365v1",
        "authors": [
          "Prateek Puri"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.523356",
      "entities": [
        "Artificial Intelligence",
        "Policy",
        "Intel",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21212v1",
      "title": "Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21212v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "Effective urban planning is crucial for enhancing residents' quality of life and ensuring societal stability, playing a pivotal role in the sustainable development of cities. Current planning methods heavily rely on human experts, which are time-consuming and labor-intensive, or utilize deep learning algorithms, often limiting stakeholder involvement. To bridge these gaps, we propose Intelli-Planner, a novel framework integrating Deep Reinforcement Learning (DRL) with large language models (LLMs) to facilitate participatory and customized planning scheme generation. Intelli-Planner utilizes demographic, geographic data, and planning preferences to determine high-level planning requirements and demands for each functional type. During training, a knowledge enhancement module is employed to enhance the decision-making capability of the policy network. Additionally, we establish a multi-dimensional evaluation system and employ LLM-based stakeholders for satisfaction scoring. Experimental validation across diverse urban settings shows that Intelli-Planner surpasses traditional baselines and achieves comparable performance to state-of-the-art DRL-based methods in objective metrics, while enhancing stakeholder satisfaction and convergence speed. These findings underscore the effectiveness and superiority of our framework, highlighting the potential for integrating the latest advancements in LLMs with DRL approaches to revolutionize tasks related to functional areas planning.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21212v1",
        "authors": [
          "Xixian Yong",
          "Peilin Sun",
          "Zihe Wang"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.523561",
      "entities": [
        "Towards Customized Urban Planning",
        "Reinforcement Learning Effective",
        "Large Language Model Empowered",
        "Deep Reinforcement Learning",
        "Deep Learning",
        "Framework",
        "Policy",
        "Intel",
        "Act",
        "MIT",
        "LLM",
        "DRL",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2601.21165v1",
      "title": "FrontierScience: Evaluating AI's Ability to Perform Expert-Level Scientific Tasks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.21165v1",
        "published_date": "2026-01-29"
      },
      "content": {
        "abstract": "We introduce FrontierScience, a benchmark evaluating expert-level scientific reasoning in frontier language models. Recent model progress has nearly saturated existing science benchmarks, which often rely on multiple-choice knowledge questions or already published information. FrontierScience addresses this gap through two complementary tracks: (1) Olympiad, consisting of international olympiad problems at the level of IPhO, IChO, and IBO, and (2) Research, consisting of PhD-level, open-ended problems representative of sub-tasks in scientific research. FrontierScience contains several hundred questions (including 160 in the open-sourced gold set) covering subfields across physics, chemistry, and biology, from quantum electrodynamics to synthetic organic chemistry. All Olympiad problems are originally produced by international Olympiad medalists and national team coaches to ensure standards of difficulty, originality, and factuality. All Research problems are research sub-tasks written and verified by PhD scientists (doctoral candidates, postdoctoral researchers, or professors). For Research, we introduce a granular rubric-based evaluation framework to assess model capabilities throughout the process of solving a research task, rather than judging only a standalone final answer.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.21165v1",
        "authors": [
          "Miles Wang",
          "Robi Lin",
          "Kat Hu"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-01-30T18:50:56.523742",
      "entities": [
        "Level Scientific Tasks We",
        "Perform Expert",
        "All Olympiad",
        "All Research",
        "For Research",
        "Framework",
        "Standard",
        "Act",
        "IBO",
        "AI",
        "UN"
      ]
    }
  ]
}