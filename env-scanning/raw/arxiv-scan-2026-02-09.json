{
  "agent_metadata": {
    "agent_name": "arxiv-agent",
    "model_used": "sonnet",
    "papers_collected": 120,
    "steeps_categories_scanned": 6,
    "scan_date": "2026-02-09",
    "status": "success",
    "execution_time": 32.2,
    "process_id": 77133
  },
  "items": [
    {
      "id": "arxiv-2602.06965v1",
      "title": "MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06965v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO's broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06965v1",
        "authors": [
          "Ankan Deria",
          "Komal Kumar",
          "Adinath Madhavrao Dukre"
        ],
        "arxiv_categories": [
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.030495",
      "entities": [
        "Understanding Multimodal Large Language",
        "Medical Images Multimodal",
        "MLLM",
        "SOTA",
        "MIT",
        "Act",
        "LLM",
        "VQA",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06963v1",
      "title": "Charge-$4e$ superconductor with parafermionic vortices: A path to universal topological quantum computation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06963v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Topological superconductors (TSCs) provide a promising route to fault-tolerant quantum information processing. However, the canonical Majorana platform based on $2e$ TSCs remains computationally constrained. In this work, we find a $4e$ TSC that overcomes these constraints by combining a charge-$4e$ condensate with an Abelian chiral $\\mathbb{Z}_3$ topological order in an intertwined fashion. Remarkably, this $4e$ TSC can be obtained by proliferating vortex-antivortex pairs in a stack of two $2e$ $p+ip$ TSCs, or by melting a $ν=2/3$ quantum Hall state. Specific to this TSC, the $hc/(4e)$ fluxes act as charge-conjugation defects in the topological order, whose braiding with anyons transmutes anyons into their antiparticles. This symmetry enrichment leads to $\\mathbb{Z}_3$ parafermion zero modes trapped in the elementary vortex cores, which naturally encode qutrits. Braiding the parafermion defects alone generates the full many-qutrit Clifford group. We further show that a simple single-probe interferometric measurement enables topologically protected magic-state preparation, promoting Clifford operations to a universal gate set. Importantly, the non-Abelian excitations in the $4e$ TSC are confined to externally controlled defects, making them uniquely identifiable and amenable to controlled creation and motion with superconducting-circuit technology. Our results establish hierarchical electron aggregation as a complementary principle for engineering topological quantum matter with enhanced computational power.",
        "keywords": [
          "cond-mat.str-el",
          "quant-ph",
          "hep-th"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06963v1",
        "authors": [
          "Zhengyan Darius Shi",
          "Zhaoyu Han",
          "Srinivas Raghu"
        ],
        "arxiv_categories": [
          "cond-mat.str-el",
          "quant-ph",
          "hep-th"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.031023",
      "entities": [
        "TSC",
        "WHO",
        "EPA",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06964v1",
      "title": "Learning a Generative Meta-Model of LLM Activations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06964v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating \"meta-models\" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.",
        "keywords": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06964v1",
        "authors": [
          "Grace Luo",
          "Jiahai Feng",
          "Trevor Darrell"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.031169",
      "entities": [
        "Activations Existing",
        "Generative Meta",
        "Neural Network",
        "Fusion",
        "Meta",
        "Bill",
        "Act",
        "LLM",
        "PCA",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06960v1",
      "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06960v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
        "keywords": [
          "cs.AI",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06960v1",
        "authors": [
          "Yuchen Yan",
          "Liang Jiang",
          "Jin Jiang"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CL"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.031363",
      "entities": [
        "Reinforcement Learning Large",
        "Efficient Infinite",
        "Horizon Reasoning",
        "Framework",
        "Qwen-1",
        "MIT",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06959v1",
      "title": "CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06959v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06959v1",
        "authors": [
          "Kaiyi Huang",
          "Yukun Huang",
          "Yu Li"
        ],
        "arxiv_categories": [
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.031588",
      "entities": [
        "Cinematic Video Generation Cinematic",
        "Effective Scene Representation",
        "Unreal Engine",
        "Framework",
        "VGGT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06953v1",
      "title": "DAWN: Dependency-Aware Fast Inference for Diffusion LLMs",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06953v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.",
        "keywords": [
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06953v1",
        "authors": [
          "Lizhuo Luo",
          "Zhuoran Shi",
          "Jiajun Luo"
        ],
        "arxiv_categories": [
          "cs.CL"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.031777",
      "entities": [
        "Aware Fast Inference",
        "Fusion",
        "DAWN",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06949v1",
      "title": "DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06949v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.",
        "keywords": [
          "cs.CV",
          "cs.AI",
          "cs.RO",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06949v1",
        "authors": [
          "Shenyuan Gao",
          "William Liang",
          "Kaiyuan Zheng"
        ],
        "arxiv_categories": [
          "cs.CV",
          "cs.AI",
          "cs.RO",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.031976",
      "entities": [
        "Generalist Robot World Model",
        "Scale Human Videos Being",
        "Policy",
        "Robot",
        "MIT",
        "OOD",
        "FPS",
        "AMD",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06948v1",
      "title": "Agentic Uncertainty Reveals Agentic Overconfidence",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06948v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.",
        "keywords": [
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06948v1",
        "authors": [
          "Jean Kaddour",
          "Srijan Patel",
          "Gbètondji Dovonon"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.032066",
      "entities": [
        "Agentic Uncertainty Reveals Agentic",
        "Overconfidence Can",
        "Standard",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06942v1",
      "title": "Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06942v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a \"subwords manifest\", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this \"subwords manifest\" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.",
        "keywords": [
          "cs.AI",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06942v1",
        "authors": [
          "Duygu Altinok"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CL"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.032307",
      "entities": [
        "Optimal Turkish Subword Strategies",
        "Morphology Interplay Tokenization",
        "Systematic Evaluation",
        "Framework",
        "MIT",
        "POS",
        "CER",
        "WER",
        "Act",
        "NLI",
        "STS",
        "NER",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06941v1",
      "title": "Endogenous Resistance to Activation Steering in Language Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06941v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.",
        "keywords": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06941v1",
        "authors": [
          "Alex McKenzie",
          "Keenan Pepper",
          "Stijn Servaes"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CL",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.032496",
      "entities": [
        "Endogenous Steering Resistance",
        "Endogenous Resistance",
        "Language Models Large",
        "Activation Steering",
        "Llama-3.3",
        "Llama-3",
        "Gemma-2",
        "Meta",
        "SAE",
        "Act",
        "ESR",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06939v1",
      "title": "Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06939v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often focus on practical algorithm designs and offer limited theoretical treatment to address key questions, such as what dynamics are indeed capturable by the Bellman framework and how to inspire new algorithm classes with optimal approximations. In this paper, we present a novel topological viewpoint on temporal-difference (TD) based RL. We show that TD errors can be viewed as 1-cochain in the topological space of state transitions, while Markov dynamics are then interpreted as topological integrability. This novel view enables us to obtain a Hodge-type decomposition of TD errors into an integrable component and a topological residual, through a Bellman-de Rham projection. We further propose HodgeFlow Policy Search (HFPS) by fitting a potential network to minimize the non-integrable projection residual in RL, achieving stability/sensitivity guarantees. In numerical evaluations, HFPS is shown to significantly improve RL performance under non-Markovian.",
        "keywords": [
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06939v1",
        "authors": [
          "Zuyuan Zhang",
          "Sizhe Tang",
          "Tian Lan"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.032674",
      "entities": [
        "Learning Beyond Markov Dynamics",
        "Cochain Perspectives",
        "Difference Signals",
        "Policy Search",
        "Framework",
        "Policy",
        "HFPS",
        "MIT",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06938v1",
      "title": "Reliable Mislabel Detection for Video Capsule Endoscopy Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06938v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "The classification performance of deep neural networks relies strongly on access to large, accurately annotated datasets. In medical imaging, however, obtaining such datasets is particularly challenging since annotations must be provided by specialized physicians, which severely limits the pool of annotators. Furthermore, class boundaries can often be ambiguous or difficult to define which further complicates machine learning-based classification. In this paper, we want to address this problem and introduce a framework for mislabel detection in medical datasets. This is validated on the two largest, publicly available datasets for Video Capsule Endoscopy, an important imaging procedure for examining the gastrointestinal tract based on a video stream of lowresolution images. In addition, potentially mislabeled samples identified by our pipeline were reviewed and re-annotated by three experienced gastroenterologists. Our results show that the proposed framework successfully detects incorrectly labeled data and results in an improved anomaly detection performance after cleaning the datasets compared to current baselines.",
        "keywords": [
          "cs.CV",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06938v1",
        "authors": [
          "Julia Werner",
          "Julius Oexle",
          "Oliver Bause"
        ],
        "arxiv_categories": [
          "cs.CV",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.032823",
      "entities": [
        "Video Capsule Endoscopy Data",
        "Reliable Mislabel Detection",
        "Video Capsule Endoscopy",
        "Machine Learning",
        "Neural Network",
        "Framework",
        "MIT",
        "Act",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06934v1",
      "title": "Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06934v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \\emph{readers} and \\emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers and/or writers, enabling the concise expression of rich multidirectional communication modalities. GLP was designed as a language for grassroots platforms -- distributed systems with multiple instances that can operate independently of each other and of any global resource, and can coalesce into ever larger instances -- with its target architecture being smartphones communicating peer-to-peer. The operational semantics of Concurrent (single-agent) GLP and of multiagent GLP (maGLP) were defined via transition systems/multiagent transition systems, respectively. Here, we describe the mathematics developed to facilitate the workstation- and smartphone-based implementations of GLP by AI in Dart. We developed dGLP -- implementation-ready deterministic operational semantics for single-agent GLP -- and proved it correct with respect to the Concurrent GLP operational semantics; dGLP was used by AI as a formal spec, from which it developed a workstation-based implementation of GLP. We developed madGLP -- an implementation-ready multiagent operational semantics for maGLP -- and proved it correct with respect to the maGLP operational semantics; madGLP is deterministic at the agent level (not at the system level due to communication asynchrony), and is being used by AI as a formal spec from which it develops a smartphone-based implementation of maGLP.",
        "keywords": [
          "cs.DC",
          "cs.MA",
          "cs.PL",
          "cs.LO",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06934v1",
        "authors": [
          "Ehud Shapiro"
        ],
        "arxiv_categories": [
          "cs.DC",
          "cs.MA",
          "cs.PL",
          "cs.LO",
          "cs.AI"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.033051",
      "entities": [
        "Implementing Grassroots Logic Programs",
        "Multiagent Transition Systems",
        "Grassroots Logic Programs",
        "NIST",
        "GLP",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06925v1",
      "title": "Strategizing at Speed: A Learned Model Predictive Game for Multi-Agent Drone Racing",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06925v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Autonomous drone racing pushes the boundaries of high-speed motion planning and multi-agent strategic decision-making. Success in this domain requires drones not only to navigate at their limits but also to anticipate and counteract competitors' actions. In this paper, we study a fundamental question that arises in this domain: how deeply should an agent strategize before taking an action? To this end, we compare two planning paradigms: the Model Predictive Game (MPG), which finds interaction-aware strategies at the expense of longer computation times, and contouring Model Predictive Control (MPC), which computes strategies rapidly but does not reason about interactions. We perform extensive experiments to study this trade-off, revealing that MPG outperforms MPC at moderate velocities but loses its advantage at higher speeds due to latency. To address this shortcoming, we propose a Learned Model Predictive Game (LMPG) approach that amortizes model predictive gameplay to reduce latency. In both simulation and hardware experiments, we benchmark our approach against MPG and MPC in head-to-head races, finding that LMPG outperforms both baselines.",
        "keywords": [
          "cs.RO",
          "cs.GT"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06925v1",
        "authors": [
          "Andrei-Carlo Papuc",
          "Lasse Peters",
          "Sihao Sun"
        ],
        "arxiv_categories": [
          "cs.RO",
          "cs.GT"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.033214",
      "entities": [
        "Agent Drone Racing Autonomous",
        "Learned Model Predictive Game",
        "Model Predictive Control",
        "Model Predictive Game",
        "Drone",
        "LMPG",
        "MIT",
        "DOE",
        "MPG",
        "Act",
        "MPC",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06923v1",
      "title": "From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06923v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on \"world models\" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous \"AI Physicist\" approaches have successfully recovered such laws, they typically rely on strong, domain-specific priors that effectively \"bake in\" the physics. Conversely, Vafa et al. recently showed that generic Transformers fail to acquire these world models, achieving high predictive accuracy without capturing the underlying physical laws. We bridge this gap by systematically introducing three minimal inductive biases. We show that ensuring spatial smoothness (by formulating prediction as continuous regression) and stability (by training with noisy contexts to mitigate error accumulation) enables generic Transformers to surpass prior failures and learn a coherent Keplerian world model, successfully fitting ellipses to planetary trajectories. However, true physical insight requires a third bias: temporal locality. By restricting the attention window to the immediate past -- imposing the simple assumption that future states depend only on the local state rather than a complex history -- we force the model to abandon curve-fitting and discover Newtonian force representations. Our results demonstrate that simple architectural choices determine whether an AI becomes a curve-fitter or a physicist, marking a critical step toward automated scientific discovery.",
        "keywords": [
          "physics.class-ph",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06923v1",
        "authors": [
          "Ziming Liu",
          "Sophia Sanborn",
          "Surya Ganguli"
        ],
        "arxiv_categories": [
          "physics.class-ph",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.033412",
      "entities": [
        "Inductive Biases Guide Learned",
        "Transformers Can",
        "World Models",
        "Transformer",
        "From Kepler",
        "Intel",
        "Wind",
        "MIT",
        "WTO",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06920v1",
      "title": "Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06920v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.",
        "keywords": [
          "cs.AI",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06920v1",
        "authors": [
          "Samir Abdaljalil",
          "Parichit Sharma",
          "Erchin Serpedin"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CL"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.033632",
      "entities": [
        "MIT",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06914v1",
      "title": "Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06914v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06914v1",
        "authors": [
          "Darryl Hannan",
          "John Cooper",
          "Dylan White"
        ],
        "arxiv_categories": [
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.033841",
      "entities": [
        "Vision Token Specialization",
        "Seeing Beyond Redundancy",
        "Task Complexity",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06913v1",
      "title": "Symmetry and localisation in causally constrained quantum operator dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06913v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "This paper explores the connection between causality and many-body dynamics by studying the algebraic structure of tri-partite unitaries ('walls') which permanently arrest local operator spreading in their time-periodic evolution. We show that the resulting causally independent subsystems arise from the invariance of an embedded sub-algebra in the system (ie. a generalised symmetry) that leads to the splitting of operator space into commuting sub-algebras. The commutant structure of the invariant algebra is then used to construct local conserved quantities. Using representation theory of finite matrix algebras, the general form of wall gates is derived as unitary automorphisms. Taking causal independence as a minimal model for non-ergodic dynamics, we study its effect on probes of many-body quantum chaos. We prove an entanglement area-law due to local constraints and we study its stability against projective measurements. In a random ensemble exhibiting causal independence, we compare spectral correlations with the universal (chaotic) ensemble using the spectral form factor. Our results offer a rigorous understanding of locally constrained quantum dynamics from a quantum information perspective.",
        "keywords": [
          "math-ph",
          "cond-mat.other",
          "quant-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06913v1",
        "authors": [
          "Marcell D. Kovács",
          "Christopher J. Turner",
          "Lluís Masanes"
        ],
        "arxiv_categories": [
          "math-ph",
          "cond-mat.other",
          "quant-ph"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.034000",
      "entities": [
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06912v1",
      "title": "PANC: Prior-Aware Normalized Cut for Object Segmentation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06912v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Fully unsupervised segmentation pipelines naively seek the most salient object, should this be present. As a result, most of the methods reported in the literature deliver non-deterministic partitions that are sensitive to initialization, seed order, and threshold heuristics. We propose PANC, a weakly supervised spectral segmentation framework that uses a minimal set of annotated visual tokens to produce stable, controllable, and reproducible object masks. From the TokenCut approach, we augment the token-token affinity graph with a handful of priors coupled to anchor nodes. By manipulating the graph topology, we bias the spectral eigenspace toward partitions that are consistent with the annotations. Our approach preserves the global grouping enforced by dense self-supervised visual features, trading annotated tokens for significant gains in reproducibility, user control, and segmentation quality. Using 5 to 30 annotations per dataset, our training-free method achieves state-of-the-art performance among weakly and unsupervised approaches on standard benchmarks (e.g., DUTS-TE, ECSSD, MS COCO). Contrarily, it excels in domains where dense labels are costly or intra-class differences are subtle. We report strong and reliable results on homogeneous, fine-grained, and texture-limited domains, achieving 96.8% (+14.43% over SotA), 78.0% (+0.2%), and 78.8% (+0.37%) average mean intersection-over-union (mIoU) on CrackForest (CFD), CUB-200-2011, and HAM10000 datasets, respectively. For multi-object benchmarks, the framework showcases explicit, user-controllable semantic segmentation.",
        "keywords": [
          "cs.CV",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06912v1",
        "authors": [
          "Juan Gutiérrez",
          "Victor Gutiérrez-Garcia",
          "José Luis Blanco-Murillo"
        ],
        "arxiv_categories": [
          "cs.CV",
          "cs.AI"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.034202",
      "entities": [
        "Object Segmentation Fully",
        "Aware Normalized Cut",
        "Framework",
        "Standard",
        "CUB-200",
        "ECSSD",
        "PANC",
        "DUTS",
        "COCO",
        "NIST",
        "MIT",
        "CUB",
        "CFD",
        "EU",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.06911v1",
      "title": "TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06911v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench",
        "keywords": [
          "cs.AI",
          "cs.CR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06911v1",
        "authors": [
          "Saad Hossain",
          "Tom Tseng",
          "Punya Syon Pandey"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CR"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-09T15:11:15.034399",
      "entities": [
        "Systematically Stress",
        "Safety Under Fine",
        "Tampering As",
        "Framework",
        "Standard",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06885v1",
      "title": "Identification and Estimation of Network Models with Nonparametric Unobserved Heterogeneity",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06885v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Homophily based on observables is widespread in networks. Therefore, homophily based on unobservables (fixed effects) is also likely to be an important determinant of the interaction outcomes. Failing to properly account for latent homophily (and other complex forms of unobserved heterogeneity) can result in inconsistent estimators and misleading policy implications. To address this concern, we consider a network model with nonparametric unobserved heterogeneity, leaving the role of the fixed effects unspecified. We argue that the interaction outcomes can be used to identify agents with the same values of the fixed effects. The variation in the observed characteristics of such agents allows us to identify the effects of the covariates, while controlling for the fixed effects. Building on these ideas, we construct several estimators of the parameters of interest and characterize their large sample properties. Numerical experiments illustrate the usefulness of the suggested approaches and support the asymptotic theory.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06885v1",
        "authors": [
          "Andrei Zeleneev"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.247266",
      "entities": [
        "Nonparametric Unobserved Heterogeneity Homophily",
        "Network Models",
        "Policy",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06607v1",
      "title": "Beyond Pairwise Distance: Cognitive Traversal Distance as a Holistic Measure of Scientific Novelty",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06607v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Scientific novelty is a critical construct in bibliometrics and is commonly measured by aggregating pairwise distances between the knowledge units underlying a paper. While prior work has refined how such distances are computed, less attention has been paid to how dyadic relations are aggregated to characterize novelty at the paper level. We address this limitation by introducing a network-based indicator, Cognitive Traversal Distance (CTD). Conceptualizing the historical literature as a weighted knowledge network, CTD is defined as the length of the shortest path required to connect all knowledge units associated with a paper. CTD provides a paper-level novelty measure that reflects the minimal structural distance needed to integrate multiple knowledge units, moving beyond mean- or quantile-based aggregation of pairwise distances. Using 27 million biomedical publications indexed by OpenAlex and Medical Subject Headings (MeSH) as standardized knowledge units, we evaluate CTD against expert-based novelty benchmarks from F1000Prime-recommended papers and Nobel Prize-winning publications. CTD consistently outperforms conventional aggregation-based indicators. We further show that MeSH-based CTD is less sensitive to novelty driven by the emergence of entirely new conceptual labels, clarifying its scope relative to recent text-based measures.",
        "keywords": [
          "cs.DL",
          "cs.CY",
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06607v1",
        "authors": [
          "Yi Xiang",
          "Pascal Welke",
          "Chengzhi Zhang"
        ],
        "arxiv_categories": [
          "cs.DL",
          "cs.CY",
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.247677",
      "entities": [
        "Scientific Novelty Scientific",
        "Cognitive Traversal Distance",
        "Medical Subject Headings",
        "Beyond Pairwise Distance",
        "Holistic Measure",
        "Nobel Prize",
        "Standard",
        "MIT",
        "Act",
        "CTD",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06435v1",
      "title": "Social Interactions Models with Latent Structures",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06435v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "This paper studies estimation and inference of heterogeneous peer effects featuring group fixed effects and slope heterogeneity under latent structure. We adapt the Classifier-Lasso algorithm to consistently discover latent structures and determine the number of clusters. To solve the incidental parameter problem in the binary choice model with social interactions, we propose a parametric bootstrap method to debias and establish its asymptotic validity. Monte Carlo simulations confirm strong finite sample performance of our methods. In an application to students' risky behaviors, the algorithm detects two latent clusters and finds that peer effects are significant within one of the clusters, demonstrating the practical applicability in uncovering heterogeneous social interactions.",
        "keywords": [
          "econ.EM",
          "stat.ME"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06435v1",
        "authors": [
          "Zhongjian Lin",
          "Zhentao Shi",
          "Yapeng Zheng"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.ME"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.247881",
      "entities": [
        "Social Interactions Models",
        "Monte Carlo",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06263v1",
      "title": "Chasing Tails: How Do People Respond to Wait Time Distributions?",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06263v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "We use a series of pre-registered, incentive-compatible online experiments to investigate how people evaluate and choose among different waiting time distributions. Our main findings are threefold. First, consistent with prior literature, people show an aversion to both longer expected waits and higher variance. Second, and more surprisingly, moment-based utility models fail to capture preferences when distributions have thick-right tails: indeed, decision-makers strongly prefer distributions with long-right tails (where probability mass is more evenly distributed over a larger support set) relative to tails that exhibit a spike near the maximum possible value, even when controlling for mean, variance, and higher moments. Conditional Value at Risk (CVaR) utility models commonly used in portfolio theory predict these choices well. Third, when given a choice, decision-makers overwhelmingly seek information about right-tail outcomes. These results have practical implications for service operations: (1) service designs that create a spike in long waiting times (such as priority or dedicated queue designs) may be particularly aversive; (2) when informativeness is the goal, providers should prioritize sharing right-tail probabilities or percentiles; and (3) to increase service uptake, providers can strategically disclose (or withhold) distributional information depending on right-tail shape.",
        "keywords": [
          "cs.HC",
          "eess.SY",
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06263v1",
        "authors": [
          "Evgeny Kagan",
          "Kyle Hyndman",
          "Andrew Davis"
        ],
        "arxiv_categories": [
          "cs.HC",
          "eess.SY",
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.248216",
      "entities": [
        "Wait Time Distributions",
        "How Do People Respond",
        "Conditional Value",
        "Chasing Tails",
        "Act",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.06198v1",
      "title": "Insider Purchase Signals in Microcap Equities: Gradient Boosting Detection of Abnormal Returns",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06198v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "This paper examines whether SEC Form 4 insider purchase filings predict abnormal returns in U.S. microcap stocks. The analysis covers 17,237 open-market purchases across 1,343 issuers from 2018 through 2024, restricted to market capitalizations between \\$30M and \\$500M. A gradient boosting classifier trained on insider identity, transaction history, and market conditions at disclosure achieves AUC of 0.70 on out-of-sample 2024 data. At an optimized threshold of 0.20, precision is 0.38 and recall is 0.69. The distance from the 52-week high dominates feature importance, accounting for 36% of predictive signal. A momentum pattern emerges in the data: transactions disclosed after price appreciation exceeding 10% yield the highest mean cumulative abnormal return (6.3%) and the highest probability of outperformance (36.7%). This contrasts with the simple mean-reversion intuition often applied to post-run-up entries. The result is robust to winsorization and holds across subsamples. These patterns are consistent with slower information incorporation in illiquid markets, where trend confirmation may filter for higher-conviction insider signals.",
        "keywords": [
          "q-fin.TR",
          "q-fin.ST"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06198v1",
        "authors": [
          "Hangyi Zhao"
        ],
        "arxiv_categories": [
          "q-fin.TR",
          "q-fin.ST"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.248493",
      "entities": [
        "Gradient Boosting Detection",
        "Insider Purchase Signals",
        "Microcap Equities",
        "AUC",
        "SEC",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05592v1",
      "title": "An invariant modification of the bilinear form test",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05592v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "The invariance properties of certain likelihood-based asymptotic tests as well as their extensions for M-estimation, estimating functions and the generalized method of moments have been well studied. The simulation study reported in Crudu and Osorio [Econ. Lett. 187: 108885, 2020] shows that the bilinear form test is not invariant to one-to-one transformations of the parameter space. This paper provides a set of suitable conditions to establish the invariance property under reparametrization of the bilinear form test for linear or nonlinear hypotheses that arise in extremum estimation which leads to a simple modification of the test statistic. Evidence from a Monte Carlo simulation experiment suggests good performance of the proposed methodology.",
        "keywords": [
          "math.ST",
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05592v1",
        "authors": [
          "Angelo Garate",
          "Felipe Osorio",
          "Federico Crudu"
        ],
        "arxiv_categories": [
          "math.ST",
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.248708",
      "entities": [
        "Monte Carlo",
        "EPA",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05542v1",
      "title": "Trimming of extreme votes and favoritism: Evidence from the field",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05542v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Despite a large body of theoretical literature on voting mechanisms, there is no documented evidence from real-world panel evaluations about the effect of trimming the extreme votes on sincere voting. We provide the first such evidence by comparing subjective evaluations of experts from different countries in competitive settings with and without a trimming mechanism. In these evaluations, some of the evaluated subjects are experts' compatriots. Using data on 29,383 subjective evaluations, we find that experts assign significantly higher scores to their compatriots in panels without trimming. However, in panels with trimming, this favoritism is generally insignificant.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05542v1",
        "authors": [
          "Alex Krumer",
          "Felix Otto",
          "Tim Pawlowski"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.248883",
      "entities": [
        "IoT",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05226v1",
      "title": "Predictive Synthesis under Sporadic Participation: Evidence from Inflation Density Surveys",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05226v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Central banks rely on density forecasts from professional surveys to assess inflation risks and communicate uncertainty. A central challenge in using these surveys is irregular participation: forecasters enter and exit, skip rounds, and reappear after long gaps. In the European Central Bank's Survey of Professional Forecasters, turnover and missingness vary substantially over time, causing the set of submitted predictions to change from quarter to quarter. Standard aggregation rules -- such as equal-weight pooling, renormalization after dropping missing forecasters, or ad hoc imputation -- can generate artificial jumps in combined predictions driven by panel composition rather than economic information, complicating real-time interpretation and obscuring forecaster performance. We develop coherent Bayesian updating rules for forecast combination under sporadic participation that maintain a well-defined latent predictive state for each forecaster even when their forecast is unobserved. Rather than relying on renormalization or imputation, the combined predictive distribution is updated through the implied conditional structure of the panel. This approach isolates genuine performance differences from mechanical participation effects and yields interpretable dynamics in forecaster influence. In the ECB survey, it improves predictive accuracy relative to equal-weight benchmarks and delivers smoother and better-calibrated inflation density forecasts, particularly during periods of high turnover.",
        "keywords": [
          "econ.EM",
          "stat.ME",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05226v1",
        "authors": [
          "Matthew C. Johnson",
          "Matteo Luciani",
          "Minzhengxiong Zhang"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.ME",
          "stat.AP"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.249223",
      "entities": [
        "Inflation Density Surveys Central",
        "Professional Forecasters",
        "Sporadic Participation",
        "European Central Bank",
        "Predictive Synthesis",
        "Standard",
        "MIT",
        "ECB",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05137v2",
      "title": "Nested Pseudo-GMM Estimation of Demand for Differentiated Products",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05137v2",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "We propose a fast algorithm for computing the GMM estimator in the BLP demand model (Berry, Levinsohn, and Pakes, 1995). Inspired by nested pseudo-likelihood methods for dynamic discrete choice models, our approach avoids repeatedly solving the inverse demand system by swapping the order of the GMM optimization and the fixed-point computation. We show that, by fixing consumer-level outside-option probabilities, BLP's market-share to mean-utility inversion becomes closed-form and, crucially, separable across products, yielding a nested pseudo-GMM algorithm with analytic gradients. The resulting estimator scales dramatically better with the number of products and is naturally suited for parallel and multithreaded implementation. In the inner loop, outside-option probabilities are treated as fixed objects while a pseudo-GMM criterion is minimized with respect to the structural parameters, substantially reducing computational cost. Monte Carlo simulations and an empirical application show that our method is significantly faster than the fastest existing alternatives, with efficiency gains that grow more than proportionally in the number of products.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05137v2",
        "authors": [
          "Victor Aguirregabiria",
          "Hui Liu",
          "Yao Luo"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.249447",
      "entities": [
        "Differentiated Products We",
        "Nested Pseudo",
        "Monte Carlo",
        "GMM",
        "BLP",
        "EPA",
        "EU",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.05112v1",
      "title": "Collaboration for the Bioeconomy -- Evidence from Innovation Output in Sweden, 1970-2021",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05112v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "Collaboration is expected to play a central role in the transition to a bioeconomy - a central pillar of a green economy. Such collaboration is supposed to connect traditional biomass processing firms with diverse actors in fields where biomass ought to substitute existing or create novel products and processes. This study analyzes the network of technology collaborations among innovating firms in Sweden between 1970 and 2021. The results reveal generally positive associations between direct and indirect ties, with meaningful increases in innovation output for each additional direct collaboration partner. Relationships between brokerage positions and innovation output were statistically insignificant, and cognitive proximity - while following theoretical expectations - materially insignificant. These associations are mostly equal between actors heavily invested in the bioeconomy and those focusing on other innovation areas, indicating that these actors operate under largely similar mechanisms linking collaboration and subsequent innovation output. These results suggest that stimulating collaboration broadly - rather than attempting to optimize collaboration compositions - could result in higher number of significant Swedish innovations, for bioeconomy and other sectors alike.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05112v1",
        "authors": [
          "Philipp Jonas Kreutzer",
          "Josef Taalbi"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.249678",
      "entities": [
        "Innovation Output",
        "MIT",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05099v1",
      "title": "Personalized Policy Learning through Discrete Experimentation: Theory and Empirical Evidence",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05099v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "Randomized Controlled Trials (RCTs), or A/B testing, have become the gold standard for optimizing various operational policies on online platforms. However, RCTs on these platforms typically cover a limited number of discrete treatment levels, while the platforms increasingly face complex operational challenges involving optimizing continuous variables, such as pricing and incentive programs. The current industry practice involves discretizing these continuous decision variables into several treatment levels and selecting the optimal discrete treatment level. This approach, however, often leads to suboptimal decisions as it cannot accurately extrapolate performance for untested treatment levels and fails to account for heterogeneity in treatment effects across user characteristics. This study addresses these limitations by developing a theoretically solid and empirically verified framework to learn personalized continuous policies based on high-dimensional user characteristics, using observations from an RCT with only a discrete set of treatment levels. Specifically, we introduce a deep learning for policy targeting (DLPT) framework that includes both personalized policy value estimation and personalized policy learning. We prove that our policy value estimators are asymptotically unbiased and consistent, and the learned policy achieves a root-n-regret bound. We empirically validate our methods in collaboration with a leading social media platform to optimize incentive levels for content creation. Results demonstrate that our DLPT framework significantly outperforms existing benchmarks, achieving substantial improvements in both evaluating the value of policies for each user group and identifying the optimal personalized policy.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05099v1",
        "authors": [
          "Zhiqi Zhang",
          "Zhiyu Zeng",
          "Ruohan Zhan"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.249975",
      "entities": [
        "Empirical Evidence Randomized Controlled",
        "Personalized Policy Learning",
        "Discrete Experimentation",
        "Deep Learning",
        "Framework",
        "Standard",
        "Policy",
        "DLPT",
        "MIT",
        "RCT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.04464v1",
      "title": "Discounted Sales of Expiring Perishables: Challenges for Demand Forecasting in Grocery Retail Practice",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.04464v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "Grocery retailers frequently apply price discounts to stimulate demand for expiring perishables. However, integrating these discounted sales into future demand forecasts presents a significant challenge. This study investigates the effectiveness of incorporating a fixed share of these sales as \\textit{regular} demand into the forecast, as commonly applied in practice. We employ a two-step regression approach on data from a major European grocery retailer, covering over 1,700 products across 676 stores. We reveal that forecasts underestimate actual demand for most SKUs when discounted sales occur. This residual uplift effect is significantly influenced by the number of sales at reduced prices. Our findings underscore the necessity for more precise approaches to integrate discounted sales into demand forecasts, thereby preventing excess inventory and the associated economic and environmental impacts of spoilage in the grocery sector.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.04464v1",
        "authors": [
          "David Winkelmann",
          "Theresa Elbracht",
          "Jonas Brenker"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.250151",
      "entities": [
        "Grocery Retail Practice Grocery",
        "Expiring Perishables",
        "Demand Forecasting",
        "Discounted Sales",
        "Act",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.04230v1",
      "title": "Validating Causal Message Passing Against Network-Aware Methods on Real Experiments",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.04230v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "Estimating total treatment effects in the presence of network interference typically requires knowledge of the underlying interaction structure. However, in many practical settings, network data is either unavailable, incomplete, or measured with substantial error. We demonstrate that causal message passing, a methodology that leverages temporal structure in outcome data rather than network topology, can recover total treatment effects comparable to network-aware approaches. We apply causal message passing to two large-scale field experiments where a recently developed bipartite graph methodology, which requires network knowledge, serves as a benchmark. Despite having no access to the interaction network, causal message passing produces effect estimates that match the network-aware approach in direction across all metrics and in statistical significance for the primary decision metric. Our findings validate the premise of causal message passing: that temporal variation in outcomes can serve as an effective substitute for network observation when estimating spillover effects. This has important practical implications: practitioners facing settings where network data is costly to collect, proprietary, or unreliable can instead exploit the temporal dynamics of their experimental data.",
        "keywords": [
          "econ.EM",
          "stat.ME"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.04230v1",
        "authors": [
          "Albert Tan",
          "Sadegh Shirani",
          "James Nordlund"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.ME"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.250377",
      "entities": [
        "Validating Causal Message Passing",
        "Real Experiments Estimating",
        "Against Network",
        "Aware Methods",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.04092v1",
      "title": "Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.04092v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "Time-to-event estimation (i.e., survival analysis) is common in health research, most often using methods that assume proportional hazards and no competing risks. Because both assumptions are frequently invalid, estimators more aligned with real-world settings have been proposed. An effect can be estimated as the difference in areas below the cumulative incidence functions of two groups up to a pre-specified time point. This approach, restricted mean time lost (RMTL), can be used in settings with competing risks as well. We extend RMTL estimation for use in an understudied health policy application in Medicare. Medicare currently supports healthcare payment for over 69 million beneficiaries, most of whom are enrolled in Medicare Advantage plans and receive insurance from private insurers. These insurers are prospectively paid by the federal government for each of their beneficiaries' anticipated health needs using an ordinary least squares linear regression algorithm. As all coefficients are positive and predictor variables are largely insurer-submitted health conditions, insurers are incentivized to upcode, or report more diagnoses than may be accurate. Such gaming is projected to cost the federal government $40 billion in 2025 alone without clear benefit to beneficiaries. We propose several novel estimators of coding intensity and possible upcoding in Medicare Advantage, including accounting for unreliable reporting. We demonstrate estimator performance in simulated data leveraging the National Institutes of Health's All of Us study and also develop an open source R package to simulate realistic labeled upcoding data, which were not previously available.",
        "keywords": [
          "econ.EM",
          "stat.ME",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.04092v1",
        "authors": [
          "Oana M. Enache",
          "Sherri Rose"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.ME",
          "stat.AP"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.250667",
      "entities": [
        "Medicare Health Plan Payment",
        "Unreliably Reported Events",
        "National Institutes",
        "Medicare Advantage",
        "Event Estimation",
        "Institute",
        "Policy",
        "Bill",
        "RMTL",
        "MIT",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.04060v1",
      "title": "The Output Convergence Debate Revisited: Lessons from recent developments in the analysis of panel data models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.04060v1",
        "published_date": "2026-02-03"
      },
      "content": {
        "abstract": "This paper provides a critical examination of the empirical basis of the output convergence debate in the light of recent developments in the analysis of dynamic heterogeneous panels with interactive effects. It shows that popular tools such as Barro's cross-country regressions and two-way fixed effects (TWFE) estimators that assume parallel trends and homogeneous dynamics lead to substantial under-estimation of the speed of convergence and misleading inference. Instead, dynamic common correlated effects (DCCE) estimators due to Chudik and Pesaran (2015a) provide consistent estimates and valid inference that are robust to nonparallel trends and correlated heterogeneity and apply even if there are breaks, trends and/or unit roots in the latent technology factor. It also suggests a way to estimate the effect of slowly moving determinants of growth. The theoretical findings are augmented with empirical evidence using Penn World Tables data, finding little evidence of per capita output convergence across countries, very slow evidence of cross country growth convergence, and reasonably fast within country convergence. Capital accumulation is found to be the most important single determinant of cross-country differences in output while slow moving indicators such as potential for conflict and protection of property rights proved to be statistically significant determinants of the steady state levels of output per capita. We are also able to replicate a positive evidence of democratization on output, but we find that the statistical significance of this effect to fall as we allow for nonparallel trends and dynamic heterogeneity.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.04060v1",
        "authors": [
          "M Hashem Pesaran",
          "Ron Smith"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.250963",
      "entities": [
        "Penn World Tables",
        "DCCE",
        "TWFE",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.03981v1",
      "title": "DeXposure-FM: A Time-series, Graph Foundation Model for Credit Exposures and Stability on Decentralized Financial Networks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.03981v1",
        "published_date": "2026-02-03"
      },
      "content": {
        "abstract": "Credit exposure in Decentralized Finance (DeFi) is often implicit and token-mediated, creating a dense web of inter-protocol dependencies. Thus, a shock to one token may result in significant and uncontrolled contagion effects. As the DeFi ecosystem becomes increasingly linked with traditional financial infrastructure through instruments, such as stablecoins, the risk posed by this dynamic demands more powerful quantification tools. We introduce DeXposure-FM, the first time-series, graph foundation model for measuring and forecasting inter-protocol credit exposure on DeFi networks, to the best of our knowledge. Employing a graph-tabular encoder, with pre-trained weight initialization, and multiple task-specific heads, DeXposure-FM is trained on the DeXposure dataset that has 43.7 million data entries, across 4,300+ protocols on 602 blockchains, covering 24,300+ unique tokens. The training is operationalized for credit-exposure forecasting, predicting the joint dynamics of (1) protocol-level flows, and (2) the topology and weights of credit-exposure links. The DeXposure-FM is empirically validated on two machine learning benchmarks; it consistently outperforms the state-of-the-art approaches, including a graph foundation model and temporal graph neural networks. DeXposure-FM further produces financial economics tools that support macroprudential monitoring and scenario-based DeFi stress testing, by enabling protocol-level systemic-importance scores, sector-level spillover and concentration measures via a forecast-then-measure pipeline. Empirical verification fully supports our financial economics tools. The model and code have been publicly available. Model: https://huggingface.co/EVIEHub/DeXposure-FM. Code: https://github.com/EVIEHub/DeXposure-FM.",
        "keywords": [
          "cs.AI",
          "econ.EM",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.03981v1",
        "authors": [
          "Aijie Shu",
          "Wenbin Wu",
          "Gbenga Ibikunle"
        ],
        "arxiv_categories": [
          "cs.AI",
          "econ.EM",
          "cs.LG"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.251274",
      "entities": [
        "Decentralized Financial Networks Credit",
        "Graph Foundation Model",
        "Decentralized Finance",
        "Machine Learning",
        "Credit Exposures",
        "Neural Network",
        "Blockchain",
        "Protocol",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.03819v1",
      "title": "Global Testing in Multivariate Regression Discontinuity Designs",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.03819v1",
        "published_date": "2026-02-03"
      },
      "content": {
        "abstract": "Regression discontinuity (RD) designs with multiple running variables arise in a growing number of empirical applications, including geographic boundaries and multi-score assignment rules. Although recent methodological work has extended estimation and inference tools to multivariate settings, far less attention has been devoted to developing global testing methods that formally assess whether a discontinuity exists anywhere along a multivariate treatment boundary. Existing approaches perform well in large samples, but can exhibit severe size distortions in moderate or small samples due to the sparsity of observations near any particular boundary point. This paper introduces a complementary global testing procedure that mitigates the small-sample weaknesses of existing multivariate RD methods by integrating multivariate machine learning estimators with a distance-based aggregation strategy, yielding a test statistic that remains reliable with limited data. Simulations demonstrate that the proposed method maintains near-nominal size and strong power, including in settings where standard multivariate estimators break down. The procedure is applied to an empirical setting to demonstrate its implementation and to illustrate how it can complement existing multivariate RD estimators.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.03819v1",
        "authors": [
          "Artem Samiahulin"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.251498",
      "entities": [
        "Multivariate Regression Discontinuity Designs",
        "Machine Learning",
        "Global Testing",
        "Standard",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.03767v1",
      "title": "Decision-oriented benchmarking to transform AI weather forecast access: Application to the Indian monsoon",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.03767v1",
        "published_date": "2026-02-03"
      },
      "content": {
        "abstract": "Artificial intelligence weather prediction (AIWP) models now often outperform traditional physics-based models on common metrics while requiring orders-of-magnitude less computing resources and time. Open-access AIWP models thus hold promise as transformational tools for helping low- and middle-income populations make decisions in the face of high-impact weather shocks. Yet, current approaches to evaluating AIWP models focus mainly on aggregated meteorological metrics without considering local stakeholders' needs in decision-oriented, operational frameworks. Here, we introduce such a framework that connects meteorology, AI, and social sciences. As an example, we apply it to the 150-year-old problem of Indian monsoon forecasting, focusing on benefits to rain-fed agriculture, which is highly susceptible to climate change. AIWP models skillfully predict an agriculturally relevant onset index at regional scales weeks in advance when evaluated out-of-sample using deterministic and probabilistic metrics. This framework informed a government-led effort in 2025 to send 38 million Indian farmers AI-based monsoon onset forecasts, which captured an unusual weeks-long pause in monsoon progression. This decision-oriented benchmarking framework provides a key component of a blueprint for harnessing the power of AIWP models to help large vulnerable populations adapt to weather shocks in the face of climate variability and change.",
        "keywords": [
          "physics.ao-ph",
          "cs.AI",
          "econ.GN",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.03767v1",
        "authors": [
          "Rajat Masiwal",
          "Colin Aitken",
          "Adam Marchakitus"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "cs.AI",
          "econ.GN",
          "cs.LG"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.251745",
      "entities": [
        "Artificial Intelligence",
        "Framework",
        "Intel",
        "AIWP",
        "NIST",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.03751v1",
      "title": "Tracing the Genetic Footprints of the UK National Health Service",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.03751v1",
        "published_date": "2026-02-03"
      },
      "content": {
        "abstract": "The establishment of the UK National Health Service (NHS) in July 1948 was one of the most consequential health policy interventions of the twentieth century, providing universal and free access to medical care and substantially expanding maternal and infant health services. In this paper, we estimate the causal effect of the NHS introduction on early-life mortality and we test whether survival is selective. We adopt a regression discontinuity design under local randomization, comparing individuals born just before and just after July 1948. Leveraging newly digitized weekly death records, we document a significant decline in stillbirths and infant mortality following the introduction of the NHS, the latter driven primarily by reductions in deaths from congenital conditions and diarrhea. We then use polygenic indexes (PGIs), fixed at conception, to track changes in population composition, showing that cohorts born at or after the NHS introduction exhibit higher PGIs associated with contextually-adverse traits (e.g., depression, COPD, and preterm birth) and lower PGIs associated with contextually-valued traits (e.g., educational attainment, self-rated health, and pregnancy length), with effect sizes as large as 7.5% of a standard deviation. These results based on the UK Biobank data are robust to family-based designs and replicate in the English Longitudinal Study of Ageing and the UK Household Longitudinal Study. Effects are strongest in socioeconomically disadvantaged areas and among males. This novel evidence on the existence and magnitude of selective survival highlights how large-scale public policies can leave a persistent imprint on population composition and generate long-term survival biases.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.03751v1",
        "authors": [
          "Nicolau Martin-Bassols",
          "Pietro Biroli",
          "Elisabetta De Cao"
        ],
        "arxiv_categories": [
          "econ.GN"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.252043",
      "entities": [
        "Household Longitudinal Study",
        "English Longitudinal Study",
        "National Health Service",
        "Genetic Footprints",
        "Standard",
        "Policy",
        "COPD",
        "NHS",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.03469v1",
      "title": "Unbiased Estimation of Central Moments in Unbalanced Two- and Three-Level Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.03469v1",
        "published_date": "2026-02-03"
      },
      "content": {
        "abstract": "This paper derives closed-form unbiased estimators of central moments in multilevel random-effects models with unbalanced group sizes. In a two-level model, we provide unbiased estimators for the second, third, and fourth central moments under both group-level and observation-level averaging. In a three-level model, we provide unbiased estimators for the second and third central moments.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.03469v1",
        "authors": [
          "Dan Ben-Moshe",
          "David Genesove"
        ],
        "arxiv_categories": [
          "econ.EM"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:22.252131",
      "entities": [
        "Unbiased Estimation",
        "Central Moments",
        "Unbalanced Two",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06703v1",
      "title": "Theoretical constraints on tidal triggering of slow earthquakes",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06703v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Tidal stress is a globally acting perturbation driven primarily by the gravitational forcing of the Moon and the Sun. Understanding how tidal stresses can trigger seismic events is essential for constraining tectonic environments that are sensitive to small stress perturbations. Here, employing a spring-block with rate-and-state friction, we investigate tidal triggering on velocity-weakening stable sliding faults with stiffness slightly exceeding the critical stiffness. We first apply idealized step-like and boxcar normal stress perturbations to demonstrate a resonance-like amplification of slip rate when the perturbation period approaches the intrinsic frictional timescale of state evolution. Next, we perform nondimensional analyses and numerical simulations with harmonic tidal-like perturbations to identify the key parameters controlling tidal triggering and their admissible ranges. Triggered slip events are further characterized using physically interpretable quantities, including radiation efficiency and tidal phase. Our results show that even small stress perturbations can trigger periodic as well as complex slip events on stable sliding faults. The triggering behavior is primarily controlled by the normalized perturbation period and the normalized perturbation amplitude. An increase in the normalized period shifts event timing from the peak of tidal stress toward the peak of stress rate, whereas increasing the normalized amplitude promotes a transition from slow to fast events. The parameter space permitting triggered events suggests that the parameter which characterizes the instantaneous frictional strength of an interface, should not exceed tens to hundreds of kilopascals, and that the characteristic slip distance for frictional weakening is likely on the order of micrometers.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06703v1",
        "authors": [
          "Yishuo Zhou",
          "Ankit Gupta",
          "Hideo Aochi"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.463213",
      "entities": [
        "MIT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06649v1",
      "title": "Growth Models Under Uniform Catastrophes",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06649v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "We consider stochastic growth models for populations organized in colonies and subject to uniform catastrophes. To assess population viability, we analyze scenarios in which individuals adopt dispersion strategies after catastrophic events. For these models, we derive explicit expressions for the survival probability and the mean time to extinction, both with and without spatial constraints. In addition, we complement this analysis by comparing uniform catastrophes with binomial and geometric catastrophes in models with dispersion and no spatial restrictions. Here, the terms uniform, binomial and geometric refer to the probability distributions governing the number of individuals that survive immediately after a catastrophe. This comparison allows us to quantify the impact of different types of catastrophic events on population persistence.",
        "keywords": [
          "math.PR",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06649v1",
        "authors": [
          "Joan Amaya",
          "Valdivino V. Junior",
          "Fábio P. Machado"
        ],
        "arxiv_categories": [
          "math.PR",
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.463507",
      "entities": [
        "Growth Models Under Uniform",
        "Catastrophes We",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06640v1",
      "title": "Habitat heterogeneity and dispersal network structure as drivers of metacommunity dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06640v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Spatial structure and species interactions jointly shape the dynamics and biodiversity of ecological systems, yet most theoretical models either neglect spatial heterogeneity or sacrifice analytical tractability. Here, we provide a unified microscopic, mechanistic framework for deriving effective metapopulation and metacommunity models from individual-based ecological dynamics on arbitrary dispersal networks. The resulting coarse-grained description features an effective dispersal kernel that encodes both microscopic dynamical parameters and network topology. Based on this framework, we demonstrate exact analytical results for species persistence in both homogeneous and heterogeneous landscapes, including a generalization of the classical concept of metapopulation capacity to non-uniform local extinction rates. Incorporating stochasticity arising from finite carrying capacities, we obtain a reduced one-dimensional description that reveals universal finite-size scaling laws for extinction times and fluctuations. Extending the approach to multiple competing species, we prove that in homogeneous environments monodominance can be avoided only in a fine-tuned, marginally stable coexistence state, and that the classic metapopulation capacity gives only a necessary but not sufficient condition for persistence. We demonstrate that heterogeneous habitats can support stable coexistence, but only above a critical level of heterogeneity. Finally, we outline how additional ecological processes can be systematically incorporated within the same formalism. Together, these results provide analytical benchmarks and a general route for constructing spatially explicit ecological theories based on an interpretable underlying mechanistic foundation.",
        "keywords": [
          "cond-mat.stat-mech",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06640v1",
        "authors": [
          "Davide Bernardi",
          "Alice Doimo",
          "Giorgio Nicoletti"
        ],
        "arxiv_categories": [
          "cond-mat.stat-mech",
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.463958",
      "entities": [
        "Framework",
        "Meta",
        "NIST",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06606v1",
      "title": "Multiple timescales in collective motion: daily and intraday upstream fish migration focusing on Feller condition",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06606v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Fish migration is a collective phenomenon that has multiple timescales, ranging from daily to intraday (hourly or even finer). We propose a unified mathematical approach using diffusion bridges, nonlinear stochastic differential equations with pinned initial and terminal conditions, to model both daily and intraday fish migration phenomena. Drift and diffusion coefficients of these bridges are determined based on time-dependent parameterized average and variance curves fitted against fish count data, with which the unique existence of their solutions is rigorously guaranteed. We show that sample paths of the diffusion bridges have qualitatively distinctive properties depending on the Feller condition, namely, the ratio between the sizes of diffusion and drift. Our application study about the juvenile upstream migration of Plecoglossus altivelis altivelis (Ayu) in Japan clarifies similarities and differences between daily and intraday migration phenomena. Particularly, we discuss that the daily and intraday fish count data correspond to distinctive Feller indices, showing that the former is qualitatively less randomized and intermittent. The results obtained in this study suggest that the Feller condition potentially serves as an effective tool for evaluating fish migration phenomena of Ayu across different timescales.",
        "keywords": [
          "math.DS",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06606v1",
        "authors": [
          "Hidekazu Yoshioka"
        ],
        "arxiv_categories": [
          "math.DS",
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.464302",
      "entities": [
        "Fusion",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06473v1",
      "title": "On large-scale oceanic wind-drift currents",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06473v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Starting from the Navier--Stokes equations in rotating spherical coordinates with constant density and eddy viscosity varying only with depth, and appropriate, physically motivated boundary conditions, we derive an asymptotic model for the description of non-equatorial wind-generated oceanic drift currents. We do not invoke any tangent-plane approximations, thus allowing for large-scale flows that would not be captured by the classical $f$-plane approach. The strategy is to identify two small intrinsic scales for the flow (namely, the ratio between the depth of the Ekman layer and the Earth's radius, and the Rossby number) and, after a careful scaling, perform a double asymptotic expansion with respect to these small parameters. This leads to a system of linear ordinary differential equations with nonlinear boundary conditions for the leading-order dynamics, in addition to which we identify the governing equations for the first-order correction with respect to the Rossby number. First, we establish the existence and uniqueness of the solution to the leading-order equations and show that the solution behaves like a classical Ekman spiral for any eddy viscosity profile; moreover, we discuss the solution of the equations for the first-order correction, for which we also provide a priori bounds in terms of the leading-order solution. Finally, we discuss several cases of explicit eddy viscosity profiles (constant, linearly decreasing, linearly increasing, piecewise linear, and exponentially decaying) and compute the surface deflection angle of the wind-drift current. We obtain results that are remarkably consistent with observations.",
        "keywords": [
          "math-ph",
          "physics.flu-dyn",
          "math.AP",
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06473v1",
        "authors": [
          "Christian Puntini",
          "Luigi Roberti",
          "Eduard Stefanescu"
        ],
        "arxiv_categories": [
          "math-ph",
          "physics.flu-dyn",
          "math.AP",
          "physics.ao-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.464701",
      "entities": [
        "Wind",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06429v1",
      "title": "Reclaiming First Principles: A Differentiable Framework for Conceptual Hydrologic Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06429v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Conceptual hydrologic models remain the cornerstone of rainfall-runoff modeling, yet their calibration is often slow and numerically fragile. Most gradient-based parameter estimation methods rely on finite-difference approximations or automatic differentiation frameworks (e.g., JAX, PyTorch and TensorFlow), which are computationally demanding and introduce truncation errors, solver instabilities, and substantial overhead. These limitations are particularly acute for the ODE systems of conceptual watershed models. Here we introduce a fully analytic and computationally efficient framework for differentiable hydrologic modeling based on exact parameter sensitivities. By augmenting the governing ODE system with sensitivity equations, we jointly evolve the model states and the Jacobian matrix with respect to all parameters. This Jacobian then provides fully analytic gradient vectors for any differentiable loss function. These include classical objective functions such as the sum of absolute and squared residuals, widely used hydrologic performance metrics such as the Nash-Sutcliffe and Kling-Gupta efficiencies, robust loss functions that down-weight extreme events, and hydrograph-based functionals such as flow-duration and recession curves. The analytic sensitivities eliminate the step-size dependence and noise inherent to numerical differentiation, while avoiding the instability of adjoint methods and the overhead of modern machine-learning autodiff toolchains. The resulting gradients are deterministic, physically interpretable, and straightforward to embed in gradient-based optimizers. Overall, this work enables rapid, stable, and transparent gradient-based calibration of conceptual hydrologic models, unlocking the full potential of differentiable modeling without reliance on external, opaque, or CPU-intensive automatic-differentiation libraries.",
        "keywords": [
          "physics.geo-ph",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06429v1",
        "authors": [
          "Jasper A. Vrugt",
          "Jonathan M. Frame",
          "Ethan Bollman"
        ],
        "arxiv_categories": [
          "physics.geo-ph",
          "cs.LG"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.465154",
      "entities": [
        "Conceptual Hydrologic Models Conceptual",
        "Reclaiming First Principles",
        "Differentiable Framework",
        "Framework",
        "NIST",
        "MIT",
        "CPU",
        "Act",
        "JAX",
        "ODE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06287v1",
      "title": "Toward generative machine learning for boosting ensembles of climate simulations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06287v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Accurately quantifying uncertainty in predictions and projections arising from irreducible internal climate variability is critical for informed decision making. Such uncertainty is typically assessed using ensembles produced with physics based climate models. However, computational constraints impose a trade off between generating the large ensembles required for robust uncertainty estimation and increasing model resolution to better capture fine scale dynamics. Generative machine learning offers a promising pathway to alleviate these constraints. We develop a conditional Variational Autoencoder (cVAE) trained on a limited sample of climate simulations to generate arbitrary large ensembles. The approach is applied to output from monthly CMIP6 historical and future scenario experiments produced with the Canadian Centre for Climate Modelling and Analysis' (CCCma's) Earth system model CanESM5. We show that the cVAE model learns the underlying distribution of the data and generates physically consistent samples that reproduce realistic low and high moment statistics, including extremes. Compared with more sophisticated generative architectures, cVAEs offer a mathematically transparent, interpretable, and computationally efficient framework. Their simplicity lead to some limitations, such as overly smooth outputs, spectral bias, and underdispersion, that we discuss along with strategies to mitigate them. Specifically, we show that incorporating output noise improves the representation of climate relevant multiscale variability, and we propose a simple method to achieve this. Finally, we show that cVAE-enhanced ensembles capture realistic global teleconnection patterns, even under climate conditions absent from the training data.",
        "keywords": [
          "physics.ao-ph",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06287v1",
        "authors": [
          "Parsa Gooya",
          "Reinel Sospedra-Alfonso",
          "Johannes Exenberger"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "cs.AI",
          "cs.LG"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.465582",
      "entities": [
        "Variational Autoencoder",
        "Climate Modelling",
        "Machine Learning",
        "Canadian Centre",
        "Framework",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06279v1",
      "title": "Structural barriers to complete homogenization and wormholing in dissolving porous and fractured rocks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06279v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Dissolution in porous media and fractured rocks alters both the chemical composition of the fluid and the physical properties of the solid. Depending on system conditions, reactive flow may enlarge pores uniformly, widen pre-existing channels, or trigger instabilities that form wormholes. The resulting pattern reflects feedbacks among advection, diffusion, surface reaction, and the initial heterogeneity of the medium. Porous and fractured media can exhibit distinct characteristics -- for example, the presence of large fractures can significantly alter the network topology and overall connectivity of the system. We quantify these differences with three network models -- a regular pore network, a disordered pore network, and a discrete fracture network -- evaluated with a unified metric: the flow focusing profile. This metric effectively captures evolution of flow paths across all systems: it reveals a focusing front that propagates from the inlet in the wormholing regime, a system-wide decrease in focusing during uniform dissolution, and the progressive enlargement of pre-existing flow paths in the channeling regime. The metric shows that uniform dissolution cannot eliminate heterogeneity resulting from the network topology. This structural heterogeneity -- rather than just pore-diameter or fracture-aperture variance -- sets a fundamental limit on flow homogenization and must be accounted for when upscaling dissolution kinetics from pore or fracture scale to the reservoir level.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06279v1",
        "authors": [
          "Tomasz Szawełło",
          "Jeffrey D. Hyman",
          "Peter K. Kang"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.465954",
      "entities": [
        "Fusion",
        "MIT",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06200v1",
      "title": "Threshold Resource Redistribution in Spatially-Structured Kinship Networks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06200v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "We present a model for a threshold-based resource redistribution process in a spatially-explicit population, characterizing the relation between kinship network structure, local interactions and persistence. We find that population survival becomes possible for lower resource densities, but leads to increased network heterogeneity and locally centralized clusters. We interpret this in relation to a feedback between the kinship network structure and reproduction ability. Agents receive stochastic resources and solicit additional resources from connected individuals when below a minimum, with each agent contributing a fraction of their excess based on relatedness. We first analyze a fully-connected population with uniform redistribution fraction and discuss mean field expectations as well as finite size corrections. We extend this model to a hub-and-spoke network, exploring the impact of network asymmetry or centrality on resource distribution. We then develop a spatially-limited population model with diffusion, local pairing, reproduction and mortality. Redistribution is introduced as a function of relatedness (generational distance through most-recent common ancestor) and distance. Redistribution-dependent populations exhibit a higher level of relational closeness with increased clustering for agents of highest node strength. These results highlight the interaction of resource density, cooperation and kinship in a spatially-limited regime.",
        "keywords": [
          "nlin.AO",
          "physics.soc-ph",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06200v1",
        "authors": [
          "Alina Kochocki"
        ],
        "arxiv_categories": [
          "nlin.AO",
          "physics.soc-ph",
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.466313",
      "entities": [
        "Threshold Resource Redistribution",
        "Structured Kinship Networks We",
        "Fusion",
        "MIT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06106v1",
      "title": "To clean or not to clean: The free-rider problem in sequentially shared resources",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06106v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Shared resources enhance productivity yet at the same time provide channels for biological and digital contamination, turning physical or digital hygiene into a cooperation dilemma prone to free-riding. Here we introduce a game of sequential sharing of common resources, an empirically parameterized evolutionary model of population dynamics in sequential-use settings such as gyms and shared workspaces. The success of the strategies implemented in the model, such as cleaning equipment before or after use, are based on the trade-offs between cleaning costs, contamination risk, and social incentives to mitigate disease transmission. We find that cooperative hygiene can be achieved by lowering the effective costs of cleaning, strengthening pro-social incentives, and monitoring population-level noncompliance. Remarkably, stability of fully altruistic populations is primarily affected by the cleaning costs. In contrast, increasing effective infection costs, for example through punishment, appears less important in this case. The model's evolutionary dynamics exhibit multi-stability, hysteresis, and abrupt shifts in strategy composition, broadly consistent with empirical observations from shared-use facilities. Our framework offers testable predictions and is amenable to quantitative calibration with behavioral and environmental data. Our predictions can be used to inform the design of cost-effective public health and digital security policies.",
        "keywords": [
          "physics.soc-ph",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06106v1",
        "authors": [
          "Alexander Feigel",
          "Alexandre V. Morozov"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.466674",
      "entities": [
        "Framework",
        "MIT",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05917v1",
      "title": "The Effects of Non-ideal Mixing in Planetary Magma Oceans and Atmospheres",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05917v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Sub-Neptunes with hydrogen-rich envelopes are expected to sustain long-lived magma oceans that continuously exchange volatiles with their overlying atmospheres. Capturing these interactions is key to understanding the chemical evolution and present-day diversity of sub-Neptunes, super-Earths, and terrestrial planets, particularly in light of new JWST observations and upcoming missions. Recent advances in both geochemistry and astrophysics now allow the integration of experimental constraints and thermodynamic models across melt, metal, and gas phases. Here we extend a global chemical equilibrium model to include non-ideal behavior in all three phases. Our framework combines fugacity corrections for gas species with activity coefficients for silicate and metal species, enabling a fully coupled description of volatile partitioning. We show that for planetary embryos (0.5 M$_\\oplus$ at 2350 K), non-ideality introduces only modest corrections to atmosphere-magma ocean interface (AMOI) pressures, volatile inventories, and interior compositions. In contrast, for sub-Neptunes with higher temperatures ($\\approx$ 3000 K) and pressures, non-ideal effects are more pronounced, though still modest in absolute terms$-$typically within 20% and at most a factor of two. Including activity and fugacity coefficients simultaneously increases the AMOI pressure, enhances water retention in the mantle and the envelope. Our results demonstrate that non-ideality must be treated globally: applying corrections to only one phase leads to incomplete or even misleading trends. These findings highlight the importance of self-consistent global thermodynamic treatments for interpreting atmospheric spectra and interior structures of sub-Neptunes and super-Earths.",
        "keywords": [
          "astro-ph.EP",
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05917v1",
        "authors": [
          "Aaron Werlen",
          "Edward D. Young",
          "Hilke E. Schlichting"
        ],
        "arxiv_categories": [
          "astro-ph.EP",
          "physics.geo-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.467096",
      "entities": [
        "Planetary Magma Oceans",
        "Atmospheres Sub",
        "Framework",
        "Hydrogen",
        "Meta",
        "AMOI",
        "JWST",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05583v1",
      "title": "Intermittent precipitation and spatial Allee effects drive irregular vegetation patterns in semiarid ecosystems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05583v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Vegetation in semi-arid ecosystems frequently organizes into spatially heterogeneous mosaics that regulate ecosystem functioning, productivity, and resilience. These patterns arise from local biological interactions, including facilitation among neighboring plants and competition for limiting resources. Classical theoretical approaches have attributed such organization to scale-dependent feedbacks, predicting regular spatial patterns and abrupt transitions to collapse. However, growing empirical and theoretical evidence reveal that environmental variability and demographic stochasticity can fundamentally reshape spatial organization, driving irregular clusters, dynamic mosaics, and gradual rather than catastrophic vegetation declines. In drylands, rainfall variability is a dominant source of environmental forcing: precipitation typically occurs in short, irregular pulses that transiently enhance survival and recruitment before competitive interactions again dominate. Near persistence thresholds, ecosystem dynamics are therefore governed not only by average climatic conditions but also by the timing and spatial coincidence of favorable events. Under these conditions, positive density dependence and local facilitation can critically determine whether vegetation patches persist, expand, or collapse. Here, we develop an individual-based model that integrates intermittent precipitation with local Allee effects to examine how stochastic rainfall shapes spatial organization and persistence. We show that the interaction between pulsed resource availability and density-dependent survival generates irregular cluster structures and strongly modulates extinction risk, with resilience emerging from local spatial covariance and neighborhood density rather than from total biomass alone. These results highlight the importance of individual-level, stochastic processes in determining ecosystem resilience.",
        "keywords": [
          "nlin.AO",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05583v1",
        "authors": [
          "Àlex Giménez-Romero",
          "Bernard A. Afful",
          "Priscilla E. Greenwood"
        ],
        "arxiv_categories": [
          "nlin.AO",
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.467552",
      "entities": [
        "MIT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05560v1",
      "title": "Depth estimation of a monoharmonic source using a vertical linear array at fixed distance",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05560v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Estimating the depth of a monoharmonic sound source at a fixed range using a vertical linear array (VLA) is challenging in the absence of seabed environmental parameters, and relevant research remains scarce. The orthogonality constrained modal search based depth estimation (OCMS-D) method is proposed in this paper, which enables the estimation of the depth of a monoharmonic source at a fixed range using a VLA under unknown seabed parameters. Using the sparsity of propagating normal modes and the orthogonality of mode depth functions, OCMS-D estimates the normal mode parameters under a fixed source-array distance at first. The estimated normal mode parameters are then used to estimate the source depth. To ensure the precision of the source depth estimation, the method utilizes information on both the amplitude distribution and the sign (positive/negative) patterns of the estimated mode depth functions at the inferred source depth. Numerical simulations evaluate the performance of OCMS-D under different conditions. The effectiveness of OCMS-D is also verified by the Yellow Sea experiment and the SWellEx-96 experiment. In the Yellow Sea experiment, the depth estimation absolute errors by OCMS-D with a 4-second time window are less than 2.4 m. And the depth estimation absolute errors in the SWellEx-96 experiment with a 10-second time window are less than 5.4 m for the shallow source and less than 10.8 m for the deep source.",
        "keywords": [
          "physics.ao-ph",
          "eess.SP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05560v1",
        "authors": [
          "Yangjin Xu",
          "Wei Gao",
          "Xiaolei Li"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "eess.SP"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.467921",
      "entities": [
        "Yellow Sea",
        "SWellEx-96",
        "Wind",
        "OCMS",
        "VLA",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05416v1",
      "title": "Reduced-Order Surrogates for Forced Flexible Mesh Coastal-Ocean Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05416v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "While POD-based surrogates are widely explored for hydrodynamic applications, the use of Koopman Autoencoders for real-world coastal-ocean modelling remains relatively limited. This paper introduces a flexible Koopman autoencoder formulation that incorporates meteorological forcings and boundary conditions, and systematically compares its performance against POD-based surrogates. The Koopman autoencoder employs a learned linear temporal operator in latent space, enabling eigenvalue regularization to promote temporal stability. This strategy is evaluated alongside temporal unrolling techniques for achieving stable and accurate long-term predictions. The models are assessed on three test cases spanning distinct dynamical regimes, with prediction horizons up to one year at 30-minute temporal resolution. Across all cases, the Koopman autoencoder with temporal unrolling yields the best overall accuracy compared to the POD-based surrogates, achieving relative root-mean-squared-errors of 0.01-0.13 and $R^2$-values of 0.65-0.996. Prediction errors are largest for current velocities, and smallest for water surface elevations. Comparing to in-situ observations, the surrogate yields -0.65% to 12% change in water surface elevation prediction error when compared to prediction errors of the physics-based model. These error levels, corresponding to a few centimeters, are acceptable for many practical applications, while inference speed-ups of 300-1400x enables workflows such as ensemble forecasting and long climate simulations for coastal-ocean modelling.",
        "keywords": [
          "cs.CE",
          "physics.ao-ph",
          "cs.AI",
          "physics.flu-dyn",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05416v1",
        "authors": [
          "Freja Høgholm Petersen",
          "Jesper Sandvig Mariegaard",
          "Rocco Palmitessa"
        ],
        "arxiv_categories": [
          "cs.CE",
          "physics.ao-ph",
          "cs.AI",
          "physics.flu-dyn",
          "cs.LG"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.468310",
      "entities": [
        "Forced Flexible Mesh Coastal",
        "Koopman Autoencoders",
        "Ocean Models While",
        "Order Surrogates",
        "MIT",
        "POD",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05274v1",
      "title": "Specieslike clusters based on identical ancestor points",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05274v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "We introduce several axioms which may or may not hold for any given subgraph of the directed graph of all organisms (past, present and future) where edges represent biological parenthood, with the simplifying background assumption that life does not go extinct. We argue these axioms are plausible for species: if one were to define species based purely on genealogical relationships, it would be reasonable to define them in such a way as to satisfy these axioms. The main axiom we introduce, which we call the identical ancestor point axiom, states that for any organism in any species, either the species contains at most finitely many descendants of that organism, or else the species contains at most finitely many non-descendants of that organism. We show that this (together with a convexity axiom) reduces the subjectivity of species, in a technical sense. We call connected sets satisfying these two axioms \"specieslike clusters.\" We consider the question of identifying a set of biologically plausible constraints that would guarantee every organism inhabits a maximal specieslike cluster subject to those constraints. We provide one such set consisting of two constraints and show that no proper subset thereof suffices.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05274v1",
        "authors": [
          "Samuel Allen Alexander"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.468611",
      "entities": [
        "DOE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05248v2",
      "title": "Thermodynamic Origin of Degree-Day Scaling in Phase-Change Systems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05248v2",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Phase transitions impose topological constraints on thermodynamic state variables, masking energetic fluctuations at the phase boundary. This constraint is most apparent in melting systems, where temperature remains pinned despite continued energy input. Here we resolve this information loss by introducing a latent temperature-a counterfactual trajectory describing the system's unconstrained thermal evolution. We show that energy conservation alone enforces a rigorous duality between the total latent heat dissipated during phase change and the accumulated exceedance of the latent temperature above the melting point. This duality is mathematically equivalent to the one-dimensional Wasserstein-1 distance between the latent and observed temperature trajectories, with the transport cost set by a characteristic surface dissipation timescale and melting energy. Applied to ice-sheet surface melting, this timescale admits a direct physical interpretation in terms of radiative and turbulent heat loss. The same framework yields a first-principles derivation of the empirical Positive Degree Day law and predicts realistic degree-day factors that emerge from surface energy balance, without ad hoc calibration. More broadly, phase change emerges as an optimal transport process that projects continuous energetic variability onto a constrained thermodynamic boundary.",
        "keywords": [
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05248v2",
        "authors": [
          "Zhiang Xie"
        ],
        "arxiv_categories": [
          "physics.ao-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.468950",
      "entities": [
        "Change Systems Phase",
        "Positive Degree Day",
        "Wasserstein-1",
        "Day Scaling",
        "Framework",
        "MIT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05237v1",
      "title": "On the Adversarial Robustness of Hydrological Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05237v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "The evaluation of hydrological models is essential for both model selection and reliability assessment. However, simply comparing predictions to observations is insufficient for understanding the global landscape of model behavior. This is especially true for many deep learning models, whose structures are complex. Further, in risk-averse operational settings, water managers require models that are trustworthy and provably safe, as non-robustness can put our critical infrastructure at risk. Motivated by the need to select reliable models for operational deployment, we introduce and explore adversarial robustness analysis in hydrological modeling, evaluating whether small, targeted perturbations to meteorological forcings induce substantial changes in simulated discharge. We compare physical-conceptual and deep learning-based hydrological models across 1,347 German catchments under perturbations of varying magnitudes, using the fast gradient sign method (FGSM). We find that, as expected, the FGSM perturbations systematically reduce KGE and increase MSE. However, catastrophic failure is rare and, surprisingly, LSTMs generally demonstrate greater robustness than HBV models. Further, changes in both the predicted hydrographs and the internal model states often respond approximately linearly (at least locally) as perturbation size increases, providing a compact summary of how errors grow under such perturbations. Similar patterns are also observed for random perturbations, suggesting that small input changes usually introduce approximately proportional changes in model output. Overall, these findings support further consideration of LSTMs for operational deployment (due both to their predictive power and robustness), and motivate future work on both characterizing model responses to input changes and improving robustness through architectural modifications and training design.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05237v1",
        "authors": [
          "Yang Yang",
          "Joseph Janssen",
          "Hoshin Gupta"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.469395",
      "entities": [
        "Adversarial Robustness",
        "Deep Learning",
        "FGSM",
        "KGE",
        "WHO",
        "MSE",
        "Act",
        "HBV",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05196v1",
      "title": "Learning virulence-transmission relationships using causal inference",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05196v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "The relationship between traits that influence pathogen virulence and transmission is part of the central canon of the evolution and ecology of infectious disease. However, identifying directional and mechanistic relationships among traits remains a key challenge in various subfields of biology, as models often assume static, fixed links between characteristics. Here, we introduce learning evolutionary trait relationships (LETR), a data-driven framework that applies Granger-causality principles to determine which traits drive others and how these relationships change over time. LETR integrates causal discovery with generative mapping and transfer-operator analysis to link short-term predictability with long-term trait distributions. Using a synthetic myxomatosis virus-host data set, we show that LETR reliably recovers known directional influences, such as virulence driving transmission. Applying the framework to global pandemic (SARS-CoV-2) data, we find that past virulence improves future transmission prediction, while the reverse effect is weak. Invariant-density estimates reveal a long-term trend toward low virulence and transmission, with bimodality in virulence suggesting ecological influences or host heterogeneity. In summary, this study provides a blueprint for learning the relationship between how harmful a pathogen is and how well it spreads, which is highly idiosyncratic and context-dependent. This finding undermines simplistic models and encourages the development of new theory for the constraints underlying pathogen evolution. Further, by uniting causal inference with dynamical modeling, the LETR framework offers a general approach for uncovering mechanistic trait linkages in complex biological systems of various kinds.",
        "keywords": [
          "q-bio.QM",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05196v1",
        "authors": [
          "Sudam Surasinghe",
          "C. Brandon Ogbunugafor"
        ],
        "arxiv_categories": [
          "q-bio.QM",
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.469798",
      "entities": [
        "Framework",
        "CoV-2",
        "SARS",
        "LETR",
        "NIST",
        "NSF",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05118v1",
      "title": "Combination therapy for colorectal cancer with anti-PD-L1 and cancer vaccine: A multiscale mathematical model of tumor-immune interactions",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05118v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "The tumor-immune system plays a critical role in colorectal cancer progression. Recent preclinical and clinical studies showed that combination therapy with anti-PD-L1 and cancer vaccines improved treatment response. In this study, we developed a multiscale mathematical model of interactions among tumors, immune cells, and cytokines to investigate tumor evolutionary dynamics under different therapeutic strategies. Additionally, we established a computational framework based on approximate Bayesian computation to generate virtual tumor samples and capture inter-individual heterogeneity in treatment response. The results demonstrated that a multiple low-dose regimen significantly reduced advanced tumor burden compared to baseline treatment in anti-PD-L1 therapy. In contrast, the maximum dose therapy yielded superior tumor growth control in cancer vaccine therapy. Furthermore, cytotoxic T cells were identified as a consistent predictive biomarker both before and after treatment initiation. Notably, the cytotoxic T cells-to-regulatory T cells ratio specifically served as a robust pre-treatment predictive biomarker, offering potential clinical utility for patient stratification and therapy personalization.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05118v1",
        "authors": [
          "Chenghang Li",
          "Haifeng Zhang",
          "Xiulan Lai"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.470082",
      "entities": [
        "Framework",
        "Vaccine",
        "Act",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05083v1",
      "title": "Large-Ensemble Simulations Reveal Links Between Atmospheric Blocking Frequency and Sea Surface Temperature Variability",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05083v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "Atmospheric blocking events drive persistent weather extremes in midlatitudes, but isolating the influence of sea surface temperature (SST) from chaotic internal atmospheric variability on these events remains a challenge. We address this challenge using century-long (1900-2010), large-ensemble simulations with two computationally efficient deep-learning general circulation models. We find these models skillfully reproduce the observed blocking climatology, matching or exceeding the performance of a traditional high-resolution model and representative CMIP6 models. Averaging the large ensembles filters internal atmospheric noise to isolate the SST-forced component of blocking variability, yielding substantially higher correlations with reanalysis than for individual ensemble members. We identify robust teleconnections linking Greenland blocking frequency to North Atlantic SST and El Niño-like patterns. Furthermore, SST-forced trends in blocking frequency show a consistent decline in winter over Greenland, and an increase over Europe. These results demonstrate that SST variability exerts a significant and physically interpretable influence on blocking frequency and establishes large ensembles from deep learning models as a powerful tool for separating forced SST signals from internal noise.",
        "keywords": [
          "physics.ao-ph",
          "cs.AI",
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05083v1",
        "authors": [
          "Zilu Meng",
          "Gregory J. Hakim",
          "Wenchang Yang"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "cs.AI",
          "physics.geo-ph"
        ]
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-09T15:11:31.470676",
      "entities": [
        "Between Atmospheric Blocking Frequency",
        "Sea Surface Temperature Variability",
        "Ensemble Simulations Reveal Links",
        "North Atlantic",
        "Deep Learning",
        "SST",
        "EPA",
        "EU",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.06916v1",
      "title": "Understanding Workplace Relatedness Support among Healthcare Professionals: A Four-Layer Model and Implications for Technology Design",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06916v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Healthcare professionals (HCPs) face increasing occupational stress and burnout. Supporting HCPs need for relatedness is fundamental to their psychological wellbeing and resilience. However, how technologies could support HCPs relatedness in the workplace remains less explored. This study incorporated semi-structured interviews (n = 15) and co-design workshops (n = 21) with HCPs working in the UK National Health Service (NHS), to explore their current practices and preferences for workplace relatedness support, and how technology could be utilized to benefit relatedness. Qualitative analysis yielded a four-layer model of HCPs relatedness need, which includes Informal Interactions, Camaraderie and Bond, Community and Organizational Care, and Shared Identity. Workshops generated eight design concepts (e.g., Playful Encounter, Collocated Action, and Memories and Stories) that operationalize the four relatedness need layers. We conclude by highlighting the theoretical relevance, practical design implications, and the necessity to strengthen relatedness support for HCPs in the era of digitalization and artificial intelligence.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06916v1",
        "authors": [
          "Zheyuan Zhang",
          "Dorian Peters",
          "Lan Xiao"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.922676",
      "entities": [
        "Understanding Workplace Relatedness Support",
        "Technology Design Healthcare",
        "Healthcare Professionals",
        "National Health Service",
        "Artificial Intelligence",
        "Informal Interactions",
        "Organizational Care",
        "Playful Encounter",
        "Collocated Action",
        "Shared Identity",
        "Layer Model",
        "Intel",
        "NHS",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.06915v1",
      "title": "Directing Space: Rehearsing Architecture as Performer with Explainable AI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06915v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "As AI systems increasingly become embedded in interactive and im-mersive artistic environments, artists and technologists are discovering new opportunities to engage with their interpretive and autonomous capacities as creative collaborators in live performance. The focus of this work-in-progress is on outlining conceptual and technical foundations under which performance-makers and interactive architecture can collaborate within rehearsal settings. It introduces a rehearsal-oriented prototype system for shaping and testing AI-mediated environments within creative practice. This approach treats interactive architecture as a performative agent that senses spatial behaviour and speech, interprets these signals through a large language model, and generates real-time environmental adaptations. Designed for deployment in physical performance spaces, the system employs virtual blueprints to support iterative experimentation and creative dialogue between artists and AI agents, using reasoning traces to inform architectural interaction design grounded in dramaturgical principles.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06915v1",
        "authors": [
          "Pavlos Panagiotidis",
          "Jocelyn Spence",
          "Nils Jaeger"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.923022",
      "entities": [
        "Rehearsing Architecture",
        "Directing Space",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06910v1",
      "title": "Assessment of evidence against homogeneity in exhaustive subgroup treatment effect plots",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06910v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Exhaustive subgroup treatment effect plots are constructed by displaying all subgroup treatment effects of interest against subgroup sample size, providing a useful overview of the observed treatment effect heterogeneity in a clinical trial. As in any exploratory subgroup analysis, however, the observed estimates suffer from small sample sizes and multiplicity issues. To facilitate more interpretable exploratory assessments, this paper introduces a computationally efficient method to generate homogeneity regions within exhaustive subgroup treatment effect plots. Using the Doubly Robust (DR) learner, pseudo-outcomes are used to estimate subgroup effects and derive reference distributions, quantifying how surprising observed heterogeneity is under a homogeneous effects model. Explicit formulas are derived for the homogeneity region and different methods for calculation of the critical values are compared. The method is illustrated with a cardiovascular trial and evaluated via simulation, showing well-calibrated inference and improved performance over standard approaches using simple differences of observed group means.",
        "keywords": [
          "stat.AP",
          "stat.ME"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06910v1",
        "authors": [
          "Björn Bornkamp",
          "Jiarui Lu",
          "Frank Bretz"
        ],
        "arxiv_categories": [
          "stat.AP",
          "stat.ME"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.923312",
      "entities": [
        "Doubly Robust",
        "Standard",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06047v1",
      "title": "Git for Sketches: An Intelligent Tracking System for Capturing Design Evolution",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06047v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "During product conceptualization, capturing the non-linear history and cognitive intent is crucial. Traditional sketching tools often lose this context. We introduce DIMES (Design Idea Management and Evolution capture System), a web-based environment featuring sGIT (SketchGit), a custom visual version control architecture, and Generative AI. sGIT includes AEGIS, a module using hybrid Deep Learning and Machine Learning models to classify six stroke types. The system maps Git primitives to design actions, enabling implicit branching and multi-modal commits (stroke data + voice intent). In a comparative study, experts using DIMES demonstrated a 160% increase in breadth of concept exploration. Generative AI modules generated narrative summaries that enhanced knowledge transfer; novices achieved higher replication fidelity (Neural Transparency-based Cosine Similarity: 0.97 vs. 0.73) compared to manual summaries. AI-generated renderings also received higher user acceptance (Purchase Likelihood: 4.2 vs 3.1). This work demonstrates that intelligent version control bridges creative action and cognitive documentation, offering a new paradigm for design education.",
        "keywords": [
          "cs.HC",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06047v1",
        "authors": [
          "Sankar B",
          "Amogh A S",
          "Sandhya Baranwal"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.923696",
      "entities": [
        "Capturing Design Evolution During",
        "An Intelligent Tracking System",
        "Design Idea Management",
        "Neural Transparency",
        "Purchase Likelihood",
        "Cosine Similarity",
        "Machine Learning",
        "Deep Learning",
        "Intel",
        "DIMES",
        "AEGIS",
        "MIT",
        "Act",
        "NSF",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.06792v1",
      "title": "Redundant is Not Redundant: Automating Efficient Categorical Palette Design Unifying Color & Shape Encodings with CatPAW",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06792v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Colors and shapes are commonly used to encode categories in multi-class scatterplots. Designers often combine the two channels to create redundant encodings, aiming to enhance class distinctions. However, evidence for the effectiveness of redundancy remains conflicted, and guidelines for constructing effective combinations are limited. This paper presents four crowdsourced experiments evaluating redundant color-shape encodings and identifying high-performing configurations across different category numbers. Results show that redundancy significantly improves accuracy in assessing class-level correlations, with the strongest benefits for 5-8 categories. We also find pronounced interaction effects between colors and shapes, underscoring the need for careful pairing in designing redundant encodings. Drawing on these findings, we introduce a categorical palette design tool that enables designers to construct empirically grounded palettes for effective categorical visualization. Our work advances understanding of categorical perception in data visualization by systematically identifying effective redundant color-shape combinations and embedding these insights into a practical palette design tool.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06792v1",
        "authors": [
          "Chin Tseng",
          "Arran Zeyu Wang",
          "Ghulam Jilani Quadri"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.924127",
      "entities": [
        "Automating Efficient Categorical Palette",
        "Design Unifying Color",
        "Shape Encodings",
        "Not Redundant",
        "Guideline",
        "MIT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06778v1",
      "title": "Revisiting Emotions Representation for Recognition in the Wild",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06778v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Facial emotion recognition has been typically cast as a single-label classification problem of one out of six prototypical emotions. However, that is an oversimplification that is unsuitable for representing the multifaceted spectrum of spontaneous emotional states, which are most often the result of a combination of multiple emotions contributing at different intensities. Building on this, a promising direction that was explored recently is to cast emotion recognition as a distribution learning problem. Still, such approaches are limited in that research datasets are typically annotated with a single emotion class. In this paper, we contribute a novel approach to describe complex emotional states as probability distributions over a set of emotion classes. To do so, we propose a solution to automatically re-label existing datasets by exploiting the result of a study in which a large set of both basic and compound emotions is mapped to probability distributions in the Valence-Arousal-Dominance (VAD) space. In this way, given a face image annotated with VAD values, we can estimate the likelihood of it belonging to each of the distributions, so that emotional states can be described as a mixture of emotions, enriching their description, while also accounting for the ambiguous nature of their perception. In a preliminary set of experiments, we illustrate the advantages of this solution and a new possible direction of investigation. Data annotations are available at https://github.com/jbcnrlz/affectnet-b-annotation.",
        "keywords": [
          "cs.CV",
          "cs.HC",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06778v1",
        "authors": [
          "Joao Baptista Cardia Neto",
          "Claudio Ferrari",
          "Stefano Berretti"
        ],
        "arxiv_categories": [
          "cs.CV",
          "cs.HC",
          "cs.LG"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.924597",
      "entities": [
        "Revisiting Emotions Representation",
        "Wild Facial",
        "MIT",
        "VAD",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06759v1",
      "title": "\"Tab, Tab, Bug'': Security Pitfalls of Next Edit Suggestions in AI-Integrated IDEs",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06759v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Modern AI-integrated IDEs are shifting from passive code completion to proactive Next Edit Suggestions (NES). Unlike traditional autocompletion, NES is designed to construct a richer context from both recent user interactions and the broader codebase to suggest multi-line, cross-line, or even cross-file modifications. This evolution significantly streamlines the programming workflow into a tab-by-tab interaction and enhances developer productivity. Consequently, NES introduces a more complex context retrieval mechanism and sophisticated interaction patterns. However, existing studies focus almost exclusively on the security implications of standalone LLM-based code generation, ignoring the potential attack vectors posed by NES in modern AI-integrated IDEs. The underlying mechanisms of NES remain under-explored, and their security implications are not yet fully understood. In this paper, we conduct the first systematic security study of NES systems. First, we perform an in-depth dissection of the NES mechanisms to understand the newly introduced threat vectors. It is found that NES retrieves a significantly expanded context, including inputs from imperceptible user actions and global codebase retrieval, which increases the attack surfaces. Second, we conduct a comprehensive in-lab study to evaluate the security implications of NES. The evaluation results reveal that NES is susceptible to context poisoning and is sensitive to transactional edits and human-IDE interactions. Third, we perform a large-scale online survey involving over 200 professional developers to assess the perceptions of NES security risks in real-world development workflows. The survey results indicate a general lack of awareness regarding the potential security pitfalls associated with NES, highlighting the need for increased education and improved security countermeasures in AI-integrated IDEs.",
        "keywords": [
          "cs.HC",
          "cs.CR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06759v1",
        "authors": [
          "Yunlong Lyu",
          "Yixuan Tang",
          "Peng Chen"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.CR"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.925209",
      "entities": [
        "Next Edit Suggestions",
        "Security Pitfalls",
        "NES",
        "IDE",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06734v1",
      "title": "ClassAid: A Real-time Instructor-AI-Student Orchestration System for Classroom Programming Activities",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06734v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Generative AI is reshaping education, but it also raises concerns about instability and overreliance. In programming classrooms, we aim to leverage its feedback capabilities while reinforcing the educator's role in guiding student-AI interactions. We developed ClassAid, a real-time orchestration system that integrates TA Agents to provide personalized support and an AI-driven dashboard that visualizes student-AI interactions, enabling instructors to dynamically adjust TA Agent modes. Instructors can configure the Agent to provide technical feedback (direct coding solutions), heuristic feedback (hint-based guidance), automatic feedback (autonomously selecting technical or heuristic support), or silent operation (no AI support). We evaluated ClassAid through three aspects: (1) the TA Agents' performance, (2) feedback from 54 students and one instructor during a classroom deployment, and (3) interviews with eight educators. Results demonstrate that dynamic instructor control over AI supports effective real-time personalized feedback and provides design implications for integrating AI into authentic educational settings.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06734v1",
        "authors": [
          "Gefei Zhang",
          "Guodao Sun",
          "Meng Xia"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.925584",
      "entities": [
        "Classroom Programming Activities Generative",
        "Student Orchestration System",
        "Act",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.06714v1",
      "title": "PrefIx: Understand and Adapt to User Preference in Human-Agent Interaction",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06714v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "LLM-based agents can complete tasks correctly yet still frustrate users through poor interaction patterns, such as excessive confirmations, opaque reasoning, or misaligned pacing. Current benchmarks evaluate task accuracy but overlook how agents interact: whether they infer preferences from implicit cues, adapt dynamically, or maintain fine-grained interaction quality. We introduce Prefix, a configurable environment that evaluates both what agents accomplish and how they interact. Central to Prefix is the Interaction-as-a-Tool (IaaT) paradigm, which treats interaction behaviors as structured tool calls, unifying them with existing evaluation frameworks. We define 31 preference settings across 14 attributes and formalize user experience (UX) as a core metric alongside task accuracy. A composite LLM-as-a-Judge mechanism across seven UX dimensions achieves strong aggregate reliability (ICC > 0.79), high internal consistency (alpha = 0.943), and human correlation (rho = 0.52-0.78). Preference-aware agents show 7.6% average UX improvement and 18.5% gain in preference alignment. Our work is openly accessible.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06714v1",
        "authors": [
          "Jialin Li",
          "Zhenhao Chen",
          "Hanjun Luo"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.925945",
      "entities": [
        "Agent Interaction",
        "User Preference",
        "Framework",
        "Act",
        "ICC",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06678v1",
      "title": "Beyond Judgment: Exploring LLM as a Support System for Maternal Mental Health",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06678v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "In the age of Large Language Models (LLMs), much work has already been done on how LLMs support medication advice and serve as information providers; however, how mothers use these tools for emotional and informational support to avoid social judgment remains underexplored. In this study, we have conducted a 10-day mixed-methods exploratory survey (N=107) to investigate how mothers use LLMs as a non-judgmental resource for emotional support and regulation, as well as situational reassurance. Our findings show that mothers are asking LLMs various questions about childcare to reassure themselves and avoid judgment, particularly around childcare decisions, maternal guilt, and late-night caregiving. Open-ended responses also show that mothers are comfortable with LLMs because they do not have to think about social consequences or judgment. Although they use LLMs for quick information or reassurance to avoid judgment, the results also show that more than half of the participants value human warmth over LLMs; however, a significant minority, especially those who live in a joint family, consider LLMs to avoid human judgment. These findings help us understand how we can frame LLMs as low-risk interaction support rather than as a replacement for human support, and highlight the role of social context in shaping emotional technology use.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06678v1",
        "authors": [
          "Shayla Sharmin",
          "Sadia Afrin"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.926362",
      "entities": [
        "Maternal Mental Health In",
        "Large Language Models",
        "Beyond Judgment",
        "Support System",
        "Regulation",
        "WHO",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06674v1",
      "title": "CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06674v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "High-quality annotated datasets are crucial for advancing machine learning in medical image analysis. However, a critical gap exists: most datasets either offer a single, clean ground truth, which hides real-world expert disagreement, or they provide multiple annotations without a separate gold standard for objective evaluation. To bridge this gap, we introduce CytoCrowd, a new public benchmark for cytology analysis. The dataset features 446 high-resolution images, each with two key components: (1) raw, conflicting annotations from four independent pathologists, and (2) a separate, high-quality gold-standard ground truth established by a senior expert. This dual structure makes CytoCrowd a versatile resource. It serves as a benchmark for standard computer vision tasks, such as object detection and classification, using the ground truth. Simultaneously, it provides a realistic testbed for evaluating annotation aggregation algorithms that must resolve expert disagreements. We provide comprehensive baseline results for both tasks. Our experiments demonstrate the challenges presented by CytoCrowd and establish its value as a resource for developing the next generation of models for medical image analysis.",
        "keywords": [
          "cs.CV",
          "cs.HC",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06674v1",
        "authors": [
          "Yonghao Si",
          "Xingyuan Zeng",
          "Zhao Chen"
        ],
        "arxiv_categories": [
          "cs.CV",
          "cs.HC",
          "cs.LG"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.926732",
      "entities": [
        "Cytology Image Analysis High",
        "Annotator Benchmark Dataset",
        "Machine Learning",
        "Agreement",
        "Standard",
        "EPA",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06631v1",
      "title": "Estimating Exam Item Difficulty with LLMs: A Benchmark on Brazil's ENEM Corpus",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06631v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "As Large Language Models (LLMs) are increasingly deployed to generate educational content, a critical safety question arises: can these models reliably estimate the difficulty of the questions they produce? Using Brazil's high-stakes ENEM exam as a testbed, we benchmark ten proprietary and open-weight LLMs against official Item Response Theory (IRT) parameters for 1,031 questions. We evaluate performance along three axes: absolute calibration, rank fidelity, and context sensitivity across learner backgrounds. Our results reveal a significant trade-off: while the best models achieve moderate rank correlation, they systematically underestimate difficulty and degrade significantly on multimodal items. Crucially, we find that models exhibit limited and inconsistent plasticity when prompted with student demographic cues, suggesting they are not yet ready for context-adaptive personalization. We conclude that LLMs function best as calibrated screeners rather than authoritative oracles, supporting an \"evaluation-before-generation\" pipeline for responsible assessment design.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06631v1",
        "authors": [
          "Thiago Brant",
          "Julien Kühn",
          "Jun Pang"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.927071",
      "entities": [
        "Estimating Exam Item Difficulty",
        "Corpus As Large Language",
        "Using Brazil",
        "Oracle",
        "ENEM",
        "MIT",
        "IRT",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06613v1",
      "title": "DAVE: Distribution-aware Attribution via ViT Gradient Decomposition",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06613v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-level explanations, causing many existing methods to rely on coarse patch-level attributions. We introduce DAVE \\textit{(\\underline{D}istribution-aware \\underline{A}ttribution via \\underline{V}iT Gradient D\\underline{E}composition)}, a mathematically grounded attribution method for ViTs based on a structured decomposition of the input gradient. By exploiting architectural properties of ViTs, DAVE isolates locally equivariant and stable components of the effective input--output mapping. It separates these from architecture-induced artifacts and other sources of instability.",
        "keywords": [
          "cs.CV",
          "cs.AI",
          "cs.HC",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06613v1",
        "authors": [
          "Adam Wróbel",
          "Siddhartha Gairola",
          "Jacek Tabor"
        ],
        "arxiv_categories": [
          "cs.CV",
          "cs.AI",
          "cs.HC",
          "cs.LG"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.927351",
      "entities": [
        "Gradient Decomposition Vision Transformers",
        "Transformer",
        "DAVE",
        "EPA",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06607v1",
      "title": "Beyond Pairwise Distance: Cognitive Traversal Distance as a Holistic Measure of Scientific Novelty",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06607v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Scientific novelty is a critical construct in bibliometrics and is commonly measured by aggregating pairwise distances between the knowledge units underlying a paper. While prior work has refined how such distances are computed, less attention has been paid to how dyadic relations are aggregated to characterize novelty at the paper level. We address this limitation by introducing a network-based indicator, Cognitive Traversal Distance (CTD). Conceptualizing the historical literature as a weighted knowledge network, CTD is defined as the length of the shortest path required to connect all knowledge units associated with a paper. CTD provides a paper-level novelty measure that reflects the minimal structural distance needed to integrate multiple knowledge units, moving beyond mean- or quantile-based aggregation of pairwise distances. Using 27 million biomedical publications indexed by OpenAlex and Medical Subject Headings (MeSH) as standardized knowledge units, we evaluate CTD against expert-based novelty benchmarks from F1000Prime-recommended papers and Nobel Prize-winning publications. CTD consistently outperforms conventional aggregation-based indicators. We further show that MeSH-based CTD is less sensitive to novelty driven by the emergence of entirely new conceptual labels, clarifying its scope relative to recent text-based measures.",
        "keywords": [
          "cs.DL",
          "cs.CY",
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06607v1",
        "authors": [
          "Yi Xiang",
          "Pascal Welke",
          "Chengzhi Zhang"
        ],
        "arxiv_categories": [
          "cs.DL",
          "cs.CY",
          "econ.GN"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.927737",
      "entities": [
        "Scientific Novelty Scientific",
        "Cognitive Traversal Distance",
        "Medical Subject Headings",
        "Beyond Pairwise Distance",
        "Holistic Measure",
        "Nobel Prize",
        "Standard",
        "MIT",
        "Act",
        "CTD",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06596v1",
      "title": "Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06596v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. LLMs enable these messages to be personalised consistently across interactions, yet it remains unclear whether such personalisation improves individual messages or instead shapes users' perceptions through patterns of exposure. We explore this question in the context of LLM-generated JITAIs, which are short, context-aware messages delivered at moments deemed appropriate to support behaviour change, using physical activity as an application domain. In a controlled retrospective study, 90 participants evaluated messages generated using four LLM strategies: baseline prompting, few-shot prompting, fine-tuned models, and retrieval augmented generation, each implemented with and without Big Five Personality Traits to produce personality-aligned communication across multiple scenarios. Using ordinal multilevel models with within-between decomposition, we distinguish trial-level effects, whether personality information improves evaluations of individual messages, from person-level exposure effects, whether participants receiving higher proportions of personality-informed messages exhibit systematically different overall perceptions. Results showed no trial-level associations, but participants who received higher proportions of BFPT-informed messages rated the messages as more personalised, appropriate, and reported less negative affect. We use Communication Accommodation Theory for post-hoc analysis. These results suggest that personality-based personalisation in behaviour change systems may operate primarily through aggregate exposure rather than per-message optimisation, with implications for how adaptive systems are designed and evaluated in sustained human-AI interaction. In-situ longitudinal studies are needed to validate these findings in real-world contexts.",
        "keywords": [
          "cs.HC",
          "cs.CL",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06596v1",
        "authors": [
          "Dominik P. Hofer",
          "David Haag",
          "Rania Islambouli"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.CL",
          "cs.AI"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.928199",
      "entities": [
        "Big Five Personality Traits",
        "Relational Infrastructure",
        "Messaging Digital",
        "User Perceptions",
        "BFPT",
        "WHO",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06523v1",
      "title": "MicroBi-ConvLSTM: An Ultra-Lightweight Efficient Model for Human Activity Recognition on Resource Constrained Devices",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06523v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Human Activity Recognition (HAR) on resource constrained wearables requires models that balance accuracy against strict memory and computational budgets. State of the art lightweight architectures such as TinierHAR (34K parameters) and TinyHAR (55K parameters) achieve strong accuracy, but exceed memory budgets of microcontrollers with limited SRAM once operating system overhead is considered. We present MicroBi-ConvLSTM, an ultra-lightweight convolutional-recurrent architecture achieving 11.4K parameters on average through two stage convolutional feature extraction with 4x temporal pooling and a single bidirectional LSTM layer. This represents 2.9x parameter reduction versus TinierHAR and 11.9x versus DeepConvLSTM while preserving linear O(N) complexity. Evaluation across eight diverse HAR benchmarks shows that MicroBi-ConvLSTM maintains competitive performance within the ultra-lightweight regime: 93.41% macro F1 on UCI-HAR, 94.46% on SKODA assembly gestures, and 88.98% on Daphnet gait freeze detection. Systematic ablation reveals task dependent component contributions where bidirectionality benefits episodic event detection, but provides marginal gains on periodic locomotion. INT8 post training quantization incurs only 0.21% average F1-score degradation, yielding a 23.0 KB average deployment footprint suitable for memory constrained edge devices.",
        "keywords": [
          "cs.CV",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06523v1",
        "authors": [
          "Mridankan Mandal"
        ],
        "arxiv_categories": [
          "cs.CV",
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.928555",
      "entities": [
        "Resource Constrained Devices Human",
        "Lightweight Efficient Model",
        "Human Activity Recognition",
        "Activity Recognition",
        "An Ultra",
        "SKODA",
        "LSTM",
        "SRAM",
        "MIT",
        "HAR",
        "Act",
        "UCI",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.06506v1",
      "title": "Designing Computational Tools for Exploring Causal Relationships in Qualitative Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06506v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Exploring causal relationships for qualitative data analysis in HCI and social science research enables the understanding of user needs and theory building. However, current computational tools primarily characterize and categorize qualitative data; the few systems that analyze causal relationships either inadequately consider context, lack credibility, or produce overly complex outputs. We first conducted a formative study with 15 participants interested in using computational tools for exploring causal relationships in qualitative data to understand their needs and derive design guidelines. Based on these findings, we designed and implemented QualCausal, a system that extracts and illustrates causal relationships through interactive causal network construction and multi-view visualization. A feedback study (n = 15) revealed that participants valued our system for reducing the analytical burden and providing cognitive scaffolding, yet navigated how such systems fit within their established research paradigms, practices, and habits. We discuss broader implications for designing computational tools that support qualitative data analysis.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06506v1",
        "authors": [
          "Han Meng",
          "Qiuyuan Lyu",
          "Peinuan Qin"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.CL"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.928849",
      "entities": [
        "Exploring Causal Relationships",
        "Designing Computational Tools",
        "Qualitative Data Exploring",
        "Guideline",
        "HCI",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06489v1",
      "title": "Simulating Word Suggestion Usage in Mobile Typing to Guide Intelligent Text Entry Design",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06489v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Intelligent text entry (ITE) methods, such as word suggestions, are widely used in mobile typing, yet improving ITE systems is challenging because the cognitive mechanisms behind suggestion use remain poorly understood, and evaluating new systems often requires long-term user studies to account for behavioral adaptation. We present WSTypist, a reinforcement learning-based model that simulates how typists integrate word suggestions into typing. It builds on recent hierarchical control models of typing, but focuses on the cognitive mechanisms that underlie the high-level decision-making for effectively integrating word suggestions into manual typing: assessing efficiency gains, considering orthographic uncertainties, and including personal reliance on AI support. Our evaluations show that WSTypist simulates diverse human-like suggestion-use strategies, reproduces individual differences, and generalizes across different systems. Importantly, we demonstrate on four design cases how computational rationality models can be used to inform what-if analyses during the design process, by simulating how users might adapt to changes in the UI or in the algorithmic support, reducing the need for long-term user studies.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06489v1",
        "authors": [
          "Yang Li",
          "Anna Maria Feit"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.929167",
      "entities": [
        "Simulating Word Suggestion Usage",
        "Guide Intelligent Text Entry",
        "Design Intelligent",
        "Mobile Typing",
        "Intel",
        "ITE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06437v1",
      "title": "An attention economy model of co-evolution between content quality and audience selectivity",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06437v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Human attention has become a scarce and strategically contested resource in digital environments. Content providers increasingly engage in excessive competition for visibility, often prioritizing attention-grabbing tactics over substantive quality. Despite extensive empirical evidence, however, there is a lack of theoretical models that explain the fundamental dynamics of the attention economy. Here, we develop a minimal mathematical framework to explain how content quality and audience attention coevolve under limited attention capacity. Using an evolutionary game approach, we model strategic feedback between providers, who decide how much effort to invest in production, and consumers, who choose whether to search selectively for high-quality content or to engage passively. Analytical and numerical results reveal three characteristic regimes of content dynamics: collapse, boundary, and coexistence. The transitions between these regimes depend on how effectively audiences can distinguish content quality. When audience discriminability is weak, both selective attention and high-quality production vanish, leading to informational collapse. When discriminability is sufficient and incentives are well aligned, high- and low-quality content dynamically coexist through feedback between audience selectivity and providers' effort. These findings identify two key conditions for sustaining a healthy information ecosystem: adequate discriminability among audiences and sufficient incentives for high-effort creation. The model provides a theoretical foundation for understanding how institutional and platform designs can prevent the degradation of content quality in the attention economy.",
        "keywords": [
          "physics.soc-ph",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06437v1",
        "authors": [
          "Masaki Chujyo",
          "Isamu Okada",
          "Hitoshi Yamamoto"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "cs.CY"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.929588",
      "entities": [
        "Framework",
        "MIT",
        "WHO",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06396v1",
      "title": "InterFlow: Designing Unobtrusive AI to Empower Interviewers in Semi-Structured Interviews",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06396v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Semi-structured interviews are a common method in qualitative research. However, conducting high-quality interviews is challenging, as it requires interviewers to actively listen to participants, adapt their plans as the conversation unfolds, and probe effectively. We propose InterFlow, an AI-powered visual scaffold that helps interviewers manage the interview flow and facilitates real-time data sensemaking. The system dynamically adapts the interview script to the ongoing conversation and provides a visual timer to track interview progress and conversational balance. It further supports information capture with three levels of automation: manual entry, AI-assisted summary with user-specified focus, and a co-interview agent that proactively surfaces potential follow-up points. A within-subject user study (N = 12) indicates that InterFlow reduces interviewers' cognitive load and facilitates the interview process. Based on the user study findings, we provide design implications for unobtrusive and agency-preserving AI assistance under time-sensitive and cognitively demanding situations.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06396v1",
        "authors": [
          "Yi Wen",
          "Yu Zhang",
          "Sriram Suresh"
        ],
        "arxiv_categories": [
          "cs.HC"
        ]
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-09T15:11:31.929874",
      "entities": [
        "Structured Interviews Semi",
        "Designing Unobtrusive",
        "Empower Interviewers",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06631v1",
      "title": "Estimating Exam Item Difficulty with LLMs: A Benchmark on Brazil's ENEM Corpus",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06631v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "As Large Language Models (LLMs) are increasingly deployed to generate educational content, a critical safety question arises: can these models reliably estimate the difficulty of the questions they produce? Using Brazil's high-stakes ENEM exam as a testbed, we benchmark ten proprietary and open-weight LLMs against official Item Response Theory (IRT) parameters for 1,031 questions. We evaluate performance along three axes: absolute calibration, rank fidelity, and context sensitivity across learner backgrounds. Our results reveal a significant trade-off: while the best models achieve moderate rank correlation, they systematically underestimate difficulty and degrade significantly on multimodal items. Crucially, we find that models exhibit limited and inconsistent plasticity when prompted with student demographic cues, suggesting they are not yet ready for context-adaptive personalization. We conclude that LLMs function best as calibrated screeners rather than authoritative oracles, supporting an \"evaluation-before-generation\" pipeline for responsible assessment design.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06631v1",
        "authors": [
          "Thiago Brant",
          "Julien Kühn",
          "Jun Pang"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.455680",
      "entities": [
        "Estimating Exam Item Difficulty",
        "Corpus As Large Language",
        "Using Brazil",
        "Oracle",
        "ENEM",
        "MIT",
        "IRT",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06607v1",
      "title": "Beyond Pairwise Distance: Cognitive Traversal Distance as a Holistic Measure of Scientific Novelty",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06607v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Scientific novelty is a critical construct in bibliometrics and is commonly measured by aggregating pairwise distances between the knowledge units underlying a paper. While prior work has refined how such distances are computed, less attention has been paid to how dyadic relations are aggregated to characterize novelty at the paper level. We address this limitation by introducing a network-based indicator, Cognitive Traversal Distance (CTD). Conceptualizing the historical literature as a weighted knowledge network, CTD is defined as the length of the shortest path required to connect all knowledge units associated with a paper. CTD provides a paper-level novelty measure that reflects the minimal structural distance needed to integrate multiple knowledge units, moving beyond mean- or quantile-based aggregation of pairwise distances. Using 27 million biomedical publications indexed by OpenAlex and Medical Subject Headings (MeSH) as standardized knowledge units, we evaluate CTD against expert-based novelty benchmarks from F1000Prime-recommended papers and Nobel Prize-winning publications. CTD consistently outperforms conventional aggregation-based indicators. We further show that MeSH-based CTD is less sensitive to novelty driven by the emergence of entirely new conceptual labels, clarifying its scope relative to recent text-based measures.",
        "keywords": [
          "cs.DL",
          "cs.CY",
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06607v1",
        "authors": [
          "Yi Xiang",
          "Pascal Welke",
          "Chengzhi Zhang"
        ],
        "arxiv_categories": [
          "cs.DL",
          "cs.CY",
          "econ.GN"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.456131",
      "entities": [
        "Scientific Novelty Scientific",
        "Cognitive Traversal Distance",
        "Medical Subject Headings",
        "Beyond Pairwise Distance",
        "Holistic Measure",
        "Nobel Prize",
        "Standard",
        "MIT",
        "Act",
        "CTD",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06506v1",
      "title": "Designing Computational Tools for Exploring Causal Relationships in Qualitative Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06506v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Exploring causal relationships for qualitative data analysis in HCI and social science research enables the understanding of user needs and theory building. However, current computational tools primarily characterize and categorize qualitative data; the few systems that analyze causal relationships either inadequately consider context, lack credibility, or produce overly complex outputs. We first conducted a formative study with 15 participants interested in using computational tools for exploring causal relationships in qualitative data to understand their needs and derive design guidelines. Based on these findings, we designed and implemented QualCausal, a system that extracts and illustrates causal relationships through interactive causal network construction and multi-view visualization. A feedback study (n = 15) revealed that participants valued our system for reducing the analytical burden and providing cognitive scaffolding, yet navigated how such systems fit within their established research paradigms, practices, and habits. We discuss broader implications for designing computational tools that support qualitative data analysis.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06506v1",
        "authors": [
          "Han Meng",
          "Qiuyuan Lyu",
          "Peinuan Qin"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.CL"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.456503",
      "entities": [
        "Exploring Causal Relationships",
        "Designing Computational Tools",
        "Qualitative Data Exploring",
        "Guideline",
        "HCI",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06437v1",
      "title": "An attention economy model of co-evolution between content quality and audience selectivity",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06437v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Human attention has become a scarce and strategically contested resource in digital environments. Content providers increasingly engage in excessive competition for visibility, often prioritizing attention-grabbing tactics over substantive quality. Despite extensive empirical evidence, however, there is a lack of theoretical models that explain the fundamental dynamics of the attention economy. Here, we develop a minimal mathematical framework to explain how content quality and audience attention coevolve under limited attention capacity. Using an evolutionary game approach, we model strategic feedback between providers, who decide how much effort to invest in production, and consumers, who choose whether to search selectively for high-quality content or to engage passively. Analytical and numerical results reveal three characteristic regimes of content dynamics: collapse, boundary, and coexistence. The transitions between these regimes depend on how effectively audiences can distinguish content quality. When audience discriminability is weak, both selective attention and high-quality production vanish, leading to informational collapse. When discriminability is sufficient and incentives are well aligned, high- and low-quality content dynamically coexist through feedback between audience selectivity and providers' effort. These findings identify two key conditions for sustaining a healthy information ecosystem: adequate discriminability among audiences and sufficient incentives for high-effort creation. The model provides a theoretical foundation for understanding how institutional and platform designs can prevent the degradation of content quality in the attention economy.",
        "keywords": [
          "physics.soc-ph",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06437v1",
        "authors": [
          "Masaki Chujyo",
          "Isamu Okada",
          "Hitoshi Yamamoto"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.456996",
      "entities": [
        "Framework",
        "MIT",
        "WHO",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06371v1",
      "title": "Bilingual Bias in Large Language Models: A Taiwan Sovereignty Benchmark Study",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06371v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Large Language Models (LLMs) are increasingly deployed in multilingual contexts, yet their consistency across languages on politically sensitive topics remains understudied. This paper presents a systematic bilingual benchmark study examining how 17 LLMs respond to questions concerning the sovereignty of the Republic of China (Taiwan) when queried in Chinese versus English. We discover significant language bias -- the phenomenon where the same model produces substantively different political stances depending on the query language. Our findings reveal that 15 out of 17 tested models exhibit measurable language bias, with Chinese-origin models showing particularly severe issues including complete refusal to answer or explicit propagation of Chinese Communist Party (CCP) narratives. Notably, only GPT-4o Mini achieves a perfect 10/10 score in both languages. We propose novel metrics for quantifying language bias and consistency, including the Language Bias Score (LBS) and Quality-Adjusted Consistency (QAC). Our benchmark and evaluation framework are open-sourced to enable reproducibility and community extension.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06371v1",
        "authors": [
          "Ju-Chun Ko"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.457352",
      "entities": [
        "Taiwan Sovereignty Benchmark Study",
        "Chinese Communist Party",
        "Large Language Models",
        "Adjusted Consistency",
        "Language Bias Score",
        "Bilingual Bias",
        "Framework",
        "NIST",
        "QAC",
        "GPT",
        "LLM",
        "CCP",
        "LBS",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06302v1",
      "title": "Do LLMs Track Public Opinion? A Multi-Model Study of Favorability Predictions in the 2024 U.S. Presidential Election",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06302v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "We investigate whether Large Language Models (LLMs) can track public opinion as measured by exit polls during the 2024 U.S. presidential election cycle. Our analysis focuses on headline favorability (e.g., \"Favorable\" vs. \"Unfavorable\") of presidential candidates across multiple LLMs queried daily throughout the election season. Using the publicly available llm-election-data-2024 dataset, we evaluate predictions from nine LLM configurations against a curated set of five high-quality polls from major organizations including Reuters, CNN, Gallup, Quinnipiac, and ABC. We find systematic directional miscalibration. For Kamala Harris, all models overpredict favorability by 10-40% relative to polls. For Donald Trump, biases are smaller (5-10%) and poll-dependent, with substantially lower cross-model variation. These deviations persist under temporal smoothing and are not corrected by internet-augmented retrieval. We conclude that off-the-shelf LLMs do not reliably track polls when queried in a straightforward manner and discuss implications for election forecasting.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06302v1",
        "authors": [
          "Riya Parikh",
          "Sarah H. Cen",
          "Chara Podimata"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.457669",
      "entities": [
        "Presidential Election We",
        "Favorability Predictions",
        "Large Language Models",
        "Track Public Opinion",
        "For Kamala Harris",
        "For Donald Trump",
        "Model Study",
        "LLM",
        "CNN",
        "ABC",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06249v1",
      "title": "Code, Capital, and Clusters: Understanding Firm Performance in the UK AI Economy",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06249v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "The UK has established a distinctive position in the global AI landscape, driven by rapid firm formation and strategic investment. However, the interplay between AI specialisation, local socioeconomic conditions, and firm performance remains underexplored. This study analyses a comprehensive dataset of UK AI entities (2000 - 2024) from Companies House, ONS, and glass.ai. We find a strong geographical concentration in London (41.3 percent of entities) and technology-centric sectors, with general financial services reporting the highest mean operating revenue (33.9 million GBP, n=33). Firm size and AI specialisation intensity are primary revenue drivers, while local factors, Level 3 qualification rates, population density, and employment levels, provide significant marginal contributions, highlighting the dependence of AI growth on regional socioeconomic ecosystems. The forecasting models project sectoral expansion to 2030, estimating 4,651 [4,323 - 4,979, 95 percent CI] total entities and a rising dissolution ratio (2.21 percent [-0.17 - 4.60]), indicating a transition toward slower sector expansion and consolidation. These results provide robust evidence for place-sensitive policy interventions: cultivating regional AI capabilities beyond London to mitigate systemic risks; distinguishing between support for scaling (addressing capital gaps) and deepening technical specialisation; and strategically shaping ecosystem consolidation. Targeted actions are essential to foster both aggregate AI growth and balanced regional development, transforming consolidation into sustained competitive advantage.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06249v1",
        "authors": [
          "Waqar Muhammad Ashraf",
          "Diane Coyle",
          "Ramit Debnath"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.458089",
      "entities": [
        "Understanding Firm Performance",
        "Companies House",
        "Policy",
        "MIT",
        "ONS",
        "GBP",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06172v1",
      "title": "Know Your Scientist: KYC as Biosecurity Infrastructure",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06172v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Biological AI tools for protein design and structure prediction are advancing rapidly, creating dual-use risks that existing safeguards cannot adequately address. Current model-level restrictions, including keyword filtering, output screening, and content-based access denials, are fundamentally ill-suited to biology, where reliable function prediction remains beyond reach and novel threats evade detection by design. We propose a three-tier Know Your Customer (KYC) framework, inspired by anti-money laundering (AML) practices in the financial sector, that shifts governance from content inspection to user verification and monitoring. Tier I leverages research institutions as trust anchors to vouch for affiliated researchers and assume responsibility for vetting. Tier II applies output screening through sequence homology searches and functional annotation. Tier III monitors behavioral patterns to detect anomalies inconsistent with declared research purposes. This layered approach preserves access for legitimate researchers while raising the cost of misuse through institutional accountability and traceability. The framework can be implemented immediately using existing institutional infrastructure, requiring no new legislation or regulatory mandates.",
        "keywords": [
          "cs.CY",
          "cs.CR",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06172v1",
        "authors": [
          "Jonathan Feldman",
          "Tal Feldman",
          "Annie I Anton"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.CR",
          "cs.LG"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.458409",
      "entities": [
        "Biosecurity Infrastructure Biological",
        "Know Your Scientist",
        "Know Your Customer",
        "Framework",
        "AML",
        "III",
        "Act",
        "KYC",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05710v1",
      "title": "Ethology of Latent Spaces",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05710v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices. Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points. We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault's notion of the archive, Jameson's ideologeme, and Simondon's theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.",
        "keywords": [
          "cs.CL",
          "cs.CV",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05710v1",
        "authors": [
          "Philippe Boisnard"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.CV",
          "cs.CY",
          "cs.LG"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.458858",
      "entities": [
        "OpenAI",
        "LAION",
        "CLIP",
        "WHO",
        "IoT",
        "EPA",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05519v1",
      "title": "Wikipedia and Grokipedia: A Comparison of Human and Generative Encyclopedias",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05519v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "We present a comparative analysis of Wikipedia and Grokipedia to examine how generative mediation alters content selection, textual rewriting, narrative structure, and evaluative framing in encyclopedic content. We model page inclusion in Grokipedia as a function of Wikipedia page popularity, density of reference, and recent editorial activity. Inclusion is non-uniform: pages with higher visibility and greater editorial conflict in Wikipedia are more likely to appear in Grokipedia. For included pages, we distinguish between verbatim reproduction and generative rewriting. Rewriting is more frequent for pages with higher reference density and recent controversy, while highly popular pages are more often reproduced without modification. We compare editing activity across the two platforms and estimate page complexity using a fitness-complexity framework to assess whether generative mediation alters patterns of editorial participation. To assess narrative organization, we construct actor-relation networks from article texts using abstract meaning representation. Across multiple topical domains, including U.S. politics, geopolitics, and conspiracy-related narratives, narrative structure remains largely consistent between the two sources. Analysis of lead sections shows broadly correlated framing, with localized shifts in laudatory and conflict-oriented language for some topics in Grokipedia. Overall, generative systems preserve the main structural organization of encyclopedic content, while affecting how content is selected, rewritten, and framed.",
        "keywords": [
          "cs.SI",
          "physics.soc-ph",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05519v1",
        "authors": [
          "Ortal Hadad",
          "Edoardo Loru",
          "Jacopo Nudo"
        ],
        "arxiv_categories": [
          "cs.SI",
          "physics.soc-ph",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.459249",
      "entities": [
        "Generative Encyclopedias We",
        "Framework",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05485v1",
      "title": "Fine-Tuning Large Language Models for Automatic Detection of Sexually Explicit Content in Spanish-Language Song Lyrics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05485v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "The proliferation of sexually explicit content in popular music genres such as reggaeton and trap, consumed predominantly by young audiences, has raised significant societal concern regarding the exposure of minors to potentially harmful lyrical material. This paper presents an approach to the automatic detection of sexually explicit content in Spanish-language song lyrics by fine-tuning a Generative Pre-trained Transformer (GPT) model on a curated corpus of 100 songs, evenly divided between expert-labeled explicit and non-explicit categories. The proposed methodology leverages transfer learning to adapt the pre-trained model to the idiosyncratic linguistic features of urban Latin music, including slang, metaphors, and culturally specific double entendres that evade conventional dictionary-based filtering systems. Experimental evaluation on held-out test sets demonstrates that the fine-tuned model achieves 87% accuracy, 100% precision, and 100% specificity after a feedback-driven refinement loop, outperforming both its pre-feedback configuration and a non-customized baseline ChatGPT model. A comparative analysis reveals that the fine-tuned model agrees with expert human classification in 59.2% of cases versus 55.1% for the standard model, confirming that domain-specific adaptation enhances sensitivity to implicit and culturally embedded sexual references. These findings support the viability of deploying fine-tuned large language models as automated content moderation tools on music streaming platforms. Building on these technical results, the paper develops a public policy proposal for a multi-tier age-based content rating system for music analogous to the PEGI system for video games analyzed through the PESTEL framework and Kingdon's Multiple Streams Framework, establishing both the technological feasibility and the policy pathway for systematic music content regulation.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05485v1",
        "authors": [
          "Dolores Zamacola Sánchez de Lamadrid",
          "Eduardo C. Garrido-Merchán"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.459724",
      "entities": [
        "Tuning Large Language Models",
        "Multiple Streams Framework",
        "Sexually Explicit Content",
        "Automatic Detection",
        "Generative Pre",
        "Transformer",
        "Regulation",
        "Framework",
        "Standard",
        "ChatGPT",
        "PESTEL",
        "Policy",
        "Meta",
        "PEGI",
        "GPT"
      ]
    },
    {
      "id": "arxiv-2602.05483v1",
      "title": "Toward Operationalizing Rasmussen: Drift Observability on the Simplex for Evolving Systems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05483v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Monitoring drift into failure is hindered by Euclidean anomaly detection that can conflate safe operational trade-offs with risk accumulation in signals expressed as shares, and by architectural churn that makes fixed schemas (and learned models) stale before rare boundary events occur. Rasmussen's dynamic safety model motivates drift under competing pressures, but operationalizing it for software is difficult because many high-value operational signals (effort, remaining margin, incident impact) are compositional and their parts evolve. We propose a vision for drift observability on the simplex: model drift and boundary proximity in Aitchison geometry to obtain coordinate-invariant direction and distance-to-safety in interpretable balance coordinates. To remain comparable under churn, a monitor would continuously refresh its part inventory and policy-defined boundaries from engineering artifacts and apply lineage-aware aggregation. We outline early-warning diagnostics and falsifiable hypotheses for future evaluation.",
        "keywords": [
          "cs.CY",
          "eess.SY",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05483v1",
        "authors": [
          "Anatoly A. Krasnovsky"
        ],
        "arxiv_categories": [
          "cs.CY",
          "eess.SY",
          "stat.AP"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.459992",
      "entities": [
        "Toward Operationalizing Rasmussen",
        "Evolving Systems Monitoring",
        "Drift Observability",
        "Policy",
        "MIT",
        "Act",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05403v1",
      "title": "Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05403v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogenous levels. Moreover, penalty-based constraints adopted in existing methods struggle to deeply encode physical priors, leading to optimization pathologies and discrepancy between latent representations and physical transparency. To this end, we offer a physical view to interpret opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Building upon the Neural ODEs, we define the neural opinion dynamics to coordinate neural networks with physical priors, and further present the OPINN, a physics-informed neural framework for opinion dynamics modeling. Evaluated on real-world and synthetic datasets, OPINN achieves state-of-the-art performance in opinion evolution forecasting, offering a promising paradigm for the nexus of cyber, physical, and social systems.",
        "keywords": [
          "cs.SI",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05403v1",
        "authors": [
          "Chenghua Gong",
          "Yihang Jiang",
          "Hao Li"
        ],
        "arxiv_categories": [
          "cs.SI",
          "cs.AI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.460321",
      "entities": [
        "Advancing Opinion Dynamics Modeling",
        "Reaction Equation Advanced",
        "Informed Neural Networks",
        "Neural Diffusion",
        "Neural Network",
        "Framework",
        "Fusion",
        "OPINN",
        "NIST",
        "MIT",
        "DCR",
        "EPA",
        "Act",
        "EU",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.05200v1",
      "title": "FATe of Bots: Ethical Considerations of Social Bot Detection",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05200v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "A growing suite of research illustrates the negative impact of social media bots in amplifying harmful information with widespread social implications. Social bot detection algorithms have been developed to help identify these bot agents efficiently. While such algorithms can help mitigate the harmful effects of social media bots, they operate within complex socio-technical systems that include users and organizations. As such, ethical considerations are key while developing and deploying these bot detection algorithms, especially at scales as massive as social media ecosystems. In this article, we examine the ethical implications for social bot detection systems through three pillars: training datasets, algorithm development, and the use of bot agents. We do so by surveying the training datasets of existing bot detection algorithms, evaluating existing bot detection datasets, and drawing on discussions of user experiences of people being detected as bots. This examination is grounded in the FATe framework, which examines Fairness, Accountability, and Transparency in consideration of tech ethics. We then elaborate on the challenges that researchers face in addressing ethical issues with bot detection and provide recommendations for research directions. We aim for this preliminary discussion to inspire more responsible and equitable approaches towards improving the social media bot detection landscape.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05200v1",
        "authors": [
          "Lynnette Hui Xian Ng",
          "Ethan Pan",
          "Michael Miller Yoder"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.460677",
      "entities": [
        "Ethical Considerations",
        "Social Bot Detection",
        "Framework",
        "MIT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05181v1",
      "title": "Prediction Laundering: The Illusion of Neutrality, Transparency, and Governance in Polymarket",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05181v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "The growing reliance on prediction markets as epistemic infrastructures has positioned platforms like Polymarket as providers of objective, real-time probabilistic truth, yet the signals they produce often obscure uncertainty, strategic manipulation, and capital asymmetries, encouraging misplaced epistemic trust. This paper presents a qualitative sociotechnical audit of Polymarket (N = 27), combining digital ethnography, interpretive walkthroughs, and semi-structured interviews to examine how probabilistic authority is produced and contested. We introduce the concept of Prediction Laundering, drawing on MacFarlanes framework of knowledge transmission, to describe how subjective, high-uncertainty bets, strategic hedges, and capital-heavy whale activity are stripped of their original noise through algorithmic aggregation. We trace a four-stage laundering lifecycle: Structural Sanitization, where a centralized ontology scripts the bet-able future; Probabilistic Flattening, which collapses heterogeneous motives into a single signal; Architectural Masking, which conceals capital-driven influence behind apparent consensus; and Epistemic Hardening, which erases governance disputes to produce an objective historical fact. We show that this process induces epistemic vertigo and accountability gaps by offloading truth-resolution to off-platform communities such as Discord. Challenging narratives of frictionless collective intelligence, we demonstrate Epistemic Stratification, in which technical elites audit underlying mechanisms while the broader public consumes a sanitized, capital-weighted signal, and we conclude by advocating Friction-Positive Design that surfaces the social and financial frictions inherent in synthetic truth production.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05181v1",
        "authors": [
          "Yasaman Rohanifar",
          "Syed Ishtiaque Ahmed",
          "Sharifa Sultana"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.461129",
      "entities": [
        "Probabilistic Flattening",
        "Epistemic Stratification",
        "Structural Sanitization",
        "Architectural Masking",
        "Prediction Laundering",
        "Epistemic Hardening",
        "Positive Design",
        "Framework",
        "Intel",
        "IoT",
        "Act",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05114v1",
      "title": "Scalable Generation and Validation of Isomorphic Physics Problems with GenAI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05114v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "Traditional synchronous STEM assessments face growing challenges including accessibility barriers, security concerns from resource-sharing platforms, and limited comparability across institutions. We present a framework for generating and evaluating large-scale isomorphic physics problem banks using Generative AI to enable asynchronous, multi-attempt assessments. Isomorphic problems test identical concepts through varied surface features and contexts, providing richer variation than conventional parameterized questions while maintaining consistent difficulty. Our generation framework employs prompt chaining and tool use to achieve precise control over structural variations (numeric values, spatial relations) alongside diverse contextual variations. For pre-deployment validation, we evaluate generated items using 17 open-source language models (LMs) (0.6B-32B) and compare against actual student performance (N>200) across three midterm exams. Results show that 73% of deployed banks achieve statistically homogeneous difficulty, and LMs pattern correlate strongly with student performance (Pearson's $ρ$ up to 0.594). Additionally, LMs successfully identify problematic variants, such as ambiguous problem texts. Model scale also proves critical for effective validation, where extremely small (<4B) and large (>14B) models exhibit floor and ceiling effects respectively, making mid-sized models optimal for detecting difficulty outliers.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05114v1",
        "authors": [
          "Naiming Liu",
          "Leo Murch",
          "Spencer Moore"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.462020",
      "entities": [
        "Isomorphic Physics Problems",
        "Scalable Generation",
        "Framework",
        "STEM",
        "MIT",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.05109v1",
      "title": "Blockchain Technology for Public Services: A Polycentric Governance Synthesis",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05109v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "National governments are increasingly adopting blockchain to enhance transparency, trust, and efficiency in public service delivery. However, evidence on how these technologies are governed across national contexts remains fragmented and overly focused on technical features. Using Polycentric Governance Theory, this study conducts a systematic review of peer-reviewed research published between 2021 and 2025 to examine blockchain-enabled public services and the institutional, organizational, and information-management factors shaping their adoption. Following PRISMA guidelines, we synthesize findings from major digital government and information systems databases to identify key application domains, including digital identity, electronic voting, procurement, and social services, and analyze the governance arrangements underpinning these initiatives. Our analysis reveals that blockchain adoption is embedded within polycentric environments characterized by distributed authority, inter-organizational coordination, and layered accountability. Rather than adopting full decentralization, governments typically utilize hybrid and permissioned designs that allow for selective decentralization alongside centralized oversight, a pattern we conceptualize as \"controlled polycentricity.\" By reframing blockchain as a governance infrastructure that encodes rules for coordination and information-sharing, this study advances digital government theory beyond simple adoption metrics. The findings offer theoretically grounded insights for researchers and practical guidance for policymakers seeking to design and scale sustainable blockchain-enabled public services.",
        "keywords": [
          "cs.SI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05109v1",
        "authors": [
          "Hozefa Lakadawala",
          "Komla Dzigbede",
          "Yu Chen"
        ],
        "arxiv_categories": [
          "cs.SI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.462433",
      "entities": [
        "Polycentric Governance Synthesis National",
        "Blockchain Technology",
        "Public Services",
        "Blockchain",
        "Guideline",
        "PRISMA",
        "Policy",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05016v1",
      "title": "From Fragmentation to Integration: Exploring the Design Space of AI Agents for Human-as-the-Unit Privacy Management",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05016v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "Managing one's digital footprint is overwhelming, as it spans multiple platforms and involves countless context-dependent decisions. Recent advances in agentic AI offer ways forward by enabling holistic, contextual privacy-enhancing solutions. Building on this potential, we adopted a ''human-as-the-unit'' perspective and investigated users' cross-context privacy challenges through 12 semi-structured interviews. Results reveal that people rely on ad hoc manual strategies while lacking comprehensive privacy controls, highlighting nine privacy-management challenges across applications, temporal contexts, and relationships. To explore solutions, we generated nine AI agent concepts and evaluated them via a speed-dating survey with 116 US participants. The three highest-ranked concepts were all post-sharing management tools with half or full agent autonomy, with users expressing greater trust in AI accuracy than in their own efforts. Our findings highlight a promising design space where users see AI agents bridging the fragments in privacy management, particularly through automated, comprehensive post-sharing remediation of users' digital footprints.",
        "keywords": [
          "cs.ET",
          "cs.HC",
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05016v1",
        "authors": [
          "Eryue Xu",
          "Tianshi Li"
        ],
        "arxiv_categories": [
          "cs.ET",
          "cs.HC",
          "cs.CY",
          "cs.AI"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.462744",
      "entities": [
        "Unit Privacy Management Managing",
        "From Fragmentation",
        "Design Space",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.04972v1",
      "title": "Learning Context Matters: Measuring and Diagnosing Personalization Gaps in LLM-Based Instructional Design",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.04972v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "The adoption of generative AI in education has accelerated dramatically in recent years, with Large Language Models (LLMs) increasingly integrated into learning environments in the hope of providing personalized support that enhances learner engagement and knowledge retention. However, truly personalized support requires access to meaningful Learning Context (LC) regarding who the learner is, what they are trying to understand, and how they are engaging with the material. In this paper, we present a framework for measuring and diagnosing how the LC influences instructional strategy selection in LLM-based tutoring systems. Using psychometrically grounded synthetic learning contexts and a pedagogically grounded decision space, we compare LLM instructional decisions in context-blind and context-aware conditions and quantify their alignment with the pedagogical judgments of subject matter experts. Our results show that, while providing the LC induces systematic, measurable changes in instructional decisions that move LLM policies closer to the subject matter expert policy, substantial misalignment remains. To diagnose this misalignment, we introduce a relevance-impact analysis that reveals which learner characteristics are attended to, ignored, or spuriously influential in LLM instructional decision-making. This analysis, conducted in collaboration with subject matter experts, demonstrates that LC materially shapes LLM instructional planning but does not reliably induce pedagogically appropriate personalization. Our results enable principled evaluation of context-aware LLM systems and provide a foundation for improving personalization through learner characteristic prioritization, pedagogical model tuning, and LC engineering.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.04972v1",
        "authors": [
          "Johaun Hatchett",
          "Debshila Basu Mallick",
          "Brittany C. Bradford"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.463182",
      "entities": [
        "Diagnosing Personalization Gaps",
        "Learning Context Matters",
        "Large Language Models",
        "Learning Context",
        "Framework",
        "Policy",
        "DOE",
        "WHO",
        "Act",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.04813v1",
      "title": "Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.04813v1",
        "published_date": "2026-02-04"
      },
      "content": {
        "abstract": "Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection & Mitigation sub-dimension under Adaptation & Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question Answering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas such as Treatment Planning & Prescription still show substantial gaps (~59% Not Implemented).",
        "keywords": [
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.04813v1",
        "authors": [
          "Shubham Vatsal",
          "Harsh Dubey",
          "Aditi Singh"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-09T15:11:37.463663",
      "entities": [
        "External Knowledge Integration",
        "Agents Large Language Model",
        "Medical Question Answering",
        "Cognitive Capabilities",
        "Partially Implemented",
        "Knowledge Management",
        "Empirical Evaluation",
        "Interaction Patterns",
        "Dimensional Taxonomy",
        "Triggered Activation",
        "Treatment Planning",
        "Framework Typology",
        "Across Core Tasks",
        "Fully Implemented",
        "Decision Support"
      ]
    },
    {
      "id": "arxiv-2602.06631v1",
      "title": "Estimating Exam Item Difficulty with LLMs: A Benchmark on Brazil's ENEM Corpus",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06631v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "As Large Language Models (LLMs) are increasingly deployed to generate educational content, a critical safety question arises: can these models reliably estimate the difficulty of the questions they produce? Using Brazil's high-stakes ENEM exam as a testbed, we benchmark ten proprietary and open-weight LLMs against official Item Response Theory (IRT) parameters for 1,031 questions. We evaluate performance along three axes: absolute calibration, rank fidelity, and context sensitivity across learner backgrounds. Our results reveal a significant trade-off: while the best models achieve moderate rank correlation, they systematically underestimate difficulty and degrade significantly on multimodal items. Crucially, we find that models exhibit limited and inconsistent plasticity when prompted with student demographic cues, suggesting they are not yet ready for context-adaptive personalization. We conclude that LLMs function best as calibrated screeners rather than authoritative oracles, supporting an \"evaluation-before-generation\" pipeline for responsible assessment design.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06631v1",
        "authors": [
          "Thiago Brant",
          "Julien Kühn",
          "Jun Pang"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.060327",
      "entities": [
        "Estimating Exam Item Difficulty",
        "Corpus As Large Language",
        "Using Brazil",
        "Oracle",
        "ENEM",
        "MIT",
        "IRT",
        "LLM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06607v1",
      "title": "Beyond Pairwise Distance: Cognitive Traversal Distance as a Holistic Measure of Scientific Novelty",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06607v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Scientific novelty is a critical construct in bibliometrics and is commonly measured by aggregating pairwise distances between the knowledge units underlying a paper. While prior work has refined how such distances are computed, less attention has been paid to how dyadic relations are aggregated to characterize novelty at the paper level. We address this limitation by introducing a network-based indicator, Cognitive Traversal Distance (CTD). Conceptualizing the historical literature as a weighted knowledge network, CTD is defined as the length of the shortest path required to connect all knowledge units associated with a paper. CTD provides a paper-level novelty measure that reflects the minimal structural distance needed to integrate multiple knowledge units, moving beyond mean- or quantile-based aggregation of pairwise distances. Using 27 million biomedical publications indexed by OpenAlex and Medical Subject Headings (MeSH) as standardized knowledge units, we evaluate CTD against expert-based novelty benchmarks from F1000Prime-recommended papers and Nobel Prize-winning publications. CTD consistently outperforms conventional aggregation-based indicators. We further show that MeSH-based CTD is less sensitive to novelty driven by the emergence of entirely new conceptual labels, clarifying its scope relative to recent text-based measures.",
        "keywords": [
          "cs.DL",
          "cs.CY",
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06607v1",
        "authors": [
          "Yi Xiang",
          "Pascal Welke",
          "Chengzhi Zhang"
        ],
        "arxiv_categories": [
          "cs.DL",
          "cs.CY",
          "econ.GN"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.060701",
      "entities": [
        "Scientific Novelty Scientific",
        "Cognitive Traversal Distance",
        "Medical Subject Headings",
        "Beyond Pairwise Distance",
        "Holistic Measure",
        "Nobel Prize",
        "Standard",
        "MIT",
        "Act",
        "CTD",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06517v1",
      "title": "Smart On-Street Parking: Survey of Actual Implementations in Cities and Insights from Practitioners",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06517v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Smart solutions for on-street parking, which collect and leverage real-time information about on-street parking space availability to guide drivers or adjust policies, have attracted considerable attention in academia and in the corporate world, but comprehensive feedback on actual implementations was still missing. Here, we survey around 25 smart parking (SP) implementations in cities across the world using online sources. To get more candid insights, we complement this objective review with case studies centred around interviews that we conducted with practitioners from ten cities across continents (San Francisco, Saint Pete Beach, Penang, Douala, Soissons, Grand Paris Seine Ouest, Montpellier, Frauenfeld, Zurich, Perth). Summing up our observations, we underline the broad diversity of SP implementations in terms of contexts and scales, from 2-to-3-year small-scale pilot studies to large deployments that take centre stage in a city's mobility policy. Technological choices also vary widely, from the ground sensors used in pioneering deployments in the early 2010s and still in use, to static cameras (which cover more spaces per device), and to mobile cameras with automatic licence-plate recognition embarked in roaming cars, a more and more popular solution for parking control. Different attitudes to the role given to smartphone applications are also noticed. But, importantly, not only means, but also goals differ: facilitating parking control and enhancing revenue, or providing data for a curb-pricing strategy, or feeding live data into navigation algorithms to reduce parking search times. Unfortunately, their level of achievement is seldom gauged with robust metrics. Hardware durability issues are mentioned as causes of premature termination, particularly for `first-generation' ground sensors, but so, too, are fluctuating political will and changing priorities. Smaller-scale, geographically isolated implementations and pilots are particularly vulnerable to these fluctuations, to discontinued funding or defaulting start-ups, and to limited public awareness.",
        "keywords": [
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06517v1",
        "authors": [
          "Enock Ndunda",
          "Alexandre Nicolas"
        ],
        "arxiv_categories": [
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.061219",
      "entities": [
        "Grand Paris Seine Ouest",
        "Actual Implementations",
        "Practitioners Smart",
        "Saint Pete Beach",
        "Street Parking",
        "San Francisco",
        "Smart On",
        "Policy",
        "MIT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06506v1",
      "title": "Designing Computational Tools for Exploring Causal Relationships in Qualitative Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06506v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Exploring causal relationships for qualitative data analysis in HCI and social science research enables the understanding of user needs and theory building. However, current computational tools primarily characterize and categorize qualitative data; the few systems that analyze causal relationships either inadequately consider context, lack credibility, or produce overly complex outputs. We first conducted a formative study with 15 participants interested in using computational tools for exploring causal relationships in qualitative data to understand their needs and derive design guidelines. Based on these findings, we designed and implemented QualCausal, a system that extracts and illustrates causal relationships through interactive causal network construction and multi-view visualization. A feedback study (n = 15) revealed that participants valued our system for reducing the analytical burden and providing cognitive scaffolding, yet navigated how such systems fit within their established research paradigms, practices, and habits. We discuss broader implications for designing computational tools that support qualitative data analysis.",
        "keywords": [
          "cs.CY",
          "cs.HC",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06506v1",
        "authors": [
          "Han Meng",
          "Qiuyuan Lyu",
          "Peinuan Qin"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC",
          "cs.CL"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.061518",
      "entities": [
        "Exploring Causal Relationships",
        "Designing Computational Tools",
        "Qualitative Data Exploring",
        "Guideline",
        "HCI",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06497v1",
      "title": "Tensor network dynamical message passing for epidemic models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06497v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "While epidemiological modeling is pivotal for informing public health strategies, a fundamental trade-off limits its predictive fidelity: exact stochastic simulations are often computationally intractable for large-scale systems, whereas efficient analytical approximations typically fail to account for essential short-range correlations and network loops. Here, we resolve this trade-off by introducing Tensor Network Dynamical Message Passing (TNDMP), a framework grounded in a rigorous property we term \\textit{Susceptible-Induced Factorization}. This theoretical insight reveals that a susceptible node acts as a dynamical decoupler, factorizing the global evolution operator into localized components. Leveraging this, TNDMP provides a dual-mode algorithmic suite: an exact algorithm that computes local observables with minimal redundancy on tractable topologies and a scalable and tunable approximation for complex real-world networks. We demonstrate that widely adopted heuristics, such as Dynamical Message Passing (DMP) and Pair Approximation (PA), are mathematically recoverable as low-order limits of our framework. Numerical validation in synthetic and real-world networks confirms that TNDMP significantly outperforms existing methods to predict epidemic thresholds and steady states, offering a rigorous bridge between the efficiency of message passing and the accuracy of tensor network formalisms.",
        "keywords": [
          "physics.soc-ph",
          "cond-mat.stat-mech"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06497v1",
        "authors": [
          "Cheng Ye",
          "Zi-Song Shen",
          "Pan Zhang"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "cond-mat.stat-mech"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.061864",
      "entities": [
        "Tensor Network Dynamical Message",
        "Dynamical Message Passing",
        "Induced Factorization",
        "Pair Approximation",
        "Framework",
        "TNDMP",
        "MIT",
        "DMP",
        "Act",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06437v1",
      "title": "An attention economy model of co-evolution between content quality and audience selectivity",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06437v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Human attention has become a scarce and strategically contested resource in digital environments. Content providers increasingly engage in excessive competition for visibility, often prioritizing attention-grabbing tactics over substantive quality. Despite extensive empirical evidence, however, there is a lack of theoretical models that explain the fundamental dynamics of the attention economy. Here, we develop a minimal mathematical framework to explain how content quality and audience attention coevolve under limited attention capacity. Using an evolutionary game approach, we model strategic feedback between providers, who decide how much effort to invest in production, and consumers, who choose whether to search selectively for high-quality content or to engage passively. Analytical and numerical results reveal three characteristic regimes of content dynamics: collapse, boundary, and coexistence. The transitions between these regimes depend on how effectively audiences can distinguish content quality. When audience discriminability is weak, both selective attention and high-quality production vanish, leading to informational collapse. When discriminability is sufficient and incentives are well aligned, high- and low-quality content dynamically coexist through feedback between audience selectivity and providers' effort. These findings identify two key conditions for sustaining a healthy information ecosystem: adequate discriminability among audiences and sufficient incentives for high-effort creation. The model provides a theoretical foundation for understanding how institutional and platform designs can prevent the degradation of content quality in the attention economy.",
        "keywords": [
          "physics.soc-ph",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06437v1",
        "authors": [
          "Masaki Chujyo",
          "Isamu Okada",
          "Hitoshi Yamamoto"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.062276",
      "entities": [
        "Framework",
        "MIT",
        "WHO",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06436v1",
      "title": "Impact of seed node position on network robustness under localized attacks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06436v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Localized attacks (LAs), where damage propagates from a single seed node to its neighbors, pose significant threats to the robustness of complex networks. Although previous studies have extensively analyzed network vulnerability under such attacks, they typically assume random seed node placement and evaluate average robustness. However, the structural position of the seed node can significantly impact the extent of damage. This study proposes the Localized Attack Vulnerability Index (LAVI), a node-level metric that quantifies the potential impact of a LA initiated at a specific node. LAVI quantifies the cumulative number of severed links during attack progression, capturing how local connectivity and topological position amplify the resulting damage. Numerical experiments on synthetic and real-world networks demonstrate that LAVI correlates more strongly with network robustness degradation than standard centrality measures, such as degree, closeness, and betweenness. Our findings highlight that classical centrality metrics fail to capture key dynamics of spatially localized failures, while LAVI provides an accurate and generalizable indicator of node vulnerability under such disruptions.",
        "keywords": [
          "cs.SI",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06436v1",
        "authors": [
          "Masaki Chujyo",
          "Shu Liu",
          "Fujio Toriumi"
        ],
        "arxiv_categories": [
          "cs.SI",
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.062583",
      "entities": [
        "Localized Attack Vulnerability Index",
        "Standard",
        "LAVI",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06371v1",
      "title": "Bilingual Bias in Large Language Models: A Taiwan Sovereignty Benchmark Study",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06371v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Large Language Models (LLMs) are increasingly deployed in multilingual contexts, yet their consistency across languages on politically sensitive topics remains understudied. This paper presents a systematic bilingual benchmark study examining how 17 LLMs respond to questions concerning the sovereignty of the Republic of China (Taiwan) when queried in Chinese versus English. We discover significant language bias -- the phenomenon where the same model produces substantively different political stances depending on the query language. Our findings reveal that 15 out of 17 tested models exhibit measurable language bias, with Chinese-origin models showing particularly severe issues including complete refusal to answer or explicit propagation of Chinese Communist Party (CCP) narratives. Notably, only GPT-4o Mini achieves a perfect 10/10 score in both languages. We propose novel metrics for quantifying language bias and consistency, including the Language Bias Score (LBS) and Quality-Adjusted Consistency (QAC). Our benchmark and evaluation framework are open-sourced to enable reproducibility and community extension.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06371v1",
        "authors": [
          "Ju-Chun Ko"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.062877",
      "entities": [
        "Taiwan Sovereignty Benchmark Study",
        "Chinese Communist Party",
        "Large Language Models",
        "Adjusted Consistency",
        "Language Bias Score",
        "Bilingual Bias",
        "Framework",
        "NIST",
        "QAC",
        "GPT",
        "LLM",
        "CCP",
        "LBS",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06342v1",
      "title": "Mean-Field Theory for Heider Balance under Heterogeneous Social Temperatures",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06342v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "Heider balance theory provides a fundamental framework for understanding the formation of friendly and hostile relations in social networks. Existing stochastic formulations typically assume a uniform social temperature, implying that all interpersonal relations fluctuate with the same intensity. However, studies show that social interactions are highly heterogeneous, with broad variability in stability, volatility, and susceptibility to change. In this work, we introduce a generalized Heider balance model on a complete graph in which each link is assigned its own social temperature. Within a mean-field formulation, we derive a distribution-dependent self-consistency condition for the collective opinion state and identify the criteria governing the transition between polarized and non-polarized configurations. This framework reveals how the entire distribution of interaction heterogeneity shapes the macroscopic behavior of the system. We show that the functional form of the inverse-temperature distribution, in particular whether it is light-tailed or heavy-tailed, leads to qualitatively distinct phase diagrams. We also establish universal bounds for the critical transition, where the homogeneous-temperature limit provides a universal lower bound for the critical mean of an inverse-temperature distribution governing the transition. Numerical simulations confirm the theoretical predictions and highlight the nontrivial effects introduced by heterogeneity. Our results provide a unified route to understanding structural balance in realistic social systems and lay the groundwork for extensions incorporating fluctuations beyond mean field, external fields, and network topologies beyond the complete graph.",
        "keywords": [
          "physics.soc-ph",
          "cond-mat.stat-mech"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06342v1",
        "authors": [
          "Zhen Li",
          "Yuki Izumida"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "cond-mat.stat-mech"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.063296",
      "entities": [
        "Heterogeneous Social Temperatures Heider",
        "Heider Balance",
        "Framework",
        "MIT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06302v1",
      "title": "Do LLMs Track Public Opinion? A Multi-Model Study of Favorability Predictions in the 2024 U.S. Presidential Election",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06302v1",
        "published_date": "2026-02-06"
      },
      "content": {
        "abstract": "We investigate whether Large Language Models (LLMs) can track public opinion as measured by exit polls during the 2024 U.S. presidential election cycle. Our analysis focuses on headline favorability (e.g., \"Favorable\" vs. \"Unfavorable\") of presidential candidates across multiple LLMs queried daily throughout the election season. Using the publicly available llm-election-data-2024 dataset, we evaluate predictions from nine LLM configurations against a curated set of five high-quality polls from major organizations including Reuters, CNN, Gallup, Quinnipiac, and ABC. We find systematic directional miscalibration. For Kamala Harris, all models overpredict favorability by 10-40% relative to polls. For Donald Trump, biases are smaller (5-10%) and poll-dependent, with substantially lower cross-model variation. These deviations persist under temporal smoothing and are not corrected by internet-augmented retrieval. We conclude that off-the-shelf LLMs do not reliably track polls when queried in a straightforward manner and discuss implications for election forecasting.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06302v1",
        "authors": [
          "Riya Parikh",
          "Sarah H. Cen",
          "Chara Podimata"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.063590",
      "entities": [
        "Presidential Election We",
        "Favorability Predictions",
        "Large Language Models",
        "Track Public Opinion",
        "For Kamala Harris",
        "For Donald Trump",
        "Model Study",
        "LLM",
        "CNN",
        "ABC",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06249v1",
      "title": "Code, Capital, and Clusters: Understanding Firm Performance in the UK AI Economy",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06249v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "The UK has established a distinctive position in the global AI landscape, driven by rapid firm formation and strategic investment. However, the interplay between AI specialisation, local socioeconomic conditions, and firm performance remains underexplored. This study analyses a comprehensive dataset of UK AI entities (2000 - 2024) from Companies House, ONS, and glass.ai. We find a strong geographical concentration in London (41.3 percent of entities) and technology-centric sectors, with general financial services reporting the highest mean operating revenue (33.9 million GBP, n=33). Firm size and AI specialisation intensity are primary revenue drivers, while local factors, Level 3 qualification rates, population density, and employment levels, provide significant marginal contributions, highlighting the dependence of AI growth on regional socioeconomic ecosystems. The forecasting models project sectoral expansion to 2030, estimating 4,651 [4,323 - 4,979, 95 percent CI] total entities and a rising dissolution ratio (2.21 percent [-0.17 - 4.60]), indicating a transition toward slower sector expansion and consolidation. These results provide robust evidence for place-sensitive policy interventions: cultivating regional AI capabilities beyond London to mitigate systemic risks; distinguishing between support for scaling (addressing capital gaps) and deepening technical specialisation; and strategically shaping ecosystem consolidation. Targeted actions are essential to foster both aggregate AI growth and balanced regional development, transforming consolidation into sustained competitive advantage.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06249v1",
        "authors": [
          "Waqar Muhammad Ashraf",
          "Diane Coyle",
          "Ramit Debnath"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.063991",
      "entities": [
        "Understanding Firm Performance",
        "Companies House",
        "Policy",
        "MIT",
        "ONS",
        "GBP",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06200v1",
      "title": "Threshold Resource Redistribution in Spatially-Structured Kinship Networks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06200v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "We present a model for a threshold-based resource redistribution process in a spatially-explicit population, characterizing the relation between kinship network structure, local interactions and persistence. We find that population survival becomes possible for lower resource densities, but leads to increased network heterogeneity and locally centralized clusters. We interpret this in relation to a feedback between the kinship network structure and reproduction ability. Agents receive stochastic resources and solicit additional resources from connected individuals when below a minimum, with each agent contributing a fraction of their excess based on relatedness. We first analyze a fully-connected population with uniform redistribution fraction and discuss mean field expectations as well as finite size corrections. We extend this model to a hub-and-spoke network, exploring the impact of network asymmetry or centrality on resource distribution. We then develop a spatially-limited population model with diffusion, local pairing, reproduction and mortality. Redistribution is introduced as a function of relatedness (generational distance through most-recent common ancestor) and distance. Redistribution-dependent populations exhibit a higher level of relational closeness with increased clustering for agents of highest node strength. These results highlight the interaction of resource density, cooperation and kinship in a spatially-limited regime.",
        "keywords": [
          "nlin.AO",
          "physics.soc-ph",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06200v1",
        "authors": [
          "Alina Kochocki"
        ],
        "arxiv_categories": [
          "nlin.AO",
          "physics.soc-ph",
          "q-bio.PE"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.064356",
      "entities": [
        "Threshold Resource Redistribution",
        "Structured Kinship Networks We",
        "Fusion",
        "MIT",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06172v1",
      "title": "Know Your Scientist: KYC as Biosecurity Infrastructure",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06172v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Biological AI tools for protein design and structure prediction are advancing rapidly, creating dual-use risks that existing safeguards cannot adequately address. Current model-level restrictions, including keyword filtering, output screening, and content-based access denials, are fundamentally ill-suited to biology, where reliable function prediction remains beyond reach and novel threats evade detection by design. We propose a three-tier Know Your Customer (KYC) framework, inspired by anti-money laundering (AML) practices in the financial sector, that shifts governance from content inspection to user verification and monitoring. Tier I leverages research institutions as trust anchors to vouch for affiliated researchers and assume responsibility for vetting. Tier II applies output screening through sequence homology searches and functional annotation. Tier III monitors behavioral patterns to detect anomalies inconsistent with declared research purposes. This layered approach preserves access for legitimate researchers while raising the cost of misuse through institutional accountability and traceability. The framework can be implemented immediately using existing institutional infrastructure, requiring no new legislation or regulatory mandates.",
        "keywords": [
          "cs.CY",
          "cs.CR",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06172v1",
        "authors": [
          "Jonathan Feldman",
          "Tal Feldman",
          "Annie I Anton"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.CR",
          "cs.LG"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.064671",
      "entities": [
        "Biosecurity Infrastructure Biological",
        "Know Your Scientist",
        "Know Your Customer",
        "Framework",
        "AML",
        "III",
        "Act",
        "KYC",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.06106v1",
      "title": "To clean or not to clean: The free-rider problem in sequentially shared resources",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.06106v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Shared resources enhance productivity yet at the same time provide channels for biological and digital contamination, turning physical or digital hygiene into a cooperation dilemma prone to free-riding. Here we introduce a game of sequential sharing of common resources, an empirically parameterized evolutionary model of population dynamics in sequential-use settings such as gyms and shared workspaces. The success of the strategies implemented in the model, such as cleaning equipment before or after use, are based on the trade-offs between cleaning costs, contamination risk, and social incentives to mitigate disease transmission. We find that cooperative hygiene can be achieved by lowering the effective costs of cleaning, strengthening pro-social incentives, and monitoring population-level noncompliance. Remarkably, stability of fully altruistic populations is primarily affected by the cleaning costs. In contrast, increasing effective infection costs, for example through punishment, appears less important in this case. The model's evolutionary dynamics exhibit multi-stability, hysteresis, and abrupt shifts in strategy composition, broadly consistent with empirical observations from shared-use facilities. Our framework offers testable predictions and is amenable to quantitative calibration with behavioral and environmental data. Our predictions can be used to inform the design of cost-effective public health and digital security policies.",
        "keywords": [
          "physics.soc-ph",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.06106v1",
        "authors": [
          "Alexander Feigel",
          "Alexandre V. Morozov"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "q-bio.PE"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.065034",
      "entities": [
        "Framework",
        "MIT",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05915v1",
      "title": "Higher-order adaptive behaviors outperform pairwise strategies in mitigating contagion dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05915v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "When exposed to a contagion phenomenon, individuals may respond to the perceived risk of infection by adopting behavioral changes, aiming to reduce their exposure or their risk of infecting others. The social cost of such adaptive behaviors and their impact on the contagion dynamics have been investigated in pairwise networks, with binary interactions driving both contagion and risk perception. However, contagion and adaptive mechanisms can also be driven by group (higher-order) interactions. Here, we consider several adaptive behaviors triggered by awareness of risk perceived through higher-order and pairwise interactions, and we compare their impact on pairwise and higher-order contagion processes. By numerical simulations and a mean-field analytic approach, we show that adaptive behaviors driven by higher-order information are more effective in limiting the spread of a contagion, than similar mechanisms based on pairwise information. Meanwhile, they also entail a lower social cost, measured as the reduction of the intensity of interactions in the population. Indeed, adaptive mechanisms based on higher-order information lead to a heterogeneous risk perception within the population, producing a higher alert on nodes with large hyperdegree (i.e., participating in many groups), on their neighborhoods, and on large groups. This in turn prevents the spreading process to exploit the properties of these nodes and groups, which tend to drive and sustain the dynamics in the absence of adaptive behaviors.",
        "keywords": [
          "nlin.AO",
          "cs.SI",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05915v1",
        "authors": [
          "Marco Mancastroppa",
          "Márton Karsai",
          "Alain Barrat"
        ],
        "arxiv_categories": [
          "nlin.AO",
          "cs.SI",
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.065410",
      "entities": [
        "MIT",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.05836v1",
      "title": "An FWCI decomposition of Science Foundation Ireland funding",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05836v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "In response to the 2008 global financial crisis, Science Foundation Ireland (SFI), now Research Ireland, pivoted to research with potential socioeconomic impact. Given that the latter can encompass higher technology readiness levels, which typically correlates with lower academic impact, it is interesting to understand how academic impact holds up in SFI funded research. Here we decompose SFI \\textit{Investigator Awards} - arguably the most academic funding call - into $3,243$ constituent publications and field weighted citation impact (FWCI) values searchable in the SCOPUS database. Given that citation counts are skewed, we highlight the limitation of FWCI as a paper metric, which naively restricts one to comparisons of average FWCI ($\\overline{\\mathrm{FWCI}}$) in large samples. Neglecting publications with $\\textrm{FWCI} < 0.1$ ($8.8\\%$), SFI funded publications are well approximated by a lognormal distribution with $μ= -0.0761^{+0.017}_{-0.0039}$ and $ σ= 0.933^{+0.011}_{-0.012}$ at $95 \\%$ confidence level. This equates to an $\\overline{\\mathrm{FWCI}} = 1.433^{+0.029}_{-0.015}$ well above $\\overline{\\mathrm{FWCI}}=1$ internationally. Broken down by award, we correct $\\overline{\\mathrm{FWCI}}$ for small samples using simulations and find $\\sim 67\\%$ exceed \\textit{median} international academic interest, thus exhibiting a positive correlation between the potential for socioeconomic impact and academic interest.",
        "keywords": [
          "cs.DL",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05836v1",
        "authors": [
          "Eoin Ó Colgáin"
        ],
        "arxiv_categories": [
          "cs.DL",
          "physics.soc-ph"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.066282",
      "entities": [
        "Science Foundation Ireland",
        "Investigator Awards",
        "Research Ireland",
        "SCOPUS",
        "FWCI",
        "MIT",
        "Act",
        "SFI",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05710v1",
      "title": "Ethology of Latent Spaces",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05710v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices. Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points. We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault's notion of the archive, Jameson's ideologeme, and Simondon's theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.",
        "keywords": [
          "cs.CL",
          "cs.CV",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05710v1",
        "authors": [
          "Philippe Boisnard"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.CV",
          "cs.CY",
          "cs.LG"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.066736",
      "entities": [
        "OpenAI",
        "LAION",
        "CLIP",
        "WHO",
        "IoT",
        "EPA",
        "EU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05519v1",
      "title": "Wikipedia and Grokipedia: A Comparison of Human and Generative Encyclopedias",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05519v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "We present a comparative analysis of Wikipedia and Grokipedia to examine how generative mediation alters content selection, textual rewriting, narrative structure, and evaluative framing in encyclopedic content. We model page inclusion in Grokipedia as a function of Wikipedia page popularity, density of reference, and recent editorial activity. Inclusion is non-uniform: pages with higher visibility and greater editorial conflict in Wikipedia are more likely to appear in Grokipedia. For included pages, we distinguish between verbatim reproduction and generative rewriting. Rewriting is more frequent for pages with higher reference density and recent controversy, while highly popular pages are more often reproduced without modification. We compare editing activity across the two platforms and estimate page complexity using a fitness-complexity framework to assess whether generative mediation alters patterns of editorial participation. To assess narrative organization, we construct actor-relation networks from article texts using abstract meaning representation. Across multiple topical domains, including U.S. politics, geopolitics, and conspiracy-related narratives, narrative structure remains largely consistent between the two sources. Analysis of lead sections shows broadly correlated framing, with localized shifts in laudatory and conflict-oriented language for some topics in Grokipedia. Overall, generative systems preserve the main structural organization of encyclopedic content, while affecting how content is selected, rewritten, and framed.",
        "keywords": [
          "cs.SI",
          "physics.soc-ph",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05519v1",
        "authors": [
          "Ortal Hadad",
          "Edoardo Loru",
          "Jacopo Nudo"
        ],
        "arxiv_categories": [
          "cs.SI",
          "physics.soc-ph",
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.067126",
      "entities": [
        "Generative Encyclopedias We",
        "Framework",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.05485v1",
      "title": "Fine-Tuning Large Language Models for Automatic Detection of Sexually Explicit Content in Spanish-Language Song Lyrics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05485v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "The proliferation of sexually explicit content in popular music genres such as reggaeton and trap, consumed predominantly by young audiences, has raised significant societal concern regarding the exposure of minors to potentially harmful lyrical material. This paper presents an approach to the automatic detection of sexually explicit content in Spanish-language song lyrics by fine-tuning a Generative Pre-trained Transformer (GPT) model on a curated corpus of 100 songs, evenly divided between expert-labeled explicit and non-explicit categories. The proposed methodology leverages transfer learning to adapt the pre-trained model to the idiosyncratic linguistic features of urban Latin music, including slang, metaphors, and culturally specific double entendres that evade conventional dictionary-based filtering systems. Experimental evaluation on held-out test sets demonstrates that the fine-tuned model achieves 87% accuracy, 100% precision, and 100% specificity after a feedback-driven refinement loop, outperforming both its pre-feedback configuration and a non-customized baseline ChatGPT model. A comparative analysis reveals that the fine-tuned model agrees with expert human classification in 59.2% of cases versus 55.1% for the standard model, confirming that domain-specific adaptation enhances sensitivity to implicit and culturally embedded sexual references. These findings support the viability of deploying fine-tuned large language models as automated content moderation tools on music streaming platforms. Building on these technical results, the paper develops a public policy proposal for a multi-tier age-based content rating system for music analogous to the PEGI system for video games analyzed through the PESTEL framework and Kingdon's Multiple Streams Framework, establishing both the technological feasibility and the policy pathway for systematic music content regulation.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05485v1",
        "authors": [
          "Dolores Zamacola Sánchez de Lamadrid",
          "Eduardo C. Garrido-Merchán"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.067585",
      "entities": [
        "Tuning Large Language Models",
        "Multiple Streams Framework",
        "Sexually Explicit Content",
        "Automatic Detection",
        "Generative Pre",
        "Transformer",
        "Regulation",
        "Framework",
        "Standard",
        "ChatGPT",
        "PESTEL",
        "Policy",
        "Meta",
        "PEGI",
        "GPT"
      ]
    },
    {
      "id": "arxiv-2602.05483v1",
      "title": "Toward Operationalizing Rasmussen: Drift Observability on the Simplex for Evolving Systems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2602.05483v1",
        "published_date": "2026-02-05"
      },
      "content": {
        "abstract": "Monitoring drift into failure is hindered by Euclidean anomaly detection that can conflate safe operational trade-offs with risk accumulation in signals expressed as shares, and by architectural churn that makes fixed schemas (and learned models) stale before rare boundary events occur. Rasmussen's dynamic safety model motivates drift under competing pressures, but operationalizing it for software is difficult because many high-value operational signals (effort, remaining margin, incident impact) are compositional and their parts evolve. We propose a vision for drift observability on the simplex: model drift and boundary proximity in Aitchison geometry to obtain coordinate-invariant direction and distance-to-safety in interpretable balance coordinates. To remain comparable under churn, a monitor would continuously refresh its part inventory and policy-defined boundaries from engineering artifacts and apply lineage-aware aggregation. We outline early-warning diagnostics and falsifiable hypotheses for future evaluation.",
        "keywords": [
          "cs.CY",
          "eess.SY",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.05483v1",
        "authors": [
          "Anatoly A. Krasnovsky"
        ],
        "arxiv_categories": [
          "cs.CY",
          "eess.SY",
          "stat.AP"
        ]
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-09T15:11:46.067820",
      "entities": [
        "Toward Operationalizing Rasmussen",
        "Evolving Systems Monitoring",
        "Drift Observability",
        "Policy",
        "MIT",
        "Act",
        "EU",
        "AI",
        "UN"
      ]
    }
  ]
}