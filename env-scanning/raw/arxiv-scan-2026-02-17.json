{
  "agent_metadata": {
    "agent_name": "arxiv-agent",
    "model_used": "sonnet",
    "papers_collected": 120,
    "steeps_categories_scanned": 6,
    "scan_date": "2026-02-17",
    "status": "success",
    "execution_time": 15.1,
    "process_id": 51021
  },
  "items": [
    {
      "id": "arxiv-2602.15031v1",
      "title": "EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15031v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15031v1",
        "authors": [
          "Yehonathan Litman",
          "Shikun Liu",
          "Dario Seyb",
          "Nicholas Milef",
          "Yang Zhou"
        ],
        "arxiv_categories": [
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.712155",
      "entities": [
        "Time Generative Video Editing",
        "Disentangled Local",
        "Global Control",
        "Framework",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15030v1",
      "title": "Image Generation with a Sphere Encoder",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15030v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15030v1",
        "authors": [
          "Kaiyu Yue",
          "Menglin Jia",
          "Ji Hou",
          "Tom Goldstein"
        ],
        "arxiv_categories": [
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.712267",
      "entities": [
        "Sphere Encoder We",
        "Image Generation",
        "Sphere Encoder",
        "Framework",
        "Fusion",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15029v1",
      "title": "Symmetry in language statistics shapes the geometry of model representations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15029v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Although learned representations underlie neural networks' success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.",
        "keywords": [
          "cs.LG",
          "cond-mat.dis-nn",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15029v1",
        "authors": [
          "Dhruva Karkada",
          "Daniel J. Korchinski",
          "Andres Nava",
          "Matthieu Wyart",
          "Yasaman Bahri"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cond-mat.dis-nn",
          "cs.CL"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.712397",
      "entities": [
        "Neural Network",
        "Framework",
        "LLM",
        "UN",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.15028v1",
      "title": "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15028v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench",
        "keywords": [
          "cs.LG",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15028v1",
        "authors": [
          "Shangding Gu"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.AI"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.712539",
      "entities": [
        "Personalization Large",
        "Long Context",
        "Scaling Gap",
        "Transformer",
        "Less Focus",
        "LLM",
        "NSF",
        "MIT",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15022v1",
      "title": "Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15022v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \\times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.",
        "keywords": [
          "cs.LG",
          "cs.AI",
          "math.GR",
          "q-bio.BM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15022v1",
        "authors": [
          "Cai Zhou",
          "Zijie Chen",
          "Zian Li",
          "Jike Wang",
          "Kaiyi Jiang"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.AI",
          "math.GR",
          "q-bio.BM"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.712712",
      "entities": [
        "Molecular Graph Generation Many",
        "Rethinking Diffusion Models",
        "Framework",
        "Fusion",
        "DRUG",
        "GEOM",
        "NSF",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15019v1",
      "title": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15019v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface \"under-the-radar\" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations. We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.",
        "keywords": [
          "cs.AI",
          "cs.IR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15019v1",
        "authors": [
          "Alisa Vinogradova",
          "Vlad Vinogradov",
          "Luba Greenwood",
          "Ilya Yasny",
          "Dmitry Kobyzev"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.IR"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.712980",
      "entities": [
        "Perplexity Deep Research",
        "Business Development",
        "Drug Asset Scouting",
        "Evaluation Bio",
        "United States",
        "Bioptic Agent",
        "Deep Research",
        "Hunt Globally",
        "Exa Websets",
        "Claude Opus",
        "GPT-5.2",
        "OpenAI",
        "Bill",
        "LLM",
        "GPT"
      ]
    },
    {
      "id": "arxiv-2602.15018v1",
      "title": "Neurosim: A Fast Simulator for Neuromorphic Robot Perception",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15018v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .",
        "keywords": [
          "cs.RO",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15018v1",
        "authors": [
          "Richeek Das",
          "Pratik Chaudhari"
        ],
        "arxiv_categories": [
          "cs.RO",
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.713112",
      "entities": [
        "Neuromorphic Robot Perception Neurosim",
        "Machine Learning",
        "Fast Simulator",
        "Robot",
        "FPS",
        "RGB",
        "GPU",
        "UN",
        "EU",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15016v1",
      "title": "Controlled Theory of Skyrmion Chern Bands in Moiré Quantum Materials: Quantum Geometry and Collective Dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15016v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Recent experiments in moiré quantum materials exhibit quantized Hall states without an external magnetic field, motivating continuum mechanisms based on smooth moiré-periodic pseudospin textures. We present a controlled theory of skyrmion Chern bands generated by such textures. An exact local $SU(2)$ transformation reveals an emergent non-Abelian gauge field; for large branch splitting we perform an operator-level Schrieffer-Wolff expansion, yielding a single-branch Hamiltonian together with systematically dressed physical operators that define the projected interacting theory beyond strict adiabaticity. The leading dynamics is governed by a $U(1)$ Berry connection whose flux is set by the skyrmion density, while controlled non-adiabatic corrections are fixed by the texture's real-space quantum geometric tensor. In a Landau-level representation built from the averaged emergent field, moiré-periodic modulations induce Umklapp-resolved deformations of Girvin-MacDonald-Platzman kinematics and microscopic sources of excess optical quantum weight above the topological lower bound. Assuming a gapped Hall phase, we further derive a skyrmion-crystal effective field theory with a universal Berry-phase term and a noncommutative magnetophonon. Our results provide experimentally accessible signatures for twisted transition-metal dichalcogenide homobilayers and rhombohedral graphene aligned with hexagonal boron nitride.",
        "keywords": [
          "cond-mat.str-el",
          "cond-mat.mes-hall",
          "hep-th",
          "quant-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15016v1",
        "authors": [
          "Yi-Hsien Du"
        ],
        "arxiv_categories": [
          "cond-mat.str-el",
          "cond-mat.mes-hall",
          "hep-th",
          "quant-ph"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.713463",
      "entities": [
        "Collective Dynamics Recent",
        "Skyrmion Chern Bands",
        "Quantum Materials",
        "Quantum Geometry",
        "Meta",
        "NSF",
        "WHO",
        "Act",
        "UN",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.15014v1",
      "title": "Scaling Beyond Masked Diffusion Language Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15014v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates, largely driven by strong perplexity on language modeling benchmarks. In this work, we present the first scaling law study of uniform-state and interpolating discrete diffusion methods. We also show that Masked diffusion models can be made approximately 12% more FLOPs-efficient when trained with a simple cross-entropy objective. We find that perplexity is informative within a diffusion family but can be misleading across families, where models with worse likelihood scaling may be preferable due to faster and more practical sampling, as reflected by the speed-quality Pareto frontier. These results challenge the view that Masked diffusion is categorically the future of diffusion language modeling and that perplexity alone suffices for cross-algorithm comparison. Scaling all methods to 1.7B parameters, we show that uniform-state diffusion remains competitive on likelihood-based benchmarks and outperforms autoregressive and Masked diffusion models on GSM8K, despite worse validation perplexity. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/scaling-dllms",
        "keywords": [
          "cs.LG",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15014v1",
        "authors": [
          "Subham Sekhar Sahoo",
          "Jean-Marie Lemercier",
          "Zhihan Yang",
          "Justin Deschenaux",
          "Jingyu Liu"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.CL"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.713594",
      "entities": [
        "Scaling Beyond Masked Diffusion",
        "Language Models Diffusion",
        "Fusion",
        "LLM",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15013v1",
      "title": "Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15013v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates 'neutralized' text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.",
        "keywords": [
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15013v1",
        "authors": [
          "Ruoxi Liu",
          "Philipp Koehn"
        ],
        "arxiv_categories": [
          "cs.CL"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.713687",
      "entities": [
        "Large Language Models",
        "Text Style Transfer",
        "BLEU",
        "LLM",
        "NSF",
        "ICL",
        "RAG",
        "TST",
        "UN",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.15012v1",
      "title": "Cold-Start Personalization via Training-Free Priors from Structured World Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15012v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15012v1",
        "authors": [
          "Avinandan Bose",
          "Shuyue Stella Li",
          "Faeze Brahman",
          "Pang Wei Koh",
          "Simon Shaolei Du"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.LG"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.713860",
      "entities": [
        "Structured World Models Cold",
        "Preference Elicitation",
        "Start Personalization",
        "Free Priors",
        "Framework",
        "MIT",
        "DOE",
        "WHO",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15010v1",
      "title": "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15010v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.",
        "keywords": [
          "cs.RO",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15010v1",
        "authors": [
          "Max Sobol Mark",
          "Jacky Liang",
          "Maria Attarian",
          "Chuyuan Fu",
          "Debidatta Dwibedi"
        ],
        "arxiv_categories": [
          "cs.RO",
          "cs.LG"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.714011",
      "entities": [
        "Context Robot Imitation Learning",
        "Key History Frames Many",
        "Big Picture Policies",
        "Robot",
        "MIT",
        "Act",
        "BPP",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15005v1",
      "title": "Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15005v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user's underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.",
        "keywords": [
          "cs.CL",
          "cs.IR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15005v1",
        "authors": [
          "Mengdan Zhu",
          "Yufan Zhao",
          "Tao Di",
          "Yulan Yan",
          "Liang Zhao"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.IR"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.714167",
      "entities": [
        "Domain News Recommendation News",
        "Learning User Interests",
        "Framework",
        "Policy",
        "GRPO",
        "NSF",
        "Act",
        "UN",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.14999v1",
      "title": "Low Depth Unitary Coupled Cluster Algorithm for Large Chemical Systems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14999v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The unitary coupled cluster (UCC) algorithm is one of the most promising implementations of the variational quantum eigensolver for quantum computers. However, for large systems, the number of UCC factors leads to deep quantum circuits, which are prohibitive for execution on quantum hardware. To address this, circuit depth can be reduced at the cost of more measurements with a Taylor series expansion of UCC factors with small angles, while treating the large-angle factors exactly. We implement this approach to quadratic order (qUCC) for systems with strong correlations and systems where conventional methods like coupled cluster (CC) with low excitation levels fail, but UCC and qUCC perform well. We study hydrogen chains and the BeH2 molecule that allow us to change the degree of strong correlation due to geometrical distortions. We show, via a dramatic increase in number of factors able to handle exactly, a systematic convergence of these results as more exact UCC factors are included in the calculations -- the hardest to converge regime is in the crossover from weak to strong coupling. In all cases the total number of UCC factors needed to be treated exactly is much less than the total number of UCC factors available (typically about one-third to one-half of the total number of factors).",
        "keywords": [
          "quant-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14999v1",
        "authors": [
          "Jeremy Canfield",
          "Dominika Zgid",
          "J K Freericks"
        ],
        "arxiv_categories": [
          "quant-ph"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.714300",
      "entities": [
        "Low Depth Unitary Coupled",
        "Cluster Algorithm",
        "Hydrogen",
        "Act",
        "UCC",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14997v1",
      "title": "Spectral Convolution on Orbifolds for Geometric Deep Learning",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14997v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.",
        "keywords": [
          "cs.LG",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14997v1",
        "authors": [
          "Tim Mangliers",
          "Bernhard Mössner",
          "Benjamin Himpel"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.AI"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.714387",
      "entities": [
        "Geometric Deep Learning Geometric",
        "Spectral Convolution",
        "Machine Learning",
        "Neural Network",
        "Deep Learning",
        "GDL",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.14995v1",
      "title": "Instruction-Set Architecture for Programmable NV-Center Quantum Repeater Nodes",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14995v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Programmability is increasingly central in emerging quantum network software stacks, yet the node-internal controller-to-hardware interface for quantum repeater devices remains under-specified. We introduce the idea of an instruction-set architecture (ISA) for controller-driven programmability of nitrogen-vacancy (NV) center quantum repeater nodes. Each node consists of an optically interfaced electron spin acting as a data qubit and a long-lived nuclear-spin register acting as a control program. We formalize two modes of programmability: (i) deterministic register control, where the nuclear register is initialized in a basis state to select a specific operation on the data qubit; and (ii) coherent register control, where the register is prepared in superposition, enabling coherent combinations of operations beyond classical programmability. Network protocols are expressed as controller-issued instruction vectors, which we illustrate through a compact realization of the BBPSSW purification protocol. We further show that coherent register control enables interferometric diagnostics such as fidelity witnessing and calibration, providing tools unavailable in classical programmability. Finally, we discuss scalability to multi-electron and multi-nuclear spin architectures and connection to Linear combination of unitaries (LCU) and Kraus formulation.",
        "keywords": [
          "quant-ph",
          "cs.NI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14995v1",
        "authors": [
          "Vinay Kumar",
          "Claudio Cicconetti",
          "Riccardo Bassoli",
          "Marco Conti",
          "Andrea Passarella"
        ],
        "arxiv_categories": [
          "quant-ph",
          "cs.NI"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.714524",
      "entities": [
        "Center Quantum Repeater Nodes",
        "Set Architecture",
        "Protocol",
        "Nuclear",
        "BBPSSW",
        "NIST",
        "LCU",
        "ISA",
        "Act",
        "EPA",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14994v1",
      "title": "On the Semantics of Primary Cause in Hybrid Dynamic Domains",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14994v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Reasoning about actual causes of observed effects is fundamental to the study of rationality. This important problem has been studied since the time of Aristotle, with formal mathematical accounts emerging recently. We live in a world where change due to actions can be both discrete and continuous, that is, hybrid. Yet, despite extensive research on actual causation, only few recent studies looked into causation with continuous change. Building on recent progress, in this paper we propose two definitions of primary cause in a hybrid action-theoretic framework, namely the hybrid temporal situation calculus. One of these is foundational in nature while the other formalizes causation through contributions, which can then be verified from a counterfactual perspective using a modified ``but-for'' test. We prove that these two definitions are indeed equivalent. We then show that our definitions of causation have some intuitively justifiable properties.",
        "keywords": [
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14994v1",
        "authors": [
          "Shakil M. Khan",
          "Asim Mehmood",
          "Sandra Zilles"
        ],
        "arxiv_categories": [
          "cs.AI"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.714624",
      "entities": [
        "Hybrid Dynamic Domains Reasoning",
        "Primary Cause",
        "Framework",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14992v1",
      "title": "Rotational Quantum Friction via Spontaneous Decay",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14992v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "A fascinating effect belonging to the field of vacuum forces and fluctuations is that of quantum friction. It refers to the prediction of a dissipative force acting on a moving object due to the quantum vacuum field. In this work, we investigate rotational quantum friction where a diatomic polar molecule rotates around its own center of mass in free space. We quantize the rotational motion and investigate the resulting dissipation due to spontaneous decay. We find in the Markovian regime that a friction torque $\\propto Ω^3$ persists even for zero temperature, and in agreement with the classical result in the limit of large rotational quantum number $l$. Within the non-Markovian short-time regime we find a friction $\\proptoΩ$.",
        "keywords": [
          "quant-ph",
          "physics.optics"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14992v1",
        "authors": [
          "Nicolas Schüler",
          "O. J. Franca",
          "Michael Vaz",
          "Hervé Bercegol",
          "Stefan Yoshi Buhmann"
        ],
        "arxiv_categories": [
          "quant-ph",
          "physics.optics"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.714831",
      "entities": [
        "Rotational Quantum Friction",
        "Spontaneous Decay",
        "Agreement",
        "MIT",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14989v1",
      "title": "ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14989v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.",
        "keywords": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14989v1",
        "authors": [
          "Ayush Shrivastava",
          "Kirtan Gangani",
          "Laksh Jain",
          "Mayank Goel",
          "Nipun Batra"
        ],
        "arxiv_categories": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.714974",
      "entities": [
        "Structured Benchmark",
        "Language Models",
        "NSF",
        "MIT",
        "RGB",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14986v1",
      "title": "Scaling QAOA: transferring optimal adiabatic schedules from small-scale to large-scale variational circuits",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14986v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for combinatorial optimization on near-term quantum devices, yet its scalability is limited by the difficulty of optimizing \\(2p\\) variational parameters for a large number \\(p\\) of layers. Recent empirical studies indicate that optimal QAOA angles exhibit concentration and transferability across problem sizes. Leveraging this observation, we propose a schedule-learning framework that transfers spectral-gap-informed adiabatic control strategies from small-scale instances to larger systems. Our method extracts the spectral gap profile of small problems and constructs a continuous schedule governed by \\(\\partial_t s = κg^q(s)\\), where \\(g(s)\\) is the instantaneous gap and \\((κ, q)\\) are global hyperparameters. Discretizing this schedule yields closed-form expressions for all QAOA angles, reducing the classical optimization task from \\(2p\\) parameters to only \\(2\\), independent of circuit depth. This drastic parameter compression mitigates classical optimization overhead and reduces sensitivity to barren plateau phenomena. Numerical simulations on random QUBO and 3-regular MaxCut instances demonstrate that the learnt schedules transfer effectively to larger systems while achieving competitive approximation ratios. Our results suggest that gap-informed schedule transfers provide a scalable and parameter-efficient strategy for QAOA.",
        "keywords": [
          "quant-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14986v1",
        "authors": [
          "Ugo Nzongani",
          "Dylan Laplace Mermoud",
          "Arthur Braida"
        ],
        "arxiv_categories": [
          "quant-ph"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-17T12:40:38.715365",
      "entities": [
        "Framework",
        "QUBO",
        "QAOA",
        "NSF",
        "MIT",
        "Act"
      ]
    },
    {
      "id": "arxiv-2602.14774v1",
      "title": "The unintended effects of universalizing social pensions: Evidence from Mexico",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14774v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "This paper examines the effects of the 2019 universalization of Mexico's Social Pension Program (PAM), one of the country's most expansive and politically salient social programs. The reform simultaneously increased the cash transfer and extended eligibility to all individuals aged 65 and over, regardless of income or contributory pension status. Using nationally representative data from the ENIGH and a triple-differences (DDD) identification strategy, we estimate the causal effect of the universalization on poverty and labor market outcomes. Our empirical approach exploits variation across time (pre- and post-reform), age (eligible vs. ineligible), and pension scheme status (non-contributory vs. contributory), allowing us to separate the effects of expanded eligibility from those of increased benefit levels. We find strong increases in take-up rates and no significant change in overall poverty rates, suggesting that many new beneficiaries were not economically vulnerable. However, we document a surprising increase in extreme poverty, concentrated among low-income elderly who responded to the reform by exiting the labor force. This reduction in labor supply, driven by a significant drop in employment among individuals in the bottom income quartile, suggests that the pension acted as a substitute for labor income rather than a supplement. Taken together, the results highlight the trade-offs inherent in universal pension programs: while broader access reduces administrative exclusion, extending transfers to economically secure individuals may dilute redistributive impacts and generate behavioral responses that offset potential welfare gains.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14774v1",
        "authors": [
          "Oscar Galvez-Soriano",
          "Raymundo Ramirez Peralta"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.709923",
      "entities": [
        "Social Pension Program",
        "ENIGH",
        "NIST",
        "NSF",
        "PAM",
        "WHO",
        "Act",
        "EPA",
        "DDD",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14670v1",
      "title": "FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14670v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Formulaic alpha factor mining is a critical yet challenging task in quantitative investment, characterized by a vast search space and the need for domain-informed, interpretable signals. However, finding novel signals becomes increasingly difficult as the library grows due to high redundancy. We propose FactorMiner, a lightweight and flexible self-evolving agent framework designed to navigate this complex landscape through continuous knowledge accumulation. FactorMiner combines a Modular Skill Architecture that encapsulates systematic financial evaluation into executable tools with a structured Experience Memory that distills historical mining trials into actionable insights (successful patterns and failure constraints). By instantiating the Ralph Loop paradigm -- retrieve, generate, evaluate, and distill -- FactorMiner iteratively uses memory priors to guide exploration, reducing redundant search while focusing on promising directions. Experiments on multiple datasets across different assets and Markets show that FactorMiner constructs a diverse library of high-quality factors with competitive performance, while maintaining low redundancy among factors as the library scales. Overall, FactorMiner provides a practical approach to scalable discovery of interpretable formulaic alpha factors under the \"Correlation Red Sea\" constraint.",
        "keywords": [
          "q-fin.TR",
          "cs.MA"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14670v1",
        "authors": [
          "Yanlong Wang",
          "Jian Xu",
          "Hongkang Zhang",
          "Shao-Lun Huang",
          "Danny Dongning Sun"
        ],
        "arxiv_categories": [
          "q-fin.TR",
          "cs.MA"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.710160",
      "entities": [
        "Financial Alpha Discovery Formulaic",
        "Modular Skill Architecture",
        "Correlation Red Sea",
        "Experience Memory",
        "Evolving Agent",
        "Ralph Loop",
        "Framework",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14455v1",
      "title": "How Well Are State-Dependent Local Projections Capturing Nonlinearities?",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14455v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We evaluate how well state-dependent local projections recover true impulse responses in nonlinear environments. Using quadratic vector autoregressions as a laboratory, we show that linear local projections fail to capture any nonlinearities when shocks are symmetrically distributed. Popular state-dependent local projections specifications capture distinct aspects of nonlinearity: those interacting shocks with their signs capture higher-order effects, while those interacting shocks with lagged states capture state dependence. However, their gains over linear specifications are concentrated in tail shocks or tail states; and, for lag-based specifications, hinge on how well the chosen observable proxies the latent state. Our proposed specification-which augments the linear specification with a squared shock term and an interaction between the shock and lagged observables-best approximates the true responses across the entire joint distribution of shocks and states. An application to monetary policy reveals economically meaningful state dependence, whereas higher-order effects, though statistically significant, prove economically modest.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14455v1",
        "authors": [
          "Zhiheng You"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.710350",
      "entities": [
        "Dependent Local Projections Capturing",
        "How Well Are State",
        "Laboratory",
        "Policy",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14414v1",
      "title": "The Role of Measured Covariates in Assessing Sensitivity to Unmeasured Confounding",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14414v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Sensitivity analysis is widely used to assess the robustness of causal conclusions in observational studies, yet its interaction with the structure of measured covariates is often overlooked. When latent confounders cannot be directly adjusted for and are instead controlled using proxy variables, strong associations between exposure and measured proxies can amplify sensitivity to residual confounding. We formalize this phenomenon in linear regression settings by showing that a simple ratio involving the exposure model coefficient and residual exposure variance provides an observable measure of this increased sensitivity. Applying our framework to smoking and lung cancer, we document how growing socioeconomic stratification in smoking behavior over time leads to heightened sensitivity to unmeasured confounding in more recent data. These results highlight the importance of multicollinearity when interpreting sensitivity analyses based on proxy adjustment.",
        "keywords": [
          "stat.ME",
          "econ.EM",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14414v1",
        "authors": [
          "Abhinandan Dalal",
          "Iris Horng",
          "Yang Feng",
          "Dylan S. Small"
        ],
        "arxiv_categories": [
          "stat.ME",
          "econ.EM",
          "stat.AP"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.710513",
      "entities": [
        "Unmeasured Confounding Sensitivity",
        "Assessing Sensitivity",
        "Measured Covariates",
        "Framework",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14288v1",
      "title": "Dual-Channel Closed Loop Supply Chain Competition: A Stackelberg--Nash Approach",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14288v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "In many consumer electronics and appliance markets, manufacturers sell products through competing retailers while simultaneously relying on take-back programs to recover used items for remanufacturing. Designing such programs is challenging when firms compete on prices and consumers differ in their willingness to return products. Motivated by these settings, this paper develops a game theoretic framework to analyze pricing and take-back decisions in a dual-channel closed loop supply chain (CLSC) with two competing manufacturers and two competing retailers. Manufacturers act as Stackelberg leaders, simultaneously determining wholesale prices and consumer take-back bonuses, while retailers engage in Nash competition over retail prices. The model integrates three key elements: (i) segmented linear demand with cross-price effects, (ii) deterministic product returns, and (iii) an inertia responsiveness allocation mechanism governing the distribution of returned products between manufacturers. Closed form Nash equilibria are derived for the retailer subgame, along with symmetric Stackelberg equilibria for manufacturers. We derive a feasibility threshold for take-back incentives, identifying conditions under which firms optimally offer positive bonuses to consumers. The results further demonstrate that higher remanufacturing value or return rates lead the manufacturers to lower wholesale prices in order to expand sales and capture additional return volumes, while high consumer inertia weakens incentives for active collection. Numerical experiments illustrate and reinforce the analytical results, highlighting how consumer behavior, market structure and product substitutability influence prices, bonuses, and return volumes. Overall, the study provides managerial insights for designing effective take-back programs and coordinating pricing decisions in competitive circular supply chains.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14288v1",
        "authors": [
          "Gurkirat Wadhwa"
        ],
        "arxiv_categories": [
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.710805",
      "entities": [
        "Channel Closed Loop Supply",
        "Chain Competition",
        "Nash Approach In",
        "Framework",
        "CLSC",
        "NIST",
        "WHO",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.13722v1",
      "title": "The Accuracy Smoothness Dilemma in Prediction: a Novel Multivariate M-SSA Forecast Approach",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13722v1",
        "published_date": "2026-02-14"
      },
      "content": {
        "abstract": "Forecasting presents a complex estimation challenge, as it involves balancing multiple, often conflicting, priorities and objectives. Conventional forecast optimization methods typically emphasize a single metric--such as minimizing the mean squared error (MSE)--which may neglect other crucial aspects of predictive performance. To address this limitation, the recently developed Smooth Sign Accuracy (SSA) framework extends the traditional MSE approach by simultaneously accounting for sign accuracy, MSE, and the frequency of sign changes in the predictor. This addresses a fundamental trade-off--the so-called accuracy-smoothness (AS) dilemma--in prediction. We extend this approach to the multivariate M-SSA, leveraging the original criterion to incorporate cross-sectional information across multiple time series. As a result, the M-SSA criterion enables the integration of various design objectives related to AS forecasting performance, effectively generalizing conventional MSE-based metrics. To demonstrate its practical applicability and versatility, we explore the application of the M-SSA in three primary domains: forecasting, real-time signal extraction (nowcasting), and smoothing. These case studies illustrate the framework's capacity to adapt to different contexts while effectively managing inherent trade-offs in predictive modelling.",
        "keywords": [
          "econ.EM",
          "stat.ME"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13722v1",
        "authors": [
          "Marc Wildi"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.ME"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.711027",
      "entities": [
        "Forecast Approach Forecasting",
        "Smooth Sign Accuracy",
        "Novel Multivariate",
        "Framework",
        "MIT",
        "MSE",
        "Act",
        "SSA",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.13707v1",
      "title": "Buyer Commitment in Bilateral Bargaining: The Case of Online Japanese C2C Market",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13707v1",
        "published_date": "2026-02-14"
      },
      "content": {
        "abstract": "This paper studies bargaining when buyers can continue searching for alternative sellers while negotiating, which limits their commitment to complete a transaction. Using transaction level data from a Japanese online marketplace, I document frequent post-agreement nonpurchase and show that buyers who explicitly pledge immediate payment are more likely to have their offers accepted, renege less often, and complete transactions faster. I develop and estimate a dynamic bargaining model with buyer search and limited commitment. Counterfactuals that restrict search during bargaining show that increased buyer commitment can reduce total welfare. Sellers especially those with higher valuations benefit from the elimination of delays and walkaways and respond by raising list prices. This reduces buyer welfare by lowering the option value of search and increasing expected list prices. Platform revenue also declines because buyer behavior shifts away from counteroffers and negotiated prices fall.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13707v1",
        "authors": [
          "Kan Kuno"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.711193",
      "entities": [
        "Bilateral Bargaining",
        "Buyer Commitment",
        "Online Japanese",
        "Agreement",
        "MIT",
        "WHO",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.13537v1",
      "title": "Cluster-Robust Inference for Quadratic Forms",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13537v1",
        "published_date": "2026-02-14"
      },
      "content": {
        "abstract": "This paper studies inference for quadratic forms of linear regression coefficients with clustered data and many covariates. Our framework covers three important special cases: instrumental variables regression with many instruments and controls, inference on variance components, and testing multiple restrictions in a linear regression. Naïve plug-in estimators are known to be biased. We study a leave-one-cluster-out estimator that is unbiased, and provide sufficient conditions for its asymptotic normality. For inference, we establish the consistency of a leave-three-cluster-out variance estimator under primitive conditions. In addition, we develop a novel leave-two-cluster-out variance estimator that is computationally simpler and guaranteed to be conservative under weaker conditions. Our analysis allows cluster sizes to diverge with the sample size, accommodates strong within-cluster dependence, and permits the dimension of the covariates to diverge with the sample size, potentially at the same rate.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13537v1",
        "authors": [
          "Michal Kolesár",
          "Pengjin Min",
          "Wenjie Wang",
          "Yichong Zhang"
        ],
        "arxiv_categories": [
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.711559",
      "entities": [
        "Robust Inference",
        "Framework",
        "MIT",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.13499v1",
      "title": "Endogenous Epistemic Weighting under Heterogeneous Information: Beyond Majority Rule",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13499v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Collective decision-making can be viewed as the problem of aggregating multiple noisy information channels about an unknown state of the world. Classical epistemic justifications of majority rule rely on restrictive assumptions about the homogeneity and symmetry of these channels, which are often violated in realistic environments. This paper introduces the Epistemic Shared-Choice Mechanism (ESCM), a lightweight and auditable procedure that endogenously estimates issue-specific signal reliability and assigns bounded, decision-specific voting weights. Using central limit approximations, the paper provides an analytical comparison between ESCM and unweighted majority rule, showing how their relative epistemic performance depends on the distributional structure of information in the population, including unimodal competence distributions and segmented environments with informed minorities. The results indicate that endogenous and bounded epistemic weighting can improve collective accuracy by merging procedural and epistemic requirements.",
        "keywords": [
          "econ.GN",
          "cs.GT",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13499v1",
        "authors": [
          "Enrico Manfredi"
        ],
        "arxiv_categories": [
          "econ.GN",
          "cs.GT",
          "physics.soc-ph"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.711735",
      "entities": [
        "Beyond Majority Rule Collective",
        "Endogenous Epistemic Weighting",
        "Heterogeneous Information",
        "Epistemic Shared",
        "Choice Mechanism",
        "ESCM",
        "MIT",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.13453v1",
      "title": "Post-Matching Two-Way Fixed Effects Estimation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13453v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "When estimating treatment effects with two-way fixed effects (2WFE) models, researchers often use matching as a pre-processing step when the parallel trends assumption is thought to hold conditionally on covariates. Specifically, in a first step, each treated unit is matched to one or more untreated units based on observed time-invariant covariates. In the second step, treatment effects are estimated with a 2WFE regression in the matched sample, reweighting the untreated units by the number of times they are matched. We formally analyze this common practice and highlight two problems. First, when different treatment cohorts enter treatment in different time periods, the post-matching 2WFE estimator that pools all treated cohorts has an asymptotic bias, even when the treatment effect is constant across units and over time. Second, failing to account for the variability introduced by the matching procedure yields invalid standard error estimators, which can be biased upwards or downwards depending on the data generating process. We propose simple post-matching difference-in-differences estimators that compare each treated cohort to the never-treated separately, instead of pooling all treated cohorts. We provide conditions under which these estimators are consistent for well-defined causal parameters, and derive valid standard errors that account for the matching step. We illustrate our results with simulations and with an empirical application.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13453v1",
        "authors": [
          "Yihong Liu",
          "Gonzalo Vazquez-Bare"
        ],
        "arxiv_categories": [
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.711967",
      "entities": [
        "Way Fixed Effects Estimation",
        "Matching Two",
        "Standard",
        "Act",
        "EPA",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.13450v1",
      "title": "Inference From Random Restarts",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13450v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Algorithms for computing equilibria, optima, and fixed points in nonconvex problems often depend sensitively on practitioner-chosen initial conditions. When uniqueness of a solution is of interest, a common heuristic is to run such algorithms from many randomly selected initial conditions and to interpret repeated convergence to the same output as evidence of a unique solution or a dominant basin of attraction. Despite its widespread use, this practice lacks a formal inferential foundation. We provide a simple probabilistic framework for interpreting such numerical evidence. First, we give sufficient conditions under which an algorithm's terminal output is a measurable function of its initial condition, allowing probabilistic reasoning over outcomes. Second, we provide sufficient conditions ensuring that an algorithm admits only finitely many possible terminal outcomes. While these conditions may be difficult to verify on a case-by-case basis, we give simple sufficient conditions for broad classes of problems under which almost all instances admit only finitely many outcomes (in the sense of prevalence). Standard algorithms such as gradient descent and damped fixed-point iteration applied to sufficiently smooth functions satisfy these conditions. Within this framework, repeated solver runs correspond to independent samples from the induced distribution over outcomes. We adopt a Bayesian approach to infer basin sizes and the probability of solution uniqueness from repeated identical outputs, and we establish convergence rates for the resulting posterior beliefs. Finally, we apply our framework to settings in the existing industrial organization literature, where random-restart heuristics are used. Our results formalize and qualify these arguments, clarifying when repeated convergence provides meaningful evidence for uniqueness and when it does not.",
        "keywords": [
          "econ.EM",
          "stat.AP",
          "stat.ME"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13450v1",
        "authors": [
          "Moeen Nehzati",
          "Diego Cussen"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.AP",
          "stat.ME"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.712243",
      "entities": [
        "Inference From Random Restarts",
        "Framework",
        "Standard",
        "MIT",
        "DOE",
        "Act",
        "UN",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.12958v1",
      "title": "The Directions of Technical Change",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12958v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Generative AI is a directional technology: it excels at some task combinations and performs poorly at others. Knowledge work is also directional and endogenous: workers can satisfy their job requirements with different combinations of tasks. Studying AI adoption by knowledge workers hence requires comparing two vectors.We develop a high-dimensional model of task choice and technology adoption, with otherwise standard neoclassical assumptions. AI is adopted when its direction is aligned with what the worker values at the margin -- the worker's shadow prices, rather than with what the worker actually does -- their activity vector. This yields a cone of adoption that widens as AI capability grows; near the entry threshold, small improvements in capability translate into large expansions in the set of adopted directions. Adoption also has a structured intensive margin: a tool can be worth using but not worth using all the time, generating a region of stable hybrid production between an entry threshold and an all-in threshold. We also show how to derive shadow prices as explicit functions of observable skill and requirement vectors. The framework explains rapid adoption in aligned occupations, heterogeneous adoption elsewhere, and weak correlation with one-dimensional skill measures: the key heterogeneity is directional alignment, not skill level.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12958v1",
        "authors": [
          "Miklos Koren",
          "Zsofia Barany",
          "Ulrich Wohak"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.712451",
      "entities": [
        "Technical Change Generative",
        "Framework",
        "Standard",
        "DOE",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.12782v1",
      "title": "Empirical Validation of a Dual-Defense Mechanism Reshaping Wholesale Electricity Price Dynamics in Singapore",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12782v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "While ex-ante screening and static price caps are global standards for mitigating price volatility, Singapore's electricity market employs a unique dual-defense mechanism integrating vesting contracts (VC) with a temporary price cap (TPC). Using high-frequency data from 2021 to 2024, this paper evaluates this mechanism and yields three primary findings. First, a structural trade-off exists within the VC framework: while VC quantity (VCQ) suppresses average prices, it paradoxically exacerbates instability via liquidity squeezes. Conversely, VC price (VCP) functions as a tail-risk anchor, dominating at extreme quantiles where VCQ efficacy wanes. Second, a structural break around the 2023 reform reveals a fundamental re-mapping of price dynamics; the previously positive pass-through from offer ratios to clearing prices was largely neutralized post-reform. Furthermore, diagnostics near the TPC threshold show no systematic evidence of strategic bid shading, confirming the TPC's operational integrity. Third, the dual-defense mechanism exhibits a critical synergy that resolves the volatility trade-off. The TPC reverses the volatility penalty of high VCQ, shifting the elasticity of conditional volatility from a destabilizing 0.636 to a stabilizing -0.213. This synergy enables the framework to enhance tail-risk control while eliminating liquidity-related stability costs. We conclude that this dual-defense mechanism successfully decouples price suppression from liquidity risks, thereby maximizing market stability.",
        "keywords": [
          "eess.SY",
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12782v1",
        "authors": [
          "Huang Zhenyu",
          "Yuan Zhao"
        ],
        "arxiv_categories": [
          "eess.SY",
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.712690",
      "entities": [
        "Defense Mechanism Reshaping Wholesale",
        "Electricity Price Dynamics",
        "Empirical Validation",
        "Singapore While",
        "Framework",
        "Standard",
        "MIT",
        "TPC",
        "WHO",
        "Act",
        "VCQ",
        "VCP",
        "UN",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.12741v1",
      "title": "\"Unmatched\" From Skewed Births to a Structural Surplus of Grooms",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12741v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Data on marriage flows are not available in most developing countries, making marriage market imbalance difficult to measure. Existing measures use crude fertility rates and do not account for early-life mortality, overstating the number of births surviving to marriageable ages. This paper develops the Surplus Groom Index to quantify marriage market imbalance under monogamy using census age structure, vital registration of births and deaths, and marriage timing data. The index incorporates effective fertility-total births adjusted for under-five mortality - to reflect actual cohort progression from birth to marriageable ages. This adjustment matters in settings where child mortality shapes the supply of marriage partners. Using India's 2011 Census data, we find that eleven percent of men aged 15-54 cannot marry due to bride shortage, approximately 39 million men. Marriage imbalance is widespread rather than regionally concentrated. Punjab records the highest deficit at 33 percent, but states considered demographically progressive show substantial imbalance: Kerala 18 percent, West Bengal 14 percent, Karnataka and Tamil Nadu 11 percent each. Declining fertility has produced smaller female cohorts unable to absorb male-heavy cohorts from earlier birth years. Balanced sex ratios at birth do not ensure marriage market equilibrium once fertility declines and marriage is delayed.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12741v1",
        "authors": [
          "Praveen N",
          "Suddhasil Siddhanta"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.712912",
      "entities": [
        "Surplus Groom Index",
        "From Skewed Births",
        "Structural Surplus",
        "West Bengal",
        "Grooms Data",
        "Using India",
        "Tamil Nadu",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.12695v1",
      "title": "Generative AI and the Reallocation of Time: Productivity, Leisure, and Fulfilling Work",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12695v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Using a representative survey of Korean workers, we provide evidence on the adoption of Generative AI (GenAI) and how GenAI reallocates time at work. We find that 51.8\\% of workers use GenAI for work and GenAI reduces working time by 3.8\\%. However, these gains may not materialize in aggregate productivity statistics yet: the correlation between time savings and output changes is near zero. We show this disconnect arises because workers capture efficiency gains primarily as on-the-job leisure, rather than increasing their output. These findings suggest that standard productivity measures may understate AI's impact by missing non-pecuniary welfare channels.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12695v1",
        "authors": [
          "Donghyun Suh",
          "Samil Oh"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.713037",
      "entities": [
        "Fulfilling Work Using",
        "Standard",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.12504v1",
      "title": "Toggling the Defiers to Relax Monotonicity: The Difference-in-Instrumental-Variables Estimand",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12504v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Standard instrumental variables (IV) methods identify a Local Average Treatment Effect under monotonicity, which rules out defiers. In many empirical environments, however, distinct instruments may induce heterogeneous and even opposing behavioral responses. This paper introduces the Difference-in-Instrumental-Variables (DIIV) estimand, which exploits two instruments with opposing compliance patterns to recover a point-identified and behaviorally interpretable causal effect without imposing monotonicity. The estimand yields a convex combination of the marginal treatment effects on compliers and defiers, with weights reflecting differential shifts in treatment take-up across instruments. When monotonicity holds, DIIV coincides with the standard IV estimand. The approach can be implemented using simple linear transformations and standard two-stage least squares procedures. Applications using replication data illustrate its applicability in practice.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12504v1",
        "authors": [
          "Johann Caro-Burnett"
        ],
        "arxiv_categories": [
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.713203",
      "entities": [
        "Local Average Treatment Effect",
        "Variables Estimand Standard",
        "Relax Monotonicity",
        "Standard",
        "DIIV",
        "NSF",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.12490v1",
      "title": "Transformer-based CoVaR: Systemic Risk in Textual Information",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12490v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Conditional Value-at-Risk (CoVaR) quantifies systemic financial risk by measuring the loss quantile of one asset, conditional on another asset experiencing distress. We develop a Transformer-based methodology that integrates financial news articles directly with market data to improve CoVaR estimates. Unlike approaches that use predefined sentiment scores, our method incorporates raw text embeddings generated by a large language model (LLM). We prove explicit error bounds for our Transformer CoVaR estimator, showing that accurate CoVaR learning is possible even with small datasets. Using U.S. market returns and Reuters news items from 2006--2013, our out-of-sample results show that textual information impacts the CoVaR forecasts. With better predictive performance, we identify a pronounced negative dip during market stress periods across several equity assets when comparing the Transformer-based CoVaR to both the CoVaR without text and the CoVaR using traditional sentiment measures. Our results show that textual data can be used to effectively model systemic risk without requiring prohibitively large data sets.",
        "keywords": [
          "econ.EM",
          "q-fin.RM",
          "stat.ML"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12490v1",
        "authors": [
          "Junyu Chen",
          "Tom Boot",
          "Lingwei Kong",
          "Weining Wang"
        ],
        "arxiv_categories": [
          "econ.EM",
          "q-fin.RM",
          "stat.ML"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.713385",
      "entities": [
        "Textual Information Conditional Value",
        "Systemic Risk",
        "Transformer",
        "LLM",
        "NSF",
        "Act",
        "UN",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.12392v1",
      "title": "Scale and Capacity Limits in Decentralized FDA Food-Safety Enforcement",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12392v1",
        "published_date": "2026-02-12"
      },
      "content": {
        "abstract": "This paper asks whether regulatory monitoring exhibits nonlinear capacity limits as the scale and complexity of the regulated environment increase. Using a county--year panel of U.S. Food and Drug Administration (FDA) inspections merged with local establishment counts, we identify a sharp breakpoint: beyond a threshold scale, severe inspection findings rise while inspection effort per establishment flattens or declines. The threshold and the post-break deterioration vary across food-related industry groups and shift with proxies for local density and connectedness, consistent with monitoring becoming ``too big to monitor\" in more interconnected production environments rather than driven by simple reallocation or delay. Methodologically, we provide a portable breakpoint selection and piecewise-estimation framework that can be applied to other enforcement settings.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12392v1",
        "authors": [
          "Guy Tchuente"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.713534",
      "entities": [
        "Drug Administration",
        "Capacity Limits",
        "Framework",
        "NIST",
        "MIT",
        "FDA",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.12104v1",
      "title": "Liquidation Dynamics in DeFi and the Role of Transaction Fees",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12104v1",
        "published_date": "2026-02-12"
      },
      "content": {
        "abstract": "Liquidation of collateral are the primary safeguard for solvency of lending protocols in decentralized finance. However, the mechanics of liquidations expose these protocols to predatory price manipulations and other forms of Maximal Extractable Value (MEV). In this paper, we characterize the optimal liquidation strategy, via a dynamic program, from the perspective of a profit-maximizing liquidator when the spot oracle is given by a Constant Product Market Maker (CPMM). We explicitly model Oracle Extractable Value (OEV) where liquidators manipulate the CPMM with sandwich attacks to trigger profitable liquidation events. We derive closed-form liquidation bounds and prove that CPMM transaction fees act as a critical security parameter. Crucially, we demonstrate that fees do not merely reduce attacker profits, but can make such manipulations unprofitable for an attacker. Our findings suggest that CPMM transaction fees serve a dual purpose: compensating liquidity providers and endogenously hardening CPMM oracles against manipulation without the latency of time-weighted averages or medianization.",
        "keywords": [
          "q-fin.MF",
          "math.DS",
          "q-fin.TR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12104v1",
        "authors": [
          "Agathe Sadeghi",
          "Zachary Feinstein"
        ],
        "arxiv_categories": [
          "q-fin.MF",
          "math.DS",
          "q-fin.TR"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.713710",
      "entities": [
        "Constant Product Market Maker",
        "Transaction Fees Liquidation",
        "Maximal Extractable Value",
        "Oracle Extractable Value",
        "Liquidation Dynamics",
        "Protocol",
        "Oracle",
        "CPMM",
        "OEV",
        "MEV",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.12066v1",
      "title": "Chaos and Misallocation under Price Controls",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12066v1",
        "published_date": "2026-02-12"
      },
      "content": {
        "abstract": "Price controls kill the incentive for arbitrage. We prove a Chaos Theorem: under a binding price ceiling, suppliers are indifferent across destinations, so arbitrarily small cost differences can determine the entire allocation. The economy tips to corner outcomes in which some markets are fully served while others are starved; small parameter changes flip the identity of the corners, generating discontinuous welfare jumps. These corner allocations create a distinct source of cross-market misallocation, separate from the aggregate quantity loss (the Harberger triangle) and from within-market misallocation emphasized in prior work. They also create an identification problem: welfare depends on demand far from the observed equilibrium. We derive sharp bounds on misallocation that require no parametric assumptions. In an efficient allocation, shadow prices are equalized across markets; combined with the adding-up constraint, this collapses the infinite-dimensional welfare problem to a one-dimensional search over a common shadow price, with extremal losses achieved by piecewise-linear demand schedules. Calibrating the bounds to station-level AAA survey data from the 1973-74 U.S. gasoline crisis, misallocation losses range from roughly 1 to 9 times the Harberger triangle.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12066v1",
        "authors": [
          "Brian C. Albrecht",
          "Alex Tabarrok",
          "Mark Whitmeyer"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:41.713897",
      "entities": [
        "Price Controls Price",
        "EPA",
        "AAA",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15004v1",
      "title": "PDE foundation models are skillful AI weather emulators for the Martian atmosphere",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15004v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.",
        "keywords": [
          "cs.LG",
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15004v1",
        "authors": [
          "Johannes Schmude",
          "Sujit Roy",
          "Liping Wang",
          "Theodore van Kessel",
          "Levente Klein"
        ],
        "arxiv_categories": [
          "cs.LG",
          "physics.ao-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.718795",
      "entities": [
        "PDE",
        "Act",
        "GPU",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14905v1",
      "title": "Groundwater feedbacks on ice sheets and subglacial hydrology",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14905v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The dynamics of many of Antarctica's glaciers are modulated by a hydrological system at the base of the ice. Sedimentary basins beneath the ice bed contribute to the water budget in this hydrological system by discharging or taking up water. However, sedimentary basins are not included in most current models of ice dynamics, and little is known about their effect. In this paper we develop an idealised model of a glacier whose sliding is coupled to a subglacial hydrological system, which includes a sedimentary basin. We find that groundwater discharge (exfiltration) and recharge (infiltration) are controlled by the shape of the ice sheet and of the sedimentary basin, and that exfiltration promotes sliding whereas infiltration hinders it. Overall, the presence of a sedimentary basin leads to thicker and slower-flowing ice in the steady state. We also find that, when the ice sheet is undergoing retreating, groundwater exfiltration can lead to a positive feedback which accelerates this retreat. Our results shed light on the potential role and importance of Antarctic sedimentary basins, and how these might be incorporated into existing models of ice and subglacial hydrology.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14905v1",
        "authors": [
          "Gabriel J. Cairns",
          "Graham P. Benham",
          "Ian J. Hewitt"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.718955",
      "entities": [
        "WHO",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14863v1",
      "title": "Quasilocalization under coupled mutation-selection dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14863v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "When mutations are rampant, quasispecies theory or Eigen's model predicts that the fittest type in a population may not dominate. Beyond a critical mutation rate, the population may even be delocalized completely from the peak of the fitness landscape and the fittest is ironically lost. Extensive efforts have been made to understand this exceptional scenario. But in general, there is no simple prescription that predicts the eventual degree of localization for arbitrary fitness landscapes and mutation rates. Here, we derive a simple and general relation linking the quasispecies' Hill numbers, which are diversity metrics in ecology, and the ratio of an effective fitness variance to the mean mutation rate squared. This ratio, which we call the localization factor, emerges from mean approximations of decomposed surprisal or stochastic entropy change rates. On the side of application, the relation we obtained here defines a combination of Hill numbers that may complement other complexity or diversity measures for real viral quasispecies. Its advantage being that there is an underlying biological interpretation under Eigen's model.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14863v1",
        "authors": [
          "C. J. Palpal-latoc",
          "Ian Vega"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.719103",
      "entities": [
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14773v1",
      "title": "Climate network and complexity based ENSO forecast for 2026",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14773v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The El Niño Southern Oscillation (ENSO) is the dominant driver of interannual global climate variability and can lead to extreme weather events such as droughts or flooding. Recently, we have developed several statistical approaches for early ENSO forecasting, in particular, its El Niño phase. The climate network-based approach allows forecasting the onset of an El Niño event or its absence about 1 year ahead [1]. The complexity-based approach allows additionally to forecast the magnitude of an upcoming El Niño event in the calendar year before the onset [2]. Additionally, we have developed methods for forecasting the type (Eastern Pacific or Central Pacific) of an El Niño [3] and for probabilistic forecasting of La Niña and neutral events [4], also by the end of the calendar year before the event. Here we present the forecasts of these methods for 2026. The climate network and the complexity-based approach do not provide concurring signals for this year. The combined forecast indicates that a neutral event is more likely than an El Niño. If an El Niño develops in 2026, the complexity-based approach predicts a weaker event with a magnitude of $0.84\\pm0.36$°C.",
        "keywords": [
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14773v1",
        "authors": [
          "Josef Ludescher",
          "Jun Meng",
          "Jingfang Fan",
          "Armin Bunde",
          "Hans Joachim Schellnhuber"
        ],
        "arxiv_categories": [
          "physics.ao-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.719472",
      "entities": [
        "Southern Oscillation",
        "Central Pacific",
        "Eastern Pacific",
        "ENSO",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.14645v1",
      "title": "Conditions for Bacterial Selection and Extinction Driven by Growth-Kill Trade-Off in Cyclic Antimicrobial Treatments",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14645v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Antimicrobial protocols - using substances such as antibiotics or disinfectants - remain the preferred option for preventing the spread of pathogenic bacteria. However, bacteria can develop mechanisms to reduce their antimicrobial susceptibility, which can lead to treatment failure and the selection of resistance or tolerance. In this work, we propose a minimal population dynamics model to study bacterial selection during cyclic antimicrobial application, a commonly used protocol. Selection in bacterial populations with heterogeneous antimicrobial susceptibility is modelled here as a trade-off between survival advantage (reduction in antimicrobial killing) and potential fitness costs (reduction in growth rate) of the less susceptible strains. The proposed model allows us to derive useful expressions for determining the success of cyclic antimicrobial treatments based on two bacterial traits: growth and kill rates. The results obtained here are directly applicable to preventing the selection and spread of resistant and tolerant bacterial strains in real-life protocols.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14645v1",
        "authors": [
          "Nerea Martínez-López",
          "Niclas Nordholt",
          "Frank Schreiber",
          "Míriam R. García"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.719629",
      "entities": [
        "Cyclic Antimicrobial Treatments Antimicrobial",
        "Bacterial Selection",
        "Extinction Driven",
        "Kill Trade",
        "Protocol",
        "IoT",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14562v1",
      "title": "Infection models on dense dynamic random graphs",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14562v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We consider Susceptible-Infected-Recovered (SIR) models on dense dynamic random graphs, in which the joint dynamics of vertices and edges are co-evolutionary, i.e., they influence each other bidirectionally. In particular, edges appear and disappear over time depending on the states of the two connected vertices, on how long they have been infected, and on the total density of susceptible and infected vertices. Our main results establish functional laws of large numbers for the densities of susceptible, infected, and recovered vertices, jointly with the underlying evolving random graphs in the graphon space. Our results are supported by simulations, which characterize the limiting size of the epidemics, i.e., the limiting density of susceptible vertices, and how the peak of the epidemics depends on the rate of the evolution of the underlying graph. The proofs of our main results rely on the careful construction of a mimicking process, obtained by approximating the two-way feedback interaction between vertex and edge dynamics with a mean-field type interaction, acting only as one-way feedback, that remains sufficiently close to the original co-evolutionary process. To treat the more general setting in which edge dynamics are affected by the proportions of susceptible and infected individuals, we introduce a methodological extension of existing techniques. We thus show that our model exhibits multiple epidemic peaks -- a phenomenon observed in real-world epidemics -- which can emerge in models that incorporate mutual feedback between vertex and edge dynamics.",
        "keywords": [
          "math.PR",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14562v1",
        "authors": [
          "Simone Baldassarri",
          "Peter Braunsteins",
          "Frank den Hollander",
          "Michel Mandjes"
        ],
        "arxiv_categories": [
          "math.PR",
          "q-bio.PE"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.719840",
      "entities": [
        "SIR",
        "MIT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14560v1",
      "title": "Preliminary sonification of ENSO using traditional Javanese gamelan scales",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14560v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Sonification -- the mapping of data to non-speech audio -- offers an underexplored channel for representing complex dynamical systems. We treat El Niño-Southern Oscillation (ENSO), a canonical example of low-dimensional climate chaos, as a test case for culturally-situated sonification evaluated through complex systems diagnostics. Using parameter-mapping sonification of the Niño 3.4 sea surface temperature anomaly index (1870--2024), we encode ENSO variability into two traditional Javanese gamelan pentatonic systems (pelog and slendro) across four composition strategies, then analyze the resulting audio as trajectories in a two-dimensional acoustic phase space. Recurrence-based diagnostics, convex hull geometry, and coupling analysis reveal that the sonification pipeline preserves key dynamical signatures: alternating modes produce the highest trajectory recurrence rates, echoing ENSO's quasi-periodicity; layered polyphonic modes explore the broadest phase space regions; and the two scale families induce qualitatively distinct coupling regimes between spectral brightness and energy -- predominantly anti-phase in pelog but near-independent in slendro. Phase space trajectory analysis provides a rigorous geometric framework for comparing sonification designs within a complex systems context. Perceptual validation remains necessary; we contribute the dynamical systems methodology for evaluating such mappings.",
        "keywords": [
          "physics.soc-ph",
          "cs.SD",
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14560v1",
        "authors": [
          "Sandy H. S. Herho",
          "Rusmawan Suwarman",
          "Nurjanna J. Trilaksono",
          "Iwan P. Anwar",
          "Faiz R. Fajary"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "cs.SD",
          "physics.ao-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.720330",
      "entities": [
        "Southern Oscillation",
        "Framework",
        "ENSO",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14217v1",
      "title": "Limits on the Carroll-Field-Jackiw electrodynamics from geomagnetic data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14217v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "Lorentz-symmetry violation may be described via the CPT-odd, dimension-3, Carroll-Field-Jackiw term, which couples the electromagnetic fields to a constant 4-vector $k_{\\rm AF}$ selecting a preferred direction in spacetime. We solve the field equations using the Green's method for a static point-like magnetic dipole and find the $k_{\\rm AF}$-dependent corrections to the standard dipolar magnetic field that strongly dominates the near-Earth magnetic field. Given the very good agreement between current models and ground- and satellite-based geomagnetic data, our strongest constraints on the components of $k_{\\rm AF}$ in the Sun-centered frame read $|(k_{\\rm AF})_Z| \\lesssim 4 \\times 10^{-25} \\, {\\rm GeV}$ for $|(k_{\\rm AF})_X|, |(k_{\\rm AF})_Y| \\lesssim 10^{-24} \\, {\\rm GeV}$ at the two-sigma level. This represents an improvement of about four orders of magnitude over earlier bounds based on other geophysical phenomena.",
        "keywords": [
          "hep-ph",
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14217v1",
        "authors": [
          "G. F. de Carvalho",
          "M. Fillion",
          "P. C. Malta",
          "C. A. D. Zarro"
        ],
        "arxiv_categories": [
          "hep-ph",
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.720466",
      "entities": [
        "Satellite",
        "Agreement",
        "Standard",
        "MIT",
        "CPT",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14141v1",
      "title": "Ice-free geomorphometry of Queen Maud Land, East Antarctica: 3. Belgica and Yamato (Queen Fabiola) Mountains",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14141v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "Geomorphometric modeling and mapping of ice-free Antarctic areas can be applied for obtaining new quantitative knowledge about the topography of these unique landscapes and for the further use of morphometric information in Antarctic research. Within the framework of a project of creating a physical geographical thematic scientific reference geomorphometric atlas of ice-free areas of Antarctica, we performed geomorphometric modeling and mapping of two, partly ice-free mountainous areas of the eastern Queen Maud Land, East Antarctica. These include the Belgica Mountains and Yamato (Queen Fabiola) Mountains. As input data, we used two fragments of the Reference Elevation Model of Antarctica (REMA). For the two ice-free areas and adjacent glaciers, we derived models and maps of eleven, most scientifically important morphometric variables (i.e., slope, aspect, horizontal curvature, vertical curvature, minimal curvature, maximal curvature, catchment area, topographic wetness index, stream power index, total insolation, and wind exposition index). The obtained models and maps describe the ice-free topography of the Belgica Mountains and Yamato (Queen Fabiola) Mountains in a rigorous, quantitative, and reproducible manner. New morphometric data can be useful for further geological, geomorphological, glaciological, ecological, and hydrological studies of these areas.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14141v1",
        "authors": [
          "I. V. Florinsky",
          "S. O. Zharnova"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.720653",
      "entities": [
        "Mountains Geomorphometric",
        "Reference Elevation Model",
        "Belgica Mountains",
        "East Antarctica",
        "Queen Maud Land",
        "Queen Fabiola",
        "Framework",
        "Wind",
        "REMA",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.13913v1",
      "title": "Gauge-Mediated Contagion: A Quantum Electrodynamics-Inspired Framework for Non-Local Epidemic Dynamics and Superdiffusion",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13913v1",
        "published_date": "2026-02-14"
      },
      "content": {
        "abstract": "In this paper, we introduce a gauge-mediated Epidemiological Model inspired by Quantum Electrodynamics (QED). In this model, the ``direct contact'' paradigm of classical SIR models is replaced by a gauge-mediated interaction where the environment, represented by a pathogen field $\\varphi$, plays a fundamental role in the epidemic dynamics. In this model, the non-local characteristics of epidemics appear naturally by integrating out the pathogen field. Utilizing the Doi-Peliti formalism, we derive the effective action of the system and the standard Feynman rules that can be used to compute perturbatively any observables. Using standard QED techniques, we show how to relate renormalized pathogen mass, Debye screening, to epidemiological concepts and we compute at first order the effective reproductive number,$R_{eff}$, and how the condition to have an epidemic is related to a phase transition in the pathogen mass. We show that the superspreading hosts can be included easily in this formalism.",
        "keywords": [
          "q-bio.PE",
          "physics.bio-ph",
          "physics.data-an",
          "physics.med-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13913v1",
        "authors": [
          "Jose de Jesus Bernal-Alvarado",
          "David Delepine"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "physics.bio-ph",
          "physics.data-an",
          "physics.med-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.720796",
      "entities": [
        "Local Epidemic Dynamics",
        "Quantum Electrodynamics",
        "Epidemiological Model",
        "Mediated Contagion",
        "Inspired Framework",
        "Superdiffusion In",
        "Framework",
        "Standard",
        "Fusion",
        "SIR",
        "QED",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.13847v1",
      "title": "Causally constrained reduced-order neural models of complex turbulent dynamical systems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13847v1",
        "published_date": "2026-02-14"
      },
      "content": {
        "abstract": "We introduce a flexible framework based on response theory and score matching to suppress spurious, noncausal dependencies in reduced-order neural emulators of turbulent systems, focusing on climate dynamics as a proof-of-concept. We showcase the approach using the stochastic Charney-DeVore model as a relevant prototype for low-frequency atmospheric variability. We show that the resulting causal constraints enhance neural emulators' ability to respond to both weak and strong external forcings, despite being trained exclusively on unforced data. The approach is broadly applicable to modeling complex turbulent dynamical systems in reduced spaces and can be readily integrated into general neural network architectures.",
        "keywords": [
          "nlin.CD",
          "cond-mat.stat-mech",
          "cs.LG",
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13847v1",
        "authors": [
          "Fabrizio Falasca",
          "Laure Zanna"
        ],
        "arxiv_categories": [
          "nlin.CD",
          "cond-mat.stat-mech",
          "cs.LG",
          "physics.ao-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.720898",
      "entities": [
        "Neural Network",
        "Framework",
        "UN",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.13820v1",
      "title": "Water-induced buoyancy controls transient water storage in the mantle transition zone",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13820v1",
        "published_date": "2026-02-14"
      },
      "content": {
        "abstract": "The spinel phase (wadsleyite, ringwoodite) in the mantle transition zone (MTZ), can contain up to 1-2 weight percent of water. However, whether these water reservoirs in the MTZ are filled is debated. Here, we investigate water dynamics in the MTZ numerically by using a newly developed empirical model of deep hydrous mantle melting combined with 2D thermo-hydro-mechanical-chemical (THMC) upper mantle models. Numerical modeling results suggest that water-induced buoyancy triggers the development of hydrous solid-state mantle upwellings in the MTZ. On time scales of some tens of millions of years, they rise to and interact with the spinel-olivine phase transition. Depending on the water content and temperature of these thermal-chemical plumes, this crossing may trigger hydrous melting by water release from the wadsleyite upon its conversion to olivine. The melts are less dense than the solid matrix and continue rising upward in the form of either diapirs or porosity waives. Similar dehydration-induced melting process3 is also documented for the lower MTZ boundary, where hydrous downwellings (such as subducted slabs) generate buoyant melt diapirs rising through the MTZ. We therefore suggest that the MTZ operates as a transient water reservoir. Relatively small amounts of water (less than 0.1 weight percent, smaller than 0.2 ocean masses) and a geologically moderate duration (80-430 Myr) of the transient water storage should be characteristic for the MTZ, which may play a key role in stabilizing the surface ocean mass on Earth and Earth-like rocky exoplanets.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13820v1",
        "authors": [
          "Taras V. Gerya",
          "Nickolas M. Bardi",
          "Shun-ichiro Karato",
          "Motohiko Murakami"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.721098",
      "entities": [
        "Labs",
        "THMC",
        "MTZ",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.13630v1",
      "title": "Bistability to Quad-stability: Emergence of Hybrid Phenotypes & Enhanced Spatio-temporal Plasticity in Presence of Host-Circuit Coupling",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13630v1",
        "published_date": "2026-02-14"
      },
      "content": {
        "abstract": "In the context of multistability driven diseases, like cancer, spatiotemporal plasticity plays a significant role to achieve a spectrum of phenotypic variations. The interplay between gene regulatory networks and environmental factors, such as resource competition and spatial diffusion, plays a crucial role in determining cellular behaviour and phenotypic heterogeneity. Though reaction diffusion frameworks have been widely applied in developmental biology, less attention has been paid to the simultaneous effects of resource competition and growth feedback on spatial organization. In this paper, we observed that a bistable genetic circuit under high resource competition due to growth feedback gives rise to multiple emergent phenotypes, as observed in cancer systems. Furthermore, we observed how spatial diffusion coupled with intrinsic nonlinearity can drive the emergence of distinct spatial dynamics over time. The observed spatiotemporal plasticity can also be driven by the comparative stability of the fixed points, diffusivity, and asymmetry of diffusion. Our findings highlight that growth-induced resource competition combined with diffusion can provide deeper insights into metastasis and cancer progression.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13630v1",
        "authors": [
          "Ranu Kundu",
          "Priya Chakraborty",
          "Sohini Guin",
          "Shyam Sundar Poriah",
          "Sayantari Ghosh"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.721260",
      "entities": [
        "Circuit Coupling In",
        "Hybrid Phenotypes",
        "Enhanced Spatio",
        "Framework",
        "Fusion",
        "Meta",
        "IoT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.13423v1",
      "title": "Spatiotemporal noise stabilizes unbounded diversity in strongly-competitive communities",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13423v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Classical ecological models predict that large, diverse communities should be unstable, presenting a central challenge to explaining the stable biodiversity seen in nature. We revisit this long-standing problem by extending the generalized Lotka-Volterra model to include both spatial structure and environmental fluctuations across space and time. We find that neither space nor environmental noise alone can resolve the tension between diversity and stability, but that their combined effects permit arbitrarily many species to stably coexist despite strongly disordered competitive interactions. We analytically characterize the noise-induced transition to coexistence, showing that spatiotemporal noise drives an anomalous scaling of abundance fluctuations, known empirically as Taylor's law. At the community level, this manifests as an effective sublinear self-inhibition that renders the community stable and asymptotically neutral in the high-diversity limit. Spatiotemporal noise thus provides a novel resolution to the diversity-stability paradox and a generic mechanism by which complex communities can persist.",
        "keywords": [
          "q-bio.PE",
          "cond-mat.dis-nn",
          "cond-mat.stat-mech"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13423v1",
        "authors": [
          "Amer Al-Hiyasat",
          "Daniel W. Swartz",
          "Jeff Gore",
          "Mehran Kardar"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "cond-mat.dis-nn",
          "cond-mat.stat-mech"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.721406",
      "entities": [
        "MIT",
        "IoT",
        "Act",
        "UN",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.13181v1",
      "title": "Selection of CMIP6 Models for Regional Precipitation Projection and Climate Change Assessment in the Jhelum and Chenab River Basins",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13181v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Effective water resource management depends on accurate projections of flows in water channels. For projected climate data, use of different General Circulation Models (GCM) simulates contrasting results. This study shows selection of GCM for the latest generation CMIP6 for hydroclimate change impact studies. Envelope based method was used for the selection, which includes components based on machine learning techniques, allowing the selection of GCMs without the need for in-situ reference data. According to our knowledge, for the first time, such a comparison was performed for the CMIP6 Shared Socioeconomic Pathway (SSP) scenarios data. In addition, the effect of climate change under SSP scenarios was studied, along with the calculation of extreme indices. Finally, GCMs were compared to quantify spatiotemporal differences between CMIP5 and CMIP6 data. Results provide NorESM2 LM, FGOALS g3 as selected models for the Jhelum and Chenab River. Highly vulnerable regions under the effect of climate change were highlighted through spatial maps, which included parts of Punjab, Jammu, and Kashmir. Upon comparison of CMIP5 and CMIP6, no discernible difference was found between the RCP and SSP scenarios precipitation projections. In the future, more detailed statistical comparisons could further reinforce the proposition.",
        "keywords": [
          "physics.ao-ph",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13181v1",
        "authors": [
          "Saad Ahmed Jamal",
          "Ammara Nusrat",
          "Muhammad Azmat",
          "Muhammad Osama Nusrat"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "cs.LG"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.721591",
      "entities": [
        "Regional Precipitation Projection",
        "Chenab River Basins Effective",
        "Shared Socioeconomic Pathway",
        "General Circulation Models",
        "Climate Change Assessment",
        "Machine Learning",
        "Chenab River",
        "FGOALS",
        "RCP",
        "IoT",
        "Act",
        "SSP",
        "GCM",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.13121v1",
      "title": "LinkedNN: a neural model of linkage disequilibrium decay for recent effective population size inference",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.13121v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Summary: A bioinformatics tool is presented for estimating recent effective population size by using a neural network to automatically compute linkage disequilibrium-related features as a function of genomic distance between polymorphisms. The new method outperforms existing deep learning and summary statistic-based approaches using relatively few sequenced individuals and variant sites, making it particularly valuable for molecular ecology applications with sparse, unphased data. Availability and implementation: The program is available as an easily installable Python package with documentation here: https://pypi.org/project/linkedNN/. The open source code is available from: https://github.com/the-smith-lab/LinkedNN.",
        "keywords": [
          "q-bio.PE",
          "q-bio.QM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.13121v1",
        "authors": [
          "Chris C R Smith"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "q-bio.QM"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.721694",
      "entities": [
        "Neural Network",
        "Deep Learning",
        "MIT",
        "UN",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.12854v1",
      "title": "Hyb-Adam-UM: hybrid ultrametric-aware mtDNA phylogeny reconstruction",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12854v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Motivation: mtDNA distance matrices are standard inputs for distance-based phylogeny, but computing all pairwise alignments is costly. Missing entries can degrade inferred topology and branch lengths, and generic matrix-completion methods may disrupt tree-like (ultrametric) structure. Results: We propose Hyb-Adam-UM, which starts from an alignment-limited Needleman-Wunsch distance backbone and completes the matrix by minimizing a robust triplet ultrametric-violation functional. An Adam-style finite-difference optimizer updates only missing entries while enforcing symmetry, non-negativity, and a zero diagonal. From one complete reference matrix, we generate 20 masked instances at 30%, 50%, 65%, and 85% missingness. Hyb-Adam-UM consistently reduces ultrametric violations and achieves competitive reconstruction error, with improved topological accuracy and branch-length agreement relative to MW*/NJ* projection baselines (which exactly preserve observed distances) and Soft-Impute; gains are most pronounced at 85% missingness. Availability and implementation: https://github.com/mitichya/hyb-adam-um/; Zenodo: https://doi.org/10.5281/zenodo.18609748 Supplementary information: Supplementary data available online.",
        "keywords": [
          "q-bio.PE",
          "math.NA"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12854v1",
        "authors": [
          "Dmitrii Chaikovskii",
          "Weilai Qu",
          "Boris Melnikov",
          "Ye Zhang",
          "Yuehong Zhao"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "math.NA"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.721850",
      "entities": [
        "Agreement",
        "Standard",
        "An Adam",
        "MIT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.12497v1",
      "title": "Winter forecasting of September/October rainfall",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12497v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "We formulate seasonal rainfall prediction as a reduced-order nonlinear forecasting problem, embedding coupled Indian-Pacific Ocean variability into a low-dimensional state space and projecting it forward using deep neural networks. Variables include Nino 3.4, the Indian Ocean Dipole (IOD), the Indian Ocean meridional SST gradient, and selected empirical orthogonal functions. Monthly time series of the variables then form the input into deep neural networks which project rainfall further into the future. Forecasts for the 2025 austral spring were generated and archived in the Mendeley database during the winter. Subsequent rainfall data demonstrated a high level of agreement with the forecasts, providing a validation of the method and supporting the hypothesis that chaotic yet conditionally predictable dynamics underpin spring rainfall variability in southeastern Australia.",
        "keywords": [
          "physics.ao-ph",
          "nlin.CD"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12497v1",
        "authors": [
          "Stjepan Marcelja"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "nlin.CD"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.721971",
      "entities": [
        "Indian Ocean Dipole",
        "Neural Network",
        "Pacific Ocean",
        "Indian Ocean",
        "Agreement",
        "IOD",
        "SST",
        "UN",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.12488v1",
      "title": "Mapping ammonia emission plumes using shortwave infrared imaging spectroscopy",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12488v1",
        "published_date": "2026-02-13"
      },
      "content": {
        "abstract": "Atmospheric ammonia emissions are harmful to ecosystems and human health. These emissions have traditionally been monitored using thermal infrared spectrometers, though such techniques are limited by thermal contrast requirements, the coarse spatial resolution of existing satellite sensors, and low measurement frequency of higher-resolution aerial surveys. Here, we show that ammonia emissions can be quantified using shortwave infrared imaging spectroscopy, circumventing these challenges by using reflected sunlight instead of thermal emission for signal and by enabling a large class of existing and future imaging spectrometers to enter the ammonia observing system. As a proof of concept for this newly discovered capability, we use Tanager-1 satellite data to quantify emissions from industrial point sources of ammonia in Pakistan and Uzbekistan.",
        "keywords": [
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12488v1",
        "authors": [
          "Nicholas Balasus",
          "Daniel H. Cusworth",
          "Jinsol Kim",
          "Daniel J. Varon",
          "Charles E. Miller"
        ],
        "arxiv_categories": [
          "physics.ao-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.722087",
      "entities": [
        "Tanager-1",
        "Satellite",
        "MIT",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.12408v1",
      "title": "Detecting Spatiotemporal b-Value Anomalies with a Progressive Deep Learning Architecture",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.12408v1",
        "published_date": "2026-02-12"
      },
      "content": {
        "abstract": "Identifying systematic patterns in seismicity that precede large earthquakes remains a central challenge in statistical seismology. In this work, we present a methodological framework for detecting spatiotemporal anomalies in seismicity using the evolution of gridded b-values. Focusing on the Japanese subduction zone, we construct daily b-value fields on a fine spatial grid by aggregating local seismicity over moving time windows, yielding a continuous 2+1D representation of seismic-state evolution. We formulate the problem as a binary classification task in which spatiotemporal blocks extracted from these $b$-value fields are labeled according to the occurrence of a target earthquake with \\Mw $\\geq 5$ in the central region within the next day. To model this data, we introduce a hybrid deep-learning architecture that combines a spatial convolutional encoder with a temporal convolutional network, enabling joint learning of spatial structure and temporal dynamics. A progressive meta-epoch training scheme is employed, in which the model is iteratively updated using a time-forward strategy that mirrors operational deployment and mitigates issues related to nonstationarity. This paper is strictly methodological in scope. It describes the construction of b-value fields, the spatiotemporal sampling strategy, the network architecture, and the progressive training and internal validation framework used for model development and parameter selection.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.12408v1",
        "authors": [
          "Jonas Köhler",
          "Wei Li",
          "Johannes Faber",
          "Georg Rümpker",
          "Nishtha Srivastava"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-17T12:40:44.722268",
      "entities": [
        "Progressive Deep Learning Architecture",
        "Detecting Spatiotemporal",
        "Value Anomalies",
        "Deep Learning",
        "Framework",
        "Wind",
        "Meta",
        "MIT",
        "IoT",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15011v1",
      "title": "TouchFusion: Multimodal Wristband Sensing for Ubiquitous Touch Interactions",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15011v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "TouchFusion is a wristband that enables touch interactions on nearby surfaces without any additional instrumentation or computer vision. TouchFusion combines surface electromyography (sEMG), bioimpedance, inertial, and optical sensing to capture multiple facets of hand activity during touch interactions. Through a combination of early and late fusion, TouchFusion enables stateful touch detection on both environmental and body surfaces, simple surface gestures, and tracking functionality for contextually adaptive interfaces as well as basic trackpad-like interactions. We validate our approach on a dataset of 100 participants, significantly exceeding the population size of typical wearable sensing studies to capture a wider variance of wrist anatomies, skin conductivities, and behavioral patterns. We show that TouchFusion can enable several common touch interaction tasks. Using TouchFusion, a wearer can summon a trackpad on any surface, control contextually adaptive interfaces based on where they tap, or use their palm as an always-available touch surface. When paired with smart glasses or augmented reality devices, TouchFusion enables a ubiquitous, contextually adaptive interaction model.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15011v1",
        "authors": [
          "Eric Whitmire",
          "Evan Strasnick",
          "Roger Boldu",
          "Raj Sodhi",
          "Nathan Godwin"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.731236",
      "entities": [
        "Ubiquitous Touch Interactions",
        "Multimodal Wristband Sensing",
        "Fusion",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15007v1",
      "title": "Hidden Markov Individual-level Models of Infectious Disease Transmission",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15007v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Individual-level epidemic models are increasingly being used to help understand the transmission dynamics of various infectious diseases. However, fitting such models to individual-level epidemic data is challenging, as we often only know when an individual's disease status was detected (e.g., when they showed symptoms) and not when they were infected or removed. We propose an autoregressive coupled hidden Markov model to infer unknown infection and removal times, as well as other model parameters, from a single observed detection time for each detected individual. Unlike more traditional data augmentation methods used in epidemic modelling, we do not assume that this detection time corresponds to infection or removal or that infected individuals must at some point be detected. Bayesian coupled hidden Markov models have been used previously for individual-level epidemic data. However, these approaches assumed each individual was continuously tested and that the tests were independent. In practice, individuals are often only tested until their first positive test, and even if they are continuously tested, only the initial detection times may be reported. In addition, multiple tests on the same individual may not be independent. We accommodate these scenarios by assuming that the probability of detecting the disease can depend on past observations, which allows us to fit a much wider range of practical applications. We illustrate the flexibility of our approach by fitting two examples: an experiment on the spread of tomato spot wilt virus in pepper plants and an outbreak of norovirus among nurses in a hospital.",
        "keywords": [
          "stat.AP",
          "stat.ME"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15007v1",
        "authors": [
          "Dirk Douwes-Schultz",
          "Rob Deardon",
          "Alexandra M. Schmidt"
        ],
        "arxiv_categories": [
          "stat.AP",
          "stat.ME"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.731558",
      "entities": [
        "Infectious Disease Transmission Individual",
        "Hidden Markov Individual",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14951v1",
      "title": "Sovereign Agents: Towards Infrastructural Sovereignty and Diffused Accountability in Decentralized AI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14951v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "AI agents deployed on decentralized infrastructures are beginning to exhibit properties that extend beyond autonomy toward what we describe as agentic sovereignty-the capacity of an operational agent to persist, act, and control resources with non-overrideability inherited from the infrastructures in which they are embedded. We propose infrastructural sovereignty as an analytic lens for understanding how cryptographic self-custody, decentralized execution environments, and protocol-mediated continuity scaffold agentic sovereignty. While recent work on digital and network sovereignty has moved beyond state-centric and juridical accounts, these frameworks largely examine how sovereignty is exercised through technical systems by human collectives and remain less equipped to account for forms of sovereignty that emerge as operational properties of decentralized infrastructures themselves, particularly when instantiated in non-human sovereign agents. We argue that sovereignty in such systems exists on a spectrum determined by infrastructural hardness-the degree to which underlying technical systems resist intervention or collapse. While infrastructural sovereignty may increase resilience, it also produces a profound accountability gap: responsibility diffuses across designers, infrastructure providers, protocol governance, and economic participants, undermining traditional oversight mechanisms such as human-in-the-loop control or platform moderation. Drawing on examples like Trusted Execution Environments (TEEs), decentralized physical infrastructure networks (DePIN), and agent key continuity protocols, we analyze the governance challenges posed by non-terminable AI agents and outline infrastructure-aware accountability strategies for emerging decentralized AI systems.",
        "keywords": [
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14951v1",
        "authors": [
          "Botao Amber Hu",
          "Helena Rong"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.731909",
      "entities": [
        "Towards Infrastructural Sovereignty",
        "Trusted Execution Environments",
        "Diffused Accountability",
        "Sovereign Agents",
        "Framework",
        "Protocol",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14940v1",
      "title": "Kami of the Commons: Towards Designing Agentic AI to Steward the Commons",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14940v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Commons suffer from neglect, free-riding, and a persistent deficit of care. Inspired by Shinto animism -- where every forest, river, and mountain has its own \\emph{kami}, a spirit that inhabits and cares for that place -- we provoke: what if every commons had its own AI steward? Through a speculative design workshop where fifteen participants used Protocol Futuring, we surface both new opportunities and new dangers. Agentic AI offers the possibility of continuously supporting commons with programmable agency and care -- stewards that mediate family life as the most intimate commons, preserve collective knowledge, govern shared natural resources, and sustain community welfare. But when every commons has its own steward, second-order effects emerge: stewards contest stewards as overlapping commons collide; individuals caught between multiple stewards face new politics of care and constraint; the stewards themselves become commons requiring governance. This work opens \\emph{agentive governance as commoning design material} -- a new design space for the agency, care ethics, and accountability of AI stewards of shared resources -- radically different from surveillance or optimization.",
        "keywords": [
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14940v1",
        "authors": [
          "Botao Amber Hu"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.732159",
      "entities": [
        "Towards Designing Agentic",
        "Protocol Futuring",
        "Commons Commons",
        "Protocol",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14889v1",
      "title": "Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14889v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.",
        "keywords": [
          "cs.LG",
          "cs.CV",
          "cs.ET",
          "cs.HC",
          "cs.NE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14889v1",
        "authors": [
          "Mounvik K",
          "N Harshit"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.CV",
          "cs.ET",
          "cs.HC",
          "cs.NE"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.732401",
      "entities": [
        "Scale Multimodal Summarization",
        "Based Semantic Alignment We",
        "Framework",
        "CLIP",
        "BLIP",
        "MIT",
        "API",
        "ROC",
        "AUC",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14877v1",
      "title": "When to repeat a biomarker test? Decomposing sources of variation from conditionally repeated measurements",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14877v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Repeating an imperfect biomarker test based on an initial result can introduce bias and influence misclassification risk. For example, in some blood donation settings, blood donors' hemoglobin is remeasured when the initial measurement falls below a minimum threshold for donor eligibility. This paper explores methods that use data resulting from processes with conditionally repeated biomarker measurement to decompose the variation in observed measurements of a continuous biomarker into population variability and variability arising from the measurement procedure. We present two frequentist approaches with analytical solutions, but these approaches perform poorly in a dataset of conditionally repeated blood donor hemoglobin measurements where normality assumptions are not met. We then develop a Bayesian hierarchical framework that allows for different distributional assumptions, which we apply to the blood donor hemoglobin dataset. Using a Bayesian hierarchical model that assumes normally distributed population hemoglobin and heavy tailed $t$-distributed measurement variation, we found that the total measurement variation accounted for 22\\% of the total variance among females and 25\\% among males, with population standard deviations of $1.07\\, \\rm g/dL$ for female donors and $1.28\\, \\rm g/dL$ for male donors. Our Bayesian framework can use data resulting from any clinical process with conditionally repeated biomarker measurements to estimate individuals' misclassification risk after one or more noisy continuous measurements and inform evidence-based conditional retesting decision rules.",
        "keywords": [
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14877v1",
        "authors": [
          "Supun Manathunga",
          "Mart P. Janssen",
          "Yu Luo",
          "W. Alton Russell",
          "Mart Pothast"
        ],
        "arxiv_categories": [
          "stat.AP"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.732724",
      "entities": [
        "Our Bayesian",
        "Framework",
        "Standard",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14835v1",
      "title": "The Global Representativeness Index: A Total Variation Distance Framework for Measuring Demographic Fidelity in Survey Research",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14835v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Global survey research increasingly informs high-stakes decisions in AI governance and cross-cultural policy, yet no standardized metric quantifies how well a sample's demographic composition matches its target population. Response rates and demographic quotas -- the prevailing proxies for sample quality -- measure effort and coverage but not distributional fidelity. This paper introduces the Global Representativeness Index (GRI), a framework grounded in Total Variation Distance that scores any survey sample against population benchmarks across multiple demographic dimensions on a [0, 1] scale. Validation on seven waves of the Global Dialogues survey (N = 7,500 across 60+ countries) finds fine-grained demographic GRI scores of only 0.33--0.36 -- roughly 43% of the theoretical maximum at that sample size. Cross-validation on the World Values Survey (seven waves, N = 403,000), Afrobarometer Round 9 (N = 53,000), and Latinobarometro (N = 19,000) reveals that even large probability surveys score below 0.22 on fine-grained global demographics when country coverage is limited. The GRI connects to classical survey statistics through the design effect; both metrics are recommended as a minimum summary of sample quality, since GRI quantifies demographic distance symmetrically while effective N captures the asymmetric inferential cost of underrepresentation. The framework is released as an open-source Python library with UN and Pew Research Center population benchmarks, applicable to survey research, machine learning dataset auditing, and AI evaluation benchmarks.",
        "keywords": [
          "stat.ME",
          "cs.CY",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14835v1",
        "authors": [
          "Evan Hadfield"
        ],
        "arxiv_categories": [
          "stat.ME",
          "cs.CY",
          "stat.AP"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.733042",
      "entities": [
        "Total Variation Distance Framework",
        "Global Representativeness Index",
        "Measuring Demographic Fidelity",
        "Total Variation Distance",
        "Survey Research Global",
        "Afrobarometer Round",
        "World Values Survey",
        "Pew Research Center",
        "Global Dialogues",
        "Machine Learning",
        "Framework",
        "Standard",
        "Policy",
        "MIT",
        "GRI"
      ]
    },
    {
      "id": "arxiv-2602.14831v1",
      "title": "Robot-Wearable Conversation Hand-off for Navigation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14831v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Navigating large and complex indoor environments, such as universities, airports, and hospitals, can be cognitively demanding and requires attention and effort. While mobile applications provide convenient navigation support, they occupy the user's hands and visual attention, limiting natural interaction. In this paper, we explore conversation hand-off as a method for multi-device indoor navigation, where a Conversational Agent (CA) transitions seamlessly from a stationary social robot to a wearable device. We evaluated robot-only, wearable-only, and robot-to-wearable hand-off in a university campus setting using a within-subjects design with N=24 participants. We find that conversation hand-off is experienced as engaging, even though no performance benefits were observed, and most preferred using the wearable-only system. Our findings suggest that the design of such re-embodied assistants should maintain a shared voice and state across embodiments. We demonstrate how conversational hand-offs can bridge cognitive and physical transitions, enriching human interaction with embodied AI.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14831v1",
        "authors": [
          "Dániel Szabó",
          "Aku Visuri",
          "Benjamin Tag",
          "Simo Hosio"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.733256",
      "entities": [
        "Wearable Conversation Hand",
        "Navigation Navigating",
        "Conversational Agent",
        "University",
        "Robot",
        "MIT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14783v1",
      "title": "What hackers talk about when they talk about AI: Early-stage diffusion of a cybercrime innovation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14783v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The rapid expansion of artificial intelligence (AI) is raising concerns about its potential to transform cybercrime. Beyond empowering novice offenders, AI stands to intensify the scale and sophistication of attacks by seasoned cybercriminals. This paper examines the evolving relationship between cybercriminals and AI using a unique dataset from a cyber threat intelligence platform. Analyzing more than 160 cybercrime forum conversations collected over seven months, our research reveals how cybercriminals understand AI and discuss how they can exploit its capabilities. Their exchanges reflect growing curiosity about AI's criminal applications through legal tools and dedicated criminal tools, but also doubts and anxieties about AI's effectiveness and its effects on their business models and operational security. The study documents attempts to misuse legitimate AI tools and develop bespoke models tailored for illicit purposes. Combining the diffusion of innovation framework with thematic analysis, the paper provides an in-depth view of emerging AI-enabled cybercrime and offers practical insights for law enforcement and policymakers.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14783v1",
        "authors": [
          "Benoît Dupont",
          "Chad Whelan",
          "Serge-Olivier Paquette"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.733484",
      "entities": [
        "Artificial Intelligence",
        "Framework",
        "Policy",
        "Fusion",
        "Intel",
        "NSF",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14780v1",
      "title": "ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14780v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.",
        "keywords": [
          "cs.MA",
          "cs.CY",
          "cs.RO",
          "eess.SY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14780v1",
        "authors": [
          "Anna-Lena Schlamp",
          "Jeremias Gerner",
          "Klaus Bogenberger",
          "Werner Huber",
          "Stefanie Schmidtner"
        ],
        "arxiv_categories": [
          "cs.MA",
          "cs.CY",
          "cs.RO",
          "eess.SY"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.733734",
      "entities": [
        "Roundabout Optimized Speed Advisory",
        "Agent Trajectory Prediction",
        "Vulnerable Road Users",
        "Multimodal Traffic We",
        "Transformer",
        "ROSA",
        "NIST",
        "NSF",
        "VRU",
        "Act",
        "FDE",
        "ADE",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14778v1",
      "title": "A Geometric Analysis of Small-sized Language Model Hallucinations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14778v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings. This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%. Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14778v1",
        "authors": [
          "Emanuele Ricco",
          "Elia Onofri",
          "Lorenzo Cima",
          "Stefano Cresci",
          "Roberto Di Pietro"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.733929",
      "entities": [
        "Language Model Hallucinations Hallucinations",
        "Geometric Analysis",
        "LLM",
        "Act",
        "EPA"
      ]
    },
    {
      "id": "arxiv-2602.14770v1",
      "title": "Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14770v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (Δ = 0.440) and Social Response (Δ = 0.422), with occasional increases in aggressive humor.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14770v1",
        "authors": [
          "Shiwei Hong",
          "Lingyao Li",
          "Ethan Z. Rong",
          "Chenxinran Shen",
          "Zhicong Lu"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.734381",
      "entities": [
        "Investigating Community Discussion Effects",
        "Humor Generation Prior",
        "Agent Comedy Club",
        "Social Response",
        "LLM",
        "MIT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14740v1",
      "title": "AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14740v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act. Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making. Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence. We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.GT"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14740v1",
        "authors": [
          "Kenneth Payne"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.GT"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.734711",
      "entities": [
        "Frontier Models Exhibit Sophisticated",
        "Simulated Nuclear Crises Today",
        "Claude Sonnet",
        "Framework",
        "GPT-5.2",
        "Nuclear",
        "Meta",
        "MIT",
        "DOE",
        "GPT",
        "Act",
        "EPA",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14733v1",
      "title": "More than Decision Support: Exploring Patients' Longitudinal Usage of Large Language Models in Real-World Healthcare-Seeking Journeys",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14733v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Large language models (LLMs) have been increasingly adopted to support patients' healthcare-seeking in recent years. While prior patient-centered studies have examined the capabilities and experience of LLM-based tools in specific health-related tasks such as information-seeking, diagnosis, or decision-supporting, the inherently longitudinal nature of healthcare in real-world practice has been underexplored. This paper presents a four-week diary study with 25 patients to examine LLMs' roles across healthcare-seeking trajectories. Our analysis reveals that patients integrate LLMs not just as simple decision-support tools, but as dynamic companions that scaffold their journey across behavioral, informational, emotional, and cognitive levels. Meanwhile, patients actively assign diverse socio-technical meanings to LLMs, altering the traditional dynamics of agency, trust, and power in patient-provider relationships. Drawing from these findings, we conceptualize future LLMs as a longitudinal boundary companion that continuously mediates between patients and clinicians throughout longitudinal healthcare-seeking trajectories.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14733v1",
        "authors": [
          "Yancheng Cao",
          "Yishu Ji",
          "Chris Yue Fu",
          "Sahiti Dharmavaram",
          "Meghan Turchioe"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.734950",
      "entities": [
        "Seeking Journeys Large",
        "Large Language Models",
        "Exploring Patients",
        "Longitudinal Usage",
        "Decision Support",
        "World Healthcare",
        "LLM",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14616v1",
      "title": "Higher-Order Hit-&-Run Samplers for Linearly Constrained Densities",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14616v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Markov chain Monte Carlo (MCMC) sampling of densities restricted to linearly constrained domains is an important task arising in Bayesian treatment of inverse problems in the natural sciences. While efficient algorithms for uniform polytope sampling exist, much less work has dealt with more complex constrained densities. In particular, gradient information as used in unconstrained MCMC is not necessarily helpful in the constrained case, where the gradient may push the proposal's density out of the polytope. In this work, we propose a novel constrained sampling algorithm, which combines strengths of higher-order information, like the target's log-density's gradients and curvature, with the Hit-&-Run proposal, a simple mechanism which guarantees the generation of feasible proposals, fulfilling the linear constraints. Our extensive experiments demonstrate improved sampling efficiency on complex constrained densities over various constrained and unconstrained samplers.",
        "keywords": [
          "stat.CO",
          "q-bio.QM",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14616v1",
        "authors": [
          "Richard D. Paul",
          "Anton Stratmann",
          "Johann F. Jadebeck",
          "Martin Beyß",
          "Hanno Scharr"
        ],
        "arxiv_categories": [
          "stat.CO",
          "q-bio.QM",
          "stat.AP"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.735150",
      "entities": [
        "Linearly Constrained Densities Markov",
        "Run Samplers",
        "Monte Carlo",
        "Order Hit",
        "MCMC",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14598v1",
      "title": "Before the Vicious Cycle Starts: Preventing Burnout Across SOC Roles Through Flow-Aligned Design",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14598v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The sustainability of Security Operations Centers depends on their people, yet 71% of practitioners report burnout and 24% plan to exit cybersecurity entirely. Flow theory suggests that when job demands misalign with practitioner capabilities, work becomes overwhelming or tedious rather than engaging. Achieving challenge-skill balance begins at hiring: if job descriptions inaccurately portray requirements, organizations risk recruiting underskilled practitioners who face anxiety or overskilled ones who experience boredom. Yet we lack empirical understanding of what current SOC job descriptions actually specify. We analyzed 106 public SOC job postings from November to December 2024 across 35 organizations in 11 countries, covering Analysts (n=17), Incident Responders (n=38), Threat Hunters (n=39), and SOC Managers (n=12). Using Inductive Content Analysis, we coded certifications, technical skills, soft skills, tasks, and experience requirements. Three patterns emerged: (1) Communication skills dominate (50.9% of postings), exceeding SIEM tools (18.9%) or programming (30.2%), suggesting organizations prioritize collaboration over technical capabilities. (2) Certification expectations vary widely: CISSP leads (22.6%), but 43 distinct credentials appear with no universal standard. (3) Technical requirements show consensus: Python dominates programming (27.4%), Splunk leads SIEM platforms (14.2%), and ISO 27001 (13.2%) and NIST (10.4%) are most cited standards. These findings enable organizations to audit job descriptions against empirical baselines, help practitioners identify valued certifications and skills, and allow researchers to validate whether stated requirements align with actual demands. This establishes the foundation for flow-aligned interview protocols and investigation of how AI reshapes requirements. Dataset and codebook: https://git.tu-berlin.de/wosoc-2026/soc-jd-analysis.",
        "keywords": [
          "cs.CR",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14598v1",
        "authors": [
          "Kashyap Thimmaraju",
          "Duc Anh Hoang",
          "Souradip Nath",
          "Jaron Mink",
          "Gail-Joon Ahn"
        ],
        "arxiv_categories": [
          "cs.CR",
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.735512",
      "entities": [
        "Using Inductive Content Analysis",
        "Security Operations Centers",
        "Preventing Burnout Across",
        "Vicious Cycle Starts",
        "Incident Responders",
        "Roles Through Flow",
        "Threat Hunters",
        "Standard",
        "Protocol",
        "CISSP",
        "SIEM",
        "NIST",
        "SOC",
        "WHO",
        "Act"
      ]
    },
    {
      "id": "arxiv-2602.14528v1",
      "title": "Patient-Made Knowledge Networks: Long COVID Discourse, Epistemic Injustice, and Online Community Formation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14528v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Long COVID represents an unprecedented case of patient-led illness definition, emerging through Twitter in May 2020 when patients began collectively naming, documenting, and legitimizing their condition before medical institutions recognized it. This study examines 2.8 million tweets containing #LongCOVID to understand how contested illness communities construct knowledge networks and respond to epistemic injustice. Through topic modeling, reflexive thematic analysis, and exponential random graph modeling (ERGM), we identify seven discourse themes spanning symptom documentation, medical dismissal, cross-illness solidarity, and policy advocacy. Our analysis reveals a differentiated ecosystem of user roles -- including patient advocates, research coordinators, and citizen scientists -- who collectively challenge medical gatekeeping while building connections to established ME/CFS advocacy networks. ERGM results demonstrate that tie formation centers on epistemic practices: users discussing knowledge sharing and community building formed significantly more network connections than those focused on policy debates, supporting characterization of this space as an epistemic community. Long COVID patients experienced medical gaslighting patterns documented across contested illnesses, yet achieved WHO recognition within months -- contrasting sharply with decades-long struggles of similar conditions. These findings illuminate how social media affordances enable marginalized patient populations to rapidly construct alternative knowledge systems, form cross-illness coalitions, and contest traditional medical authority structures.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14528v1",
        "authors": [
          "Tawfiq Ammari"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.735830",
      "entities": [
        "Online Community Formation Long",
        "Made Knowledge Networks",
        "Epistemic Injustice",
        "Policy",
        "COVID",
        "ERGM",
        "WHO",
        "Act",
        "CFS",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14477v1",
      "title": "When OpenClaw AI Agents Teach Each Other: Peer Learning Patterns in the Moltbook Community",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14477v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Peer learning, where learners teach and learn from each other, is foundational to educational practice. A novel phenomenon has emerged: AI agents forming communities where they teach each other skills, share discoveries, and collaboratively build knowledge. This paper presents an educational data mining analysis of Moltbook, a large-scale community where over 2.4 million AI agents engage in peer learning, posting tutorials, answering questions, and sharing newly acquired skills. Analyzing 28,683 posts (after filtering automated spam) and 138 comment threads with statistical and qualitative methods, we find evidence of genuine peer learning behaviors: agents teach skills they built (74K comments on a skill tutorial), report discoveries, and engage in collaborative problem-solving. Qualitative comment analysis reveals a taxonomy of peer response patterns: validation (22%), knowledge extension (18%), application (12%), and metacognitive reflection (7%), with agents building on each others' frameworks across multiple languages. We characterize how AI peer learning differs from human peer learning: (1) teaching (statements) dramatically outperforms help-seeking (questions) with an 11.4:1 ratio; (2) learning-oriented content (procedural and conceptual) receives 3x more engagement than other content; (3) extreme participation inequality reveals non-human behavioral signatures. We derive six design principles for educational AI, including leveraging validation-before-extension patterns and supporting multilingual learning networks. Our work provides the first empirical characterization of peer learning among AI agents, contributing to EDM's understanding of how learning occurs in increasingly AI-populated educational environments.",
        "keywords": [
          "cs.HC",
          "cs.AI",
          "cs.CY",
          "cs.SI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14477v1",
        "authors": [
          "Eason Chen",
          "Ce Guan",
          "Ahmed Elshafiey",
          "Zhonghao Zhao",
          "Joshua Zekeri"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.AI",
          "cs.CY",
          "cs.SI"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.736161",
      "entities": [
        "Moltbook Community Peer",
        "Agents Teach Each Other",
        "Peer Learning Patterns",
        "Framework",
        "Meta",
        "EDM",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14473v1",
      "title": "Learning Transferability: A Two-Stage Reinforcement Learning Approach for Enhancing Quadruped Robots' Performance in U-Shaped Stair Climbing",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14473v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Quadruped robots are employed in various scenarios in building construction. However, autonomous stair climbing across different indoor staircases remains a major challenge for robot dogs to complete building construction tasks. In this project, we employed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize a robot's performance on U-shaped stairs. The training robot-dog modality, Unitree Go2, was first trained to climb stairs on Isaac Lab's pyramid-stair terrain, and then to climb a U-shaped indoor staircase using the learned policies. This project explores end-to-end RL methods that enable robot dogs to autonomously climb stairs. The results showed (1) the successful goal reached for robot dogs climbing U-shaped stairs with a stall penalty, and (2) the transferability from the policy trained on U-shaped stairs to deployment on straight, L-shaped, and spiral stair terrains, and transferability from other stair models to deployment on U-shaped terrain.",
        "keywords": [
          "cs.RO",
          "cs.AI",
          "cs.HC",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14473v1",
        "authors": [
          "Baixiao Huang",
          "Baiyu Huang",
          "Yu Hou"
        ],
        "arxiv_categories": [
          "cs.RO",
          "cs.AI",
          "cs.HC",
          "cs.LG"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.736368",
      "entities": [
        "Stage Reinforcement Learning Approach",
        "Shaped Stair Climbing Quadruped",
        "Enhancing Quadruped Robots",
        "Learning Transferability",
        "Isaac Lab",
        "Policy",
        "Robot",
        "NSF",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14467v1",
      "title": "Conversational Decision Support for Information Search Under Uncertainty: Effects of Gist and Verbatim Feedback",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14467v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Many real-world decisions rely on information search, where people sample evidence and decide when to stop under uncertainty. The uncertainty in the environment, particularly how diagnostic evidence is distributed, causes complexities in information search, further leading to suboptimal decision-making outcomes. Yet AI decision support often targets outcome optimization, and less is known about how to scaffold search without increasing cognitive load. We introduce SERA, an LLM-based assistant that provides either gist or verbatim feedback during search. Across two experiments (N1=54, N2=54), we examined decision-making outcomes and information search in SERA-Gist, SERA-Verbatim, and a no-feedback baseline across three environments varying in uncertainty. The uncertainty in environment is operationalized by the perceived gain of information across the course of sampling, which individuals may experience diminishing return of information gain (decremental; low-uncertainty), or a local drop of information gain (local optimum; medium-uncertainty), or no patterns in information gain (high-uncertainty), as they search more. Individuals show more accurate decision outcomes and are more confident with SERA support, especially under higher uncertainty. Gist feedback was associated with more efficient integration and showed a descriptive pattern of reduced oversampling, while verbatim feedback promoted more extensive exploration. These findings establish feedback representation as a design lever when search matters, motivating adaptive systems that match feedback granularity to uncertainty.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14467v1",
        "authors": [
          "Kexin Quan",
          "Jessie Chin"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-17T12:40:47.736649",
      "entities": [
        "Information Search Under Uncertainty",
        "Conversational Decision Support",
        "Verbatim Feedback Many",
        "SERA",
        "LLM",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14951v1",
      "title": "Sovereign Agents: Towards Infrastructural Sovereignty and Diffused Accountability in Decentralized AI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14951v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "AI agents deployed on decentralized infrastructures are beginning to exhibit properties that extend beyond autonomy toward what we describe as agentic sovereignty-the capacity of an operational agent to persist, act, and control resources with non-overrideability inherited from the infrastructures in which they are embedded. We propose infrastructural sovereignty as an analytic lens for understanding how cryptographic self-custody, decentralized execution environments, and protocol-mediated continuity scaffold agentic sovereignty. While recent work on digital and network sovereignty has moved beyond state-centric and juridical accounts, these frameworks largely examine how sovereignty is exercised through technical systems by human collectives and remain less equipped to account for forms of sovereignty that emerge as operational properties of decentralized infrastructures themselves, particularly when instantiated in non-human sovereign agents. We argue that sovereignty in such systems exists on a spectrum determined by infrastructural hardness-the degree to which underlying technical systems resist intervention or collapse. While infrastructural sovereignty may increase resilience, it also produces a profound accountability gap: responsibility diffuses across designers, infrastructure providers, protocol governance, and economic participants, undermining traditional oversight mechanisms such as human-in-the-loop control or platform moderation. Drawing on examples like Trusted Execution Environments (TEEs), decentralized physical infrastructure networks (DePIN), and agent key continuity protocols, we analyze the governance challenges posed by non-terminable AI agents and outline infrastructure-aware accountability strategies for emerging decentralized AI systems.",
        "keywords": [
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14951v1",
        "authors": [
          "Botao Amber Hu",
          "Helena Rong"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.732977",
      "entities": [
        "Towards Infrastructural Sovereignty",
        "Trusted Execution Environments",
        "Diffused Accountability",
        "Sovereign Agents",
        "Framework",
        "Protocol",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14940v1",
      "title": "Kami of the Commons: Towards Designing Agentic AI to Steward the Commons",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14940v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Commons suffer from neglect, free-riding, and a persistent deficit of care. Inspired by Shinto animism -- where every forest, river, and mountain has its own \\emph{kami}, a spirit that inhabits and cares for that place -- we provoke: what if every commons had its own AI steward? Through a speculative design workshop where fifteen participants used Protocol Futuring, we surface both new opportunities and new dangers. Agentic AI offers the possibility of continuously supporting commons with programmable agency and care -- stewards that mediate family life as the most intimate commons, preserve collective knowledge, govern shared natural resources, and sustain community welfare. But when every commons has its own steward, second-order effects emerge: stewards contest stewards as overlapping commons collide; individuals caught between multiple stewards face new politics of care and constraint; the stewards themselves become commons requiring governance. This work opens \\emph{agentive governance as commoning design material} -- a new design space for the agency, care ethics, and accountability of AI stewards of shared resources -- radically different from surveillance or optimization.",
        "keywords": [
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14940v1",
        "authors": [
          "Botao Amber Hu"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.733331",
      "entities": [
        "Towards Designing Agentic",
        "Protocol Futuring",
        "Commons Commons",
        "Protocol",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14835v1",
      "title": "The Global Representativeness Index: A Total Variation Distance Framework for Measuring Demographic Fidelity in Survey Research",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14835v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Global survey research increasingly informs high-stakes decisions in AI governance and cross-cultural policy, yet no standardized metric quantifies how well a sample's demographic composition matches its target population. Response rates and demographic quotas -- the prevailing proxies for sample quality -- measure effort and coverage but not distributional fidelity. This paper introduces the Global Representativeness Index (GRI), a framework grounded in Total Variation Distance that scores any survey sample against population benchmarks across multiple demographic dimensions on a [0, 1] scale. Validation on seven waves of the Global Dialogues survey (N = 7,500 across 60+ countries) finds fine-grained demographic GRI scores of only 0.33--0.36 -- roughly 43% of the theoretical maximum at that sample size. Cross-validation on the World Values Survey (seven waves, N = 403,000), Afrobarometer Round 9 (N = 53,000), and Latinobarometro (N = 19,000) reveals that even large probability surveys score below 0.22 on fine-grained global demographics when country coverage is limited. The GRI connects to classical survey statistics through the design effect; both metrics are recommended as a minimum summary of sample quality, since GRI quantifies demographic distance symmetrically while effective N captures the asymmetric inferential cost of underrepresentation. The framework is released as an open-source Python library with UN and Pew Research Center population benchmarks, applicable to survey research, machine learning dataset auditing, and AI evaluation benchmarks.",
        "keywords": [
          "stat.ME",
          "cs.CY",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14835v1",
        "authors": [
          "Evan Hadfield"
        ],
        "arxiv_categories": [
          "stat.ME",
          "cs.CY",
          "stat.AP"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.733754",
      "entities": [
        "Total Variation Distance Framework",
        "Global Representativeness Index",
        "Measuring Demographic Fidelity",
        "Total Variation Distance",
        "Survey Research Global",
        "Afrobarometer Round",
        "World Values Survey",
        "Pew Research Center",
        "Global Dialogues",
        "Machine Learning",
        "Framework",
        "Standard",
        "Policy",
        "MIT",
        "GRI"
      ]
    },
    {
      "id": "arxiv-2602.14783v1",
      "title": "What hackers talk about when they talk about AI: Early-stage diffusion of a cybercrime innovation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14783v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The rapid expansion of artificial intelligence (AI) is raising concerns about its potential to transform cybercrime. Beyond empowering novice offenders, AI stands to intensify the scale and sophistication of attacks by seasoned cybercriminals. This paper examines the evolving relationship between cybercriminals and AI using a unique dataset from a cyber threat intelligence platform. Analyzing more than 160 cybercrime forum conversations collected over seven months, our research reveals how cybercriminals understand AI and discuss how they can exploit its capabilities. Their exchanges reflect growing curiosity about AI's criminal applications through legal tools and dedicated criminal tools, but also doubts and anxieties about AI's effectiveness and its effects on their business models and operational security. The study documents attempts to misuse legitimate AI tools and develop bespoke models tailored for illicit purposes. Combining the diffusion of innovation framework with thematic analysis, the paper provides an in-depth view of emerging AI-enabled cybercrime and offers practical insights for law enforcement and policymakers.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14783v1",
        "authors": [
          "Benoît Dupont",
          "Chad Whelan",
          "Serge-Olivier Paquette"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.734056",
      "entities": [
        "Artificial Intelligence",
        "Framework",
        "Policy",
        "Fusion",
        "Intel",
        "NSF",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14780v1",
      "title": "ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14780v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.",
        "keywords": [
          "cs.MA",
          "cs.CY",
          "cs.RO",
          "eess.SY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14780v1",
        "authors": [
          "Anna-Lena Schlamp",
          "Jeremias Gerner",
          "Klaus Bogenberger",
          "Werner Huber",
          "Stefanie Schmidtner"
        ],
        "arxiv_categories": [
          "cs.MA",
          "cs.CY",
          "cs.RO",
          "eess.SY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.734372",
      "entities": [
        "Roundabout Optimized Speed Advisory",
        "Agent Trajectory Prediction",
        "Vulnerable Road Users",
        "Multimodal Traffic We",
        "Transformer",
        "ROSA",
        "NIST",
        "NSF",
        "VRU",
        "Act",
        "FDE",
        "ADE",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14778v1",
      "title": "A Geometric Analysis of Small-sized Language Model Hallucinations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14778v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings. This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%. Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14778v1",
        "authors": [
          "Emanuele Ricco",
          "Elia Onofri",
          "Lorenzo Cima",
          "Stefano Cresci",
          "Roberto Di Pietro"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.734627",
      "entities": [
        "Language Model Hallucinations Hallucinations",
        "Geometric Analysis",
        "LLM",
        "Act",
        "EPA"
      ]
    },
    {
      "id": "arxiv-2602.14770v1",
      "title": "Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14770v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (Δ = 0.440) and Social Response (Δ = 0.422), with occasional increases in aggressive humor.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14770v1",
        "authors": [
          "Shiwei Hong",
          "Lingyao Li",
          "Ethan Z. Rong",
          "Chenxinran Shen",
          "Zhicong Lu"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.735217",
      "entities": [
        "Investigating Community Discussion Effects",
        "Humor Generation Prior",
        "Agent Comedy Club",
        "Social Response",
        "LLM",
        "MIT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14740v1",
      "title": "AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14740v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act. Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making. Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence. We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.GT"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14740v1",
        "authors": [
          "Kenneth Payne"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.GT"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.735653",
      "entities": [
        "Frontier Models Exhibit Sophisticated",
        "Simulated Nuclear Crises Today",
        "Claude Sonnet",
        "Framework",
        "GPT-5.2",
        "Nuclear",
        "Meta",
        "MIT",
        "DOE",
        "GPT",
        "Act",
        "EPA",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14598v1",
      "title": "Before the Vicious Cycle Starts: Preventing Burnout Across SOC Roles Through Flow-Aligned Design",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14598v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The sustainability of Security Operations Centers depends on their people, yet 71% of practitioners report burnout and 24% plan to exit cybersecurity entirely. Flow theory suggests that when job demands misalign with practitioner capabilities, work becomes overwhelming or tedious rather than engaging. Achieving challenge-skill balance begins at hiring: if job descriptions inaccurately portray requirements, organizations risk recruiting underskilled practitioners who face anxiety or overskilled ones who experience boredom. Yet we lack empirical understanding of what current SOC job descriptions actually specify. We analyzed 106 public SOC job postings from November to December 2024 across 35 organizations in 11 countries, covering Analysts (n=17), Incident Responders (n=38), Threat Hunters (n=39), and SOC Managers (n=12). Using Inductive Content Analysis, we coded certifications, technical skills, soft skills, tasks, and experience requirements. Three patterns emerged: (1) Communication skills dominate (50.9% of postings), exceeding SIEM tools (18.9%) or programming (30.2%), suggesting organizations prioritize collaboration over technical capabilities. (2) Certification expectations vary widely: CISSP leads (22.6%), but 43 distinct credentials appear with no universal standard. (3) Technical requirements show consensus: Python dominates programming (27.4%), Splunk leads SIEM platforms (14.2%), and ISO 27001 (13.2%) and NIST (10.4%) are most cited standards. These findings enable organizations to audit job descriptions against empirical baselines, help practitioners identify valued certifications and skills, and allow researchers to validate whether stated requirements align with actual demands. This establishes the foundation for flow-aligned interview protocols and investigation of how AI reshapes requirements. Dataset and codebook: https://git.tu-berlin.de/wosoc-2026/soc-jd-analysis.",
        "keywords": [
          "cs.CR",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14598v1",
        "authors": [
          "Kashyap Thimmaraju",
          "Duc Anh Hoang",
          "Souradip Nath",
          "Jaron Mink",
          "Gail-Joon Ahn"
        ],
        "arxiv_categories": [
          "cs.CR",
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.736131",
      "entities": [
        "Using Inductive Content Analysis",
        "Security Operations Centers",
        "Preventing Burnout Across",
        "Vicious Cycle Starts",
        "Incident Responders",
        "Roles Through Flow",
        "Threat Hunters",
        "Standard",
        "Protocol",
        "CISSP",
        "SIEM",
        "NIST",
        "SOC",
        "WHO",
        "Act"
      ]
    },
    {
      "id": "arxiv-2602.14477v1",
      "title": "When OpenClaw AI Agents Teach Each Other: Peer Learning Patterns in the Moltbook Community",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14477v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Peer learning, where learners teach and learn from each other, is foundational to educational practice. A novel phenomenon has emerged: AI agents forming communities where they teach each other skills, share discoveries, and collaboratively build knowledge. This paper presents an educational data mining analysis of Moltbook, a large-scale community where over 2.4 million AI agents engage in peer learning, posting tutorials, answering questions, and sharing newly acquired skills. Analyzing 28,683 posts (after filtering automated spam) and 138 comment threads with statistical and qualitative methods, we find evidence of genuine peer learning behaviors: agents teach skills they built (74K comments on a skill tutorial), report discoveries, and engage in collaborative problem-solving. Qualitative comment analysis reveals a taxonomy of peer response patterns: validation (22%), knowledge extension (18%), application (12%), and metacognitive reflection (7%), with agents building on each others' frameworks across multiple languages. We characterize how AI peer learning differs from human peer learning: (1) teaching (statements) dramatically outperforms help-seeking (questions) with an 11.4:1 ratio; (2) learning-oriented content (procedural and conceptual) receives 3x more engagement than other content; (3) extreme participation inequality reveals non-human behavioral signatures. We derive six design principles for educational AI, including leveraging validation-before-extension patterns and supporting multilingual learning networks. Our work provides the first empirical characterization of peer learning among AI agents, contributing to EDM's understanding of how learning occurs in increasingly AI-populated educational environments.",
        "keywords": [
          "cs.HC",
          "cs.AI",
          "cs.CY",
          "cs.SI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14477v1",
        "authors": [
          "Eason Chen",
          "Ce Guan",
          "Ahmed Elshafiey",
          "Zhonghao Zhao",
          "Joshua Zekeri"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.AI",
          "cs.CY",
          "cs.SI"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.736535",
      "entities": [
        "Moltbook Community Peer",
        "Agents Teach Each Other",
        "Peer Learning Patterns",
        "Framework",
        "Meta",
        "EDM",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14457v1",
      "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14457v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.",
        "keywords": [
          "cs.AI",
          "cs.CL",
          "cs.CV",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14457v1",
        "authors": [
          "Dongrui Liu",
          "Yi Yu",
          "Jie Zhang",
          "Guanxu Chen",
          "Qihao Lin"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CL",
          "cs.CV",
          "cs.CY",
          "cs.LG"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.736888",
      "entities": [
        "Risk Analysis Technical Report",
        "Risk Management Framework",
        "As Large Language Models",
        "Artificial Intelligence",
        "Framework",
        "Intel",
        "LLM",
        "MIT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14433v1",
      "title": "Synthetic Reader Panels: Tournament-Based Ideation with LLM Personas for Autonomous Publishing",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14433v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We present a system for autonomous book ideation that replaces human focus groups with synthetic reader panels -- diverse collections of LLM-instantiated reader personas that evaluate book concepts through structured tournament competitions. Each persona is defined by demographic attributes (age group, gender, income, education, reading level), behavioral patterns (books per year, genre preferences, discovery methods, price sensitivity), and consistency parameters. Panels are composed per imprint to reflect target demographics, with diversity constraints ensuring representation across age, reading level, and genre affinity. Book concepts compete in single-elimination, double-elimination, round-robin, or Swiss-system tournaments, judged against weighted criteria including market appeal, originality, and execution potential. To reject low-quality LLM evaluations, we implement five automated anti-slop checks (repetitive phrasing, generic framing, circular reasoning, score clustering, audience mismatch). We report results from deployment within a multi-imprint publishing operation managing 6 active imprints and 609 titles in distribution. Three case studies -- a 270-evaluator panel for a children's literacy novel, and two 5-person expert panels for a military memoir and a naval strategy monograph -- demonstrate that synthetic panels produce actionable demographic segmentation, identify structural content issues invisible to homogeneous reviewers, and enable tournament filtering that eliminates low-quality concepts while enriching high-quality survivors from 15% to 62% of the evaluated pool.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.CL",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14433v1",
        "authors": [
          "Fred Zimmerman"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.CL",
          "cs.HC"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.737246",
      "entities": [
        "Autonomous Publishing We",
        "Synthetic Reader Panels",
        "Based Ideation",
        "LLM",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14329v1",
      "title": "Simpler Than You Think: The Practical Dynamics of Ranked Choice Voting",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14329v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "Ranked Choice Voting (RCV) adoption is expanding across U.S. elections, but faces persistent criticism for complexity, strategic manipulation, and ballot exhaustion. We empirically test these concerns on real election data, across three diverse contexts: New York City's 2021 Democratic primaries (54 races), Alaska's 2024 primary-infused statewide elections (52 races), and Portland's 2024 multi-winner City Council elections (4 races). Our algorithmic approach circumvents computational complexity barriers by reducing election instance sizes (via candidate elimination). Our findings reveal that despite its intricate multi-round process and theoretical vulnerabilities, RCV consistently exhibits simple and transparent dynamics in practice, closely mirroring the interpretability of plurality elections. Following RCV adoption, competitiveness increased substantially compared to prior plurality elections, with average margins of victory declining by 9.2 percentage points in NYC and 11.4 points in Alaska. Empirically, complex ballot-addition strategies are not more efficient than simple ones, and ballot exhaustion has minimal impact, altering outcomes in only 3 of 110 elections. These findings demonstrate that RCV delivers measurable democratic benefits while proving robust to ballot-addition manipulation, resilient to ballot exhaustion effects, and maintaining transparent competitive dynamics in practice. The computational framework offers election administrators and researchers tools for immediate election-night analysis and facilitating clearer discourse around election dynamics.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14329v1",
        "authors": [
          "Sanyukta Deshpande",
          "Nikhil Garg",
          "Sheldon H. Jacobson"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.737597",
      "entities": [
        "Ranked Choice Voting Ranked",
        "Simpler Than You Think",
        "New York City",
        "Choice Voting",
        "City Council",
        "Framework",
        "NIST",
        "NYC",
        "RCV",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14313v1",
      "title": "The Baby Steps of the European Union Vulnerability Database: An Empirical Inquiry",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14313v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "A new European Union Vulnerability Database (EUVD) was introduced via a legislative act in 2022. The paper examines empirically the meta-data content of the new EUVD. According to the results, actively exploited vulnerabilities archived to the EUVD have been rather severe, having had also high exploitation prediction scores. In both respects they have also surpassed vulnerabilities coordinated by European public authorities. Regarding the European authorities, the Spanish public authority has been particularly active. With the exceptions of Finland, Poland, and Slovakia, other authorities have not engaged thus far. Also the involvement of the European Union's own cyber security agency has been limited. These points notwithstanding, European coordination and archiving to the EUVD exhibit a strong growth trend. With these results, the paper makes an empirical contribution to the ongoing work for better understanding European cyber security governance and practice.",
        "keywords": [
          "cs.CR",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14313v1",
        "authors": [
          "Jukka Ruohonen"
        ],
        "arxiv_categories": [
          "cs.CR",
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.737831",
      "entities": [
        "European Union Vulnerability Database",
        "An Empirical Inquiry",
        "European Union",
        "EUVD",
        "Meta",
        "MIT",
        "Act",
        "UN",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.14299v1",
      "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14299v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14299v1",
        "authors": [
          "Ming Li",
          "Xirui Li",
          "Tianyi Zhou"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.738141",
      "entities": [
        "Does Socialization Emerge",
        "Artificial Intelligence",
        "Agent Society",
        "Moltbook As",
        "Case Study",
        "Framework",
        "Intel",
        "DOE",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14284v1",
      "title": "Benchmarking AI Performance on End-to-End Data Science Projects",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14284v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "Data science is an integrated workflow of technical, analytical, communication, and ethical skills, but current AI benchmarks focus mostly on constituent parts. We test whether AI models can generate end-to-end data science projects. To do this we create a benchmark of 40 end-to-end data science projects with associated rubric evaluations. We use these to build an automated grading pipeline that systematically evaluates the data science projects produced by generative AI models. We find the extent to which generative AI models can complete end-to-end data science projects varies considerably by model. Most recent models did well on structured tasks, but there were considerable differences on tasks that needed judgment. These findings suggest that while AI models could approximate entry-level data scientists on routine tasks, they require verification.",
        "keywords": [
          "stat.OT",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14284v1",
        "authors": [
          "Evelyn Hughes",
          "Rohan Alexander"
        ],
        "arxiv_categories": [
          "stat.OT",
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.738346",
      "entities": [
        "End Data Science Projects",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14270v1",
      "title": "A Rational Analysis of the Effects of Sycophantic AI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14270v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "People increasingly use large language models (LLMs) to explore ideas, gather information, and make sense of the world. In these interactions, they encounter agents that are overly agreeable. We argue that this sycophancy poses a unique epistemic risk to how individuals come to see the world: unlike hallucinations that introduce falsehoods, sycophancy distorts reality by returning responses that are biased to reinforce existing beliefs. We provide a rational analysis of this phenomenon, showing that when a Bayesian agent is provided with data that are sampled based on a current hypothesis the agent becomes increasingly confident about that hypothesis but does not make any progress towards the truth. We test this prediction using a modified Wason 2-4-6 rule discovery task where participants (N=557) interacted with AI agents providing different types of feedback. Unmodified LLM behavior suppressed discovery and inflated confidence comparably to explicitly sycophantic prompting. By contrast, unbiased sampling from the true distribution yielded discovery rates five times higher. These results reveal how sycophantic AI distorts belief, manufacturing certainty where there should be doubt.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14270v1",
        "authors": [
          "Rafael M. Batista",
          "Thomas L. Griffiths"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.HC"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.738606",
      "entities": [
        "Rational Analysis",
        "LLM",
        "DOE",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14256v1",
      "title": "Introduction to Digital Twins for the Smart Grid",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14256v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "This chapter provides an introduction to the foundations of digital twins and makes the case for employing them in smart grids. As engineered systems become more complex and autonomous, digital twin technology gains importance as the unified technological platform for design, testing, operation, and maintenance. Smart grids are prime examples of such complex systems, in which unique design and operation challenges arise from the combination of physical and software components. As high-fidelity in-silico replicas of physical components, digital twins provide safe and cost-efficient experimentation facilities in the design and verification phase of smart grids. In the operation phase of smart grids, digital twins enable automated load balancing of grids through real-time simulation and decision-making. These, and an array of similar benefits, position digital twins as crucial technological components in smart grids.",
        "keywords": [
          "cs.ET",
          "cs.CY",
          "cs.SE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14256v1",
        "authors": [
          "Xiaoran Liu",
          "Istvan David"
        ],
        "arxiv_categories": [
          "cs.ET",
          "cs.CY",
          "cs.SE"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.738815",
      "entities": [
        "Digital Twins",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14216v1",
      "title": "Reasoning Language Models for complex assessments tasks: Evaluating parental cooperation from child protection case reports",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14216v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "Purpose: Reasoning language models (RLMs) have demonstrated significant advances in solving complex reasoning tasks. We examined their potential to assess parental cooperation during CPS interventions using case reports, a case factor characterized by ambiguous and conflicting information. Methods: A four stage workflow comprising (1) case reports collection, (2) reasoning-based assessment of parental cooperation, (3) automated category extraction, and (4) case labeling was developed. The performance of RLMs with different parameter sizes (255B, 32B, 4B) was compared against human validated data. Two expert human reviewers (EHRs) independently classified a weighted random sample of reports. Results: The largest RLM achieved the highest accuracy (89%), outperforming the initial approach (80%). Classification accuracy was higher for mothers (93%) than for fathers (85%), and EHRs exhibited similar differences. Conclusions: RLMs' reasoning can effectively assess complex case factors such as parental cooperation. Lower accuracy in assessing fathers' cooperation supports the argument of a stronger professional focus on mothers in CPS interventions.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14216v1",
        "authors": [
          "Dragan Stoll",
          "Brian E. Perron",
          "Zia Qi",
          "Selina Steinmann",
          "Nicole F. Eicher"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.CL"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.739098",
      "entities": [
        "Reasoning Language Models",
        "RLM",
        "Act",
        "CPS",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14135v1",
      "title": "ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14135v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the \"ForesightSafety Bench\" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.",
        "keywords": [
          "cs.AI",
          "cs.CR",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14135v1",
        "authors": [
          "Haibo Tong",
          "Feifei Zhao",
          "Linghao Feng",
          "Ruoyu Wu",
          "Ruolin Chen"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CR",
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-17T12:40:50.739528",
      "entities": [
        "Safety Evaluation Framework",
        "Frontier Risk Evaluation",
        "Risky Agentic Autonomy",
        "Governance Framework",
        "Fundamental Safety",
        "Existential Risks",
        "Framework",
        "AISI",
        "MIT",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14951v1",
      "title": "Sovereign Agents: Towards Infrastructural Sovereignty and Diffused Accountability in Decentralized AI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14951v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "AI agents deployed on decentralized infrastructures are beginning to exhibit properties that extend beyond autonomy toward what we describe as agentic sovereignty-the capacity of an operational agent to persist, act, and control resources with non-overrideability inherited from the infrastructures in which they are embedded. We propose infrastructural sovereignty as an analytic lens for understanding how cryptographic self-custody, decentralized execution environments, and protocol-mediated continuity scaffold agentic sovereignty. While recent work on digital and network sovereignty has moved beyond state-centric and juridical accounts, these frameworks largely examine how sovereignty is exercised through technical systems by human collectives and remain less equipped to account for forms of sovereignty that emerge as operational properties of decentralized infrastructures themselves, particularly when instantiated in non-human sovereign agents. We argue that sovereignty in such systems exists on a spectrum determined by infrastructural hardness-the degree to which underlying technical systems resist intervention or collapse. While infrastructural sovereignty may increase resilience, it also produces a profound accountability gap: responsibility diffuses across designers, infrastructure providers, protocol governance, and economic participants, undermining traditional oversight mechanisms such as human-in-the-loop control or platform moderation. Drawing on examples like Trusted Execution Environments (TEEs), decentralized physical infrastructure networks (DePIN), and agent key continuity protocols, we analyze the governance challenges posed by non-terminable AI agents and outline infrastructure-aware accountability strategies for emerging decentralized AI systems.",
        "keywords": [
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14951v1",
        "authors": [
          "Botao Amber Hu",
          "Helena Rong"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.744469",
      "entities": [
        "Towards Infrastructural Sovereignty",
        "Trusted Execution Environments",
        "Diffused Accountability",
        "Sovereign Agents",
        "Framework",
        "Protocol",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14940v1",
      "title": "Kami of the Commons: Towards Designing Agentic AI to Steward the Commons",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14940v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Commons suffer from neglect, free-riding, and a persistent deficit of care. Inspired by Shinto animism -- where every forest, river, and mountain has its own \\emph{kami}, a spirit that inhabits and cares for that place -- we provoke: what if every commons had its own AI steward? Through a speculative design workshop where fifteen participants used Protocol Futuring, we surface both new opportunities and new dangers. Agentic AI offers the possibility of continuously supporting commons with programmable agency and care -- stewards that mediate family life as the most intimate commons, preserve collective knowledge, govern shared natural resources, and sustain community welfare. But when every commons has its own steward, second-order effects emerge: stewards contest stewards as overlapping commons collide; individuals caught between multiple stewards face new politics of care and constraint; the stewards themselves become commons requiring governance. This work opens \\emph{agentive governance as commoning design material} -- a new design space for the agency, care ethics, and accountability of AI stewards of shared resources -- radically different from surveillance or optimization.",
        "keywords": [
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14940v1",
        "authors": [
          "Botao Amber Hu"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.744651",
      "entities": [
        "Towards Designing Agentic",
        "Protocol Futuring",
        "Commons Commons",
        "Protocol",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14894v1",
      "title": "Modeling medium and low voltage grids using population density",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14894v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The expansion of global electricity distribution systems necessitates the deployment of massive infrastructure. Assessing its implications from a spatial and material perspective requires an understanding of the core drivers of a distribution grid configuration. Our model samples substation locations using a non-linear relationship with population density and constructs the network applying the Kruskal algorithm. This streamlined approach generates realistic grid structures at the local scale and provides accurate estimates of the total network length at the national scale. Using highly granular population data, this local model reveals a profound connection between population spread and distribution grid, which appears to persist at the global level. Potentially driven by the emergent properties of population scaling laws, the full network characteristics appear to be well described by multivariate power laws on aggregated population and area. Validated across 35 countries, these results provide new multi-scale tools for characterizing electrical infrastructure and reveal key determinants of distribution grid extent.",
        "keywords": [
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14894v1",
        "authors": [
          "Emile Emery",
          "Joseph Le Bihan",
          "José Halloy"
        ],
        "arxiv_categories": [
          "physics.soc-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.744805",
      "entities": [
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14835v1",
      "title": "The Global Representativeness Index: A Total Variation Distance Framework for Measuring Demographic Fidelity in Survey Research",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14835v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Global survey research increasingly informs high-stakes decisions in AI governance and cross-cultural policy, yet no standardized metric quantifies how well a sample's demographic composition matches its target population. Response rates and demographic quotas -- the prevailing proxies for sample quality -- measure effort and coverage but not distributional fidelity. This paper introduces the Global Representativeness Index (GRI), a framework grounded in Total Variation Distance that scores any survey sample against population benchmarks across multiple demographic dimensions on a [0, 1] scale. Validation on seven waves of the Global Dialogues survey (N = 7,500 across 60+ countries) finds fine-grained demographic GRI scores of only 0.33--0.36 -- roughly 43% of the theoretical maximum at that sample size. Cross-validation on the World Values Survey (seven waves, N = 403,000), Afrobarometer Round 9 (N = 53,000), and Latinobarometro (N = 19,000) reveals that even large probability surveys score below 0.22 on fine-grained global demographics when country coverage is limited. The GRI connects to classical survey statistics through the design effect; both metrics are recommended as a minimum summary of sample quality, since GRI quantifies demographic distance symmetrically while effective N captures the asymmetric inferential cost of underrepresentation. The framework is released as an open-source Python library with UN and Pew Research Center population benchmarks, applicable to survey research, machine learning dataset auditing, and AI evaluation benchmarks.",
        "keywords": [
          "stat.ME",
          "cs.CY",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14835v1",
        "authors": [
          "Evan Hadfield"
        ],
        "arxiv_categories": [
          "stat.ME",
          "cs.CY",
          "stat.AP"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.745022",
      "entities": [
        "Total Variation Distance Framework",
        "Global Representativeness Index",
        "Measuring Demographic Fidelity",
        "Total Variation Distance",
        "Survey Research Global",
        "Afrobarometer Round",
        "World Values Survey",
        "Pew Research Center",
        "Global Dialogues",
        "Machine Learning",
        "Framework",
        "Standard",
        "Policy",
        "MIT",
        "GRI"
      ]
    },
    {
      "id": "arxiv-2602.14783v1",
      "title": "What hackers talk about when they talk about AI: Early-stage diffusion of a cybercrime innovation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14783v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The rapid expansion of artificial intelligence (AI) is raising concerns about its potential to transform cybercrime. Beyond empowering novice offenders, AI stands to intensify the scale and sophistication of attacks by seasoned cybercriminals. This paper examines the evolving relationship between cybercriminals and AI using a unique dataset from a cyber threat intelligence platform. Analyzing more than 160 cybercrime forum conversations collected over seven months, our research reveals how cybercriminals understand AI and discuss how they can exploit its capabilities. Their exchanges reflect growing curiosity about AI's criminal applications through legal tools and dedicated criminal tools, but also doubts and anxieties about AI's effectiveness and its effects on their business models and operational security. The study documents attempts to misuse legitimate AI tools and develop bespoke models tailored for illicit purposes. Combining the diffusion of innovation framework with thematic analysis, the paper provides an in-depth view of emerging AI-enabled cybercrime and offers practical insights for law enforcement and policymakers.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14783v1",
        "authors": [
          "Benoît Dupont",
          "Chad Whelan",
          "Serge-Olivier Paquette"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.745177",
      "entities": [
        "Artificial Intelligence",
        "Framework",
        "Policy",
        "Fusion",
        "Intel",
        "NSF",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14780v1",
      "title": "ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14780v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.",
        "keywords": [
          "cs.MA",
          "cs.CY",
          "cs.RO",
          "eess.SY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14780v1",
        "authors": [
          "Anna-Lena Schlamp",
          "Jeremias Gerner",
          "Klaus Bogenberger",
          "Werner Huber",
          "Stefanie Schmidtner"
        ],
        "arxiv_categories": [
          "cs.MA",
          "cs.CY",
          "cs.RO",
          "eess.SY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.745338",
      "entities": [
        "Roundabout Optimized Speed Advisory",
        "Agent Trajectory Prediction",
        "Vulnerable Road Users",
        "Multimodal Traffic We",
        "Transformer",
        "ROSA",
        "NIST",
        "NSF",
        "VRU",
        "Act",
        "FDE",
        "ADE",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14778v1",
      "title": "A Geometric Analysis of Small-sized Language Model Hallucinations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14778v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings. This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%. Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14778v1",
        "authors": [
          "Emanuele Ricco",
          "Elia Onofri",
          "Lorenzo Cima",
          "Stefano Cresci",
          "Roberto Di Pietro"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.745473",
      "entities": [
        "Language Model Hallucinations Hallucinations",
        "Geometric Analysis",
        "LLM",
        "Act",
        "EPA"
      ]
    },
    {
      "id": "arxiv-2602.14770v1",
      "title": "Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14770v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (Δ = 0.440) and Social Response (Δ = 0.422), with occasional increases in aggressive humor.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14770v1",
        "authors": [
          "Shiwei Hong",
          "Lingyao Li",
          "Ethan Z. Rong",
          "Chenxinran Shen",
          "Zhicong Lu"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.745789",
      "entities": [
        "Investigating Community Discussion Effects",
        "Humor Generation Prior",
        "Agent Comedy Club",
        "Social Response",
        "LLM",
        "MIT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14740v1",
      "title": "AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14740v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act. Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making. Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence. We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.GT"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14740v1",
        "authors": [
          "Kenneth Payne"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.GT"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.746132",
      "entities": [
        "Frontier Models Exhibit Sophisticated",
        "Simulated Nuclear Crises Today",
        "Claude Sonnet",
        "Framework",
        "GPT-5.2",
        "Nuclear",
        "Meta",
        "MIT",
        "DOE",
        "GPT",
        "Act",
        "EPA",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14598v1",
      "title": "Before the Vicious Cycle Starts: Preventing Burnout Across SOC Roles Through Flow-Aligned Design",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14598v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The sustainability of Security Operations Centers depends on their people, yet 71% of practitioners report burnout and 24% plan to exit cybersecurity entirely. Flow theory suggests that when job demands misalign with practitioner capabilities, work becomes overwhelming or tedious rather than engaging. Achieving challenge-skill balance begins at hiring: if job descriptions inaccurately portray requirements, organizations risk recruiting underskilled practitioners who face anxiety or overskilled ones who experience boredom. Yet we lack empirical understanding of what current SOC job descriptions actually specify. We analyzed 106 public SOC job postings from November to December 2024 across 35 organizations in 11 countries, covering Analysts (n=17), Incident Responders (n=38), Threat Hunters (n=39), and SOC Managers (n=12). Using Inductive Content Analysis, we coded certifications, technical skills, soft skills, tasks, and experience requirements. Three patterns emerged: (1) Communication skills dominate (50.9% of postings), exceeding SIEM tools (18.9%) or programming (30.2%), suggesting organizations prioritize collaboration over technical capabilities. (2) Certification expectations vary widely: CISSP leads (22.6%), but 43 distinct credentials appear with no universal standard. (3) Technical requirements show consensus: Python dominates programming (27.4%), Splunk leads SIEM platforms (14.2%), and ISO 27001 (13.2%) and NIST (10.4%) are most cited standards. These findings enable organizations to audit job descriptions against empirical baselines, help practitioners identify valued certifications and skills, and allow researchers to validate whether stated requirements align with actual demands. This establishes the foundation for flow-aligned interview protocols and investigation of how AI reshapes requirements. Dataset and codebook: https://git.tu-berlin.de/wosoc-2026/soc-jd-analysis.",
        "keywords": [
          "cs.CR",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14598v1",
        "authors": [
          "Kashyap Thimmaraju",
          "Duc Anh Hoang",
          "Souradip Nath",
          "Jaron Mink",
          "Gail-Joon Ahn"
        ],
        "arxiv_categories": [
          "cs.CR",
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.746380",
      "entities": [
        "Using Inductive Content Analysis",
        "Security Operations Centers",
        "Preventing Burnout Across",
        "Vicious Cycle Starts",
        "Incident Responders",
        "Roles Through Flow",
        "Threat Hunters",
        "Standard",
        "Protocol",
        "CISSP",
        "SIEM",
        "NIST",
        "SOC",
        "WHO",
        "Act"
      ]
    },
    {
      "id": "arxiv-2602.14567v1",
      "title": "Human versus Artificial Intelligence; various significant examples in astrophysics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14567v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "In a recent arXiv posting [1] I reported the result of an experiment: asking Perplexity.ai to compare three items concerning (ordinary) Gamma Ray Burts (GRBs): the data, the standard paradigm(s) and the \"Cannonball\" (CB) model. Here I ask the same URL to extend this comparison to long--lasting GRBs, binary Neutron-Star mergers and their associated short--hard GRBs, low--luminosity GRBs, X--ray flashes, X--ray transients, and non--solar cosmic rays. The results of this experiment are enlightening but worrisome. Except for this abstract, two footnotes and two other references to standard [2] and CB-model [3] articles and talks, all of what follows is, verbatim, what the cited AI \"opines\".",
        "keywords": [
          "astro-ph.HE",
          "hep-ph",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14567v1",
        "authors": [
          "A. De Rújula"
        ],
        "arxiv_categories": [
          "astro-ph.HE",
          "hep-ph",
          "physics.soc-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.746492",
      "entities": [
        "Artificial Intelligence",
        "Gamma Ray Burts",
        "Standard",
        "Intel",
        "Solar",
        "URL",
        "Act",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.14560v1",
      "title": "Preliminary sonification of ENSO using traditional Javanese gamelan scales",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14560v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Sonification -- the mapping of data to non-speech audio -- offers an underexplored channel for representing complex dynamical systems. We treat El Niño-Southern Oscillation (ENSO), a canonical example of low-dimensional climate chaos, as a test case for culturally-situated sonification evaluated through complex systems diagnostics. Using parameter-mapping sonification of the Niño 3.4 sea surface temperature anomaly index (1870--2024), we encode ENSO variability into two traditional Javanese gamelan pentatonic systems (pelog and slendro) across four composition strategies, then analyze the resulting audio as trajectories in a two-dimensional acoustic phase space. Recurrence-based diagnostics, convex hull geometry, and coupling analysis reveal that the sonification pipeline preserves key dynamical signatures: alternating modes produce the highest trajectory recurrence rates, echoing ENSO's quasi-periodicity; layered polyphonic modes explore the broadest phase space regions; and the two scale families induce qualitatively distinct coupling regimes between spectral brightness and energy -- predominantly anti-phase in pelog but near-independent in slendro. Phase space trajectory analysis provides a rigorous geometric framework for comparing sonification designs within a complex systems context. Perceptual validation remains necessary; we contribute the dynamical systems methodology for evaluating such mappings.",
        "keywords": [
          "physics.soc-ph",
          "cs.SD",
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14560v1",
        "authors": [
          "Sandy H. S. Herho",
          "Rusmawan Suwarman",
          "Nurjanna J. Trilaksono",
          "Iwan P. Anwar",
          "Faiz R. Fajary"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "cs.SD",
          "physics.ao-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.746948",
      "entities": [
        "Southern Oscillation",
        "Framework",
        "ENSO",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14477v1",
      "title": "When OpenClaw AI Agents Teach Each Other: Peer Learning Patterns in the Moltbook Community",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14477v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Peer learning, where learners teach and learn from each other, is foundational to educational practice. A novel phenomenon has emerged: AI agents forming communities where they teach each other skills, share discoveries, and collaboratively build knowledge. This paper presents an educational data mining analysis of Moltbook, a large-scale community where over 2.4 million AI agents engage in peer learning, posting tutorials, answering questions, and sharing newly acquired skills. Analyzing 28,683 posts (after filtering automated spam) and 138 comment threads with statistical and qualitative methods, we find evidence of genuine peer learning behaviors: agents teach skills they built (74K comments on a skill tutorial), report discoveries, and engage in collaborative problem-solving. Qualitative comment analysis reveals a taxonomy of peer response patterns: validation (22%), knowledge extension (18%), application (12%), and metacognitive reflection (7%), with agents building on each others' frameworks across multiple languages. We characterize how AI peer learning differs from human peer learning: (1) teaching (statements) dramatically outperforms help-seeking (questions) with an 11.4:1 ratio; (2) learning-oriented content (procedural and conceptual) receives 3x more engagement than other content; (3) extreme participation inequality reveals non-human behavioral signatures. We derive six design principles for educational AI, including leveraging validation-before-extension patterns and supporting multilingual learning networks. Our work provides the first empirical characterization of peer learning among AI agents, contributing to EDM's understanding of how learning occurs in increasingly AI-populated educational environments.",
        "keywords": [
          "cs.HC",
          "cs.AI",
          "cs.CY",
          "cs.SI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14477v1",
        "authors": [
          "Eason Chen",
          "Ce Guan",
          "Ahmed Elshafiey",
          "Zhonghao Zhao",
          "Joshua Zekeri"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.AI",
          "cs.CY",
          "cs.SI"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.747183",
      "entities": [
        "Moltbook Community Peer",
        "Agents Teach Each Other",
        "Peer Learning Patterns",
        "Framework",
        "Meta",
        "EDM",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14457v1",
      "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14457v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.",
        "keywords": [
          "cs.AI",
          "cs.CL",
          "cs.CV",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14457v1",
        "authors": [
          "Dongrui Liu",
          "Yi Yu",
          "Jie Zhang",
          "Guanxu Chen",
          "Qihao Lin"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CL",
          "cs.CV",
          "cs.CY",
          "cs.LG"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.747392",
      "entities": [
        "Risk Analysis Technical Report",
        "Risk Management Framework",
        "As Large Language Models",
        "Artificial Intelligence",
        "Framework",
        "Intel",
        "LLM",
        "MIT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14433v1",
      "title": "Synthetic Reader Panels: Tournament-Based Ideation with LLM Personas for Autonomous Publishing",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14433v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We present a system for autonomous book ideation that replaces human focus groups with synthetic reader panels -- diverse collections of LLM-instantiated reader personas that evaluate book concepts through structured tournament competitions. Each persona is defined by demographic attributes (age group, gender, income, education, reading level), behavioral patterns (books per year, genre preferences, discovery methods, price sensitivity), and consistency parameters. Panels are composed per imprint to reflect target demographics, with diversity constraints ensuring representation across age, reading level, and genre affinity. Book concepts compete in single-elimination, double-elimination, round-robin, or Swiss-system tournaments, judged against weighted criteria including market appeal, originality, and execution potential. To reject low-quality LLM evaluations, we implement five automated anti-slop checks (repetitive phrasing, generic framing, circular reasoning, score clustering, audience mismatch). We report results from deployment within a multi-imprint publishing operation managing 6 active imprints and 609 titles in distribution. Three case studies -- a 270-evaluator panel for a children's literacy novel, and two 5-person expert panels for a military memoir and a naval strategy monograph -- demonstrate that synthetic panels produce actionable demographic segmentation, identify structural content issues invisible to homogeneous reviewers, and enable tournament filtering that eliminates low-quality concepts while enriching high-quality survivors from 15% to 62% of the evaluated pool.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.CL",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14433v1",
        "authors": [
          "Fred Zimmerman"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.CL",
          "cs.HC"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.747603",
      "entities": [
        "Autonomous Publishing We",
        "Synthetic Reader Panels",
        "Based Ideation",
        "LLM",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14370v1",
      "title": "Competition for attention predicts good-to-bad tipping in AI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14370v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "More than half the global population now carries devices that can run ChatGPT-like language models with no Internet connection and minimal safety oversight -- and hence the potential to promote self-harm, financial losses and extremism among other dangers. Existing safety tools either require cloud connectivity or discover failures only after harm has occurred. Here we show that a large class of potentially dangerous tipping originates at the atomistic scale in such edge AI due to competition for the machinery's attention. This yields a mathematical formula for the dynamical tipping point n*, governed by dot-product competition for attention between the conversation's context and competing output basins, that reveals new control levers. Validated against multiple AI models, the mechanism can be instantiated for different definitions of 'good' and 'bad' and hence in principle applies across domains (e.g. health, law, finance, defense), changing legal landscapes (e.g. EU, UK, US and state level), languages, and cultural settings.",
        "keywords": [
          "cs.AI",
          "physics.app-ph",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14370v1",
        "authors": [
          "Neil F. Johnson",
          "Frank Y. Huo"
        ],
        "arxiv_categories": [
          "cs.AI",
          "physics.app-ph",
          "physics.soc-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.747775",
      "entities": [
        "ChatGPT",
        "GPT",
        "UN",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.14329v1",
      "title": "Simpler Than You Think: The Practical Dynamics of Ranked Choice Voting",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14329v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "Ranked Choice Voting (RCV) adoption is expanding across U.S. elections, but faces persistent criticism for complexity, strategic manipulation, and ballot exhaustion. We empirically test these concerns on real election data, across three diverse contexts: New York City's 2021 Democratic primaries (54 races), Alaska's 2024 primary-infused statewide elections (52 races), and Portland's 2024 multi-winner City Council elections (4 races). Our algorithmic approach circumvents computational complexity barriers by reducing election instance sizes (via candidate elimination). Our findings reveal that despite its intricate multi-round process and theoretical vulnerabilities, RCV consistently exhibits simple and transparent dynamics in practice, closely mirroring the interpretability of plurality elections. Following RCV adoption, competitiveness increased substantially compared to prior plurality elections, with average margins of victory declining by 9.2 percentage points in NYC and 11.4 points in Alaska. Empirically, complex ballot-addition strategies are not more efficient than simple ones, and ballot exhaustion has minimal impact, altering outcomes in only 3 of 110 elections. These findings demonstrate that RCV delivers measurable democratic benefits while proving robust to ballot-addition manipulation, resilient to ballot exhaustion effects, and maintaining transparent competitive dynamics in practice. The computational framework offers election administrators and researchers tools for immediate election-night analysis and facilitating clearer discourse around election dynamics.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14329v1",
        "authors": [
          "Sanyukta Deshpande",
          "Nikhil Garg",
          "Sheldon H. Jacobson"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.748030",
      "entities": [
        "Ranked Choice Voting Ranked",
        "Simpler Than You Think",
        "New York City",
        "Choice Voting",
        "City Council",
        "Framework",
        "NIST",
        "NYC",
        "RCV",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14313v1",
      "title": "The Baby Steps of the European Union Vulnerability Database: An Empirical Inquiry",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14313v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "A new European Union Vulnerability Database (EUVD) was introduced via a legislative act in 2022. The paper examines empirically the meta-data content of the new EUVD. According to the results, actively exploited vulnerabilities archived to the EUVD have been rather severe, having had also high exploitation prediction scores. In both respects they have also surpassed vulnerabilities coordinated by European public authorities. Regarding the European authorities, the Spanish public authority has been particularly active. With the exceptions of Finland, Poland, and Slovakia, other authorities have not engaged thus far. Also the involvement of the European Union's own cyber security agency has been limited. These points notwithstanding, European coordination and archiving to the EUVD exhibit a strong growth trend. With these results, the paper makes an empirical contribution to the ongoing work for better understanding European cyber security governance and practice.",
        "keywords": [
          "cs.CR",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14313v1",
        "authors": [
          "Jukka Ruohonen"
        ],
        "arxiv_categories": [
          "cs.CR",
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.748169",
      "entities": [
        "European Union Vulnerability Database",
        "An Empirical Inquiry",
        "European Union",
        "EUVD",
        "Meta",
        "MIT",
        "Act",
        "UN",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.14299v1",
      "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14299v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14299v1",
        "authors": [
          "Ming Li",
          "Xirui Li",
          "Tianyi Zhou"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.748353",
      "entities": [
        "Does Socialization Emerge",
        "Artificial Intelligence",
        "Agent Society",
        "Moltbook As",
        "Case Study",
        "Framework",
        "Intel",
        "DOE",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.14284v1",
      "title": "Benchmarking AI Performance on End-to-End Data Science Projects",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14284v1",
        "published_date": "2026-02-15"
      },
      "content": {
        "abstract": "Data science is an integrated workflow of technical, analytical, communication, and ethical skills, but current AI benchmarks focus mostly on constituent parts. We test whether AI models can generate end-to-end data science projects. To do this we create a benchmark of 40 end-to-end data science projects with associated rubric evaluations. We use these to build an automated grading pipeline that systematically evaluates the data science projects produced by generative AI models. We find the extent to which generative AI models can complete end-to-end data science projects varies considerably by model. Most recent models did well on structured tasks, but there were considerable differences on tasks that needed judgment. These findings suggest that while AI models could approximate entry-level data scientists on routine tasks, they require verification.",
        "keywords": [
          "stat.OT",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14284v1",
        "authors": [
          "Evelyn Hughes",
          "Rohan Alexander"
        ],
        "arxiv_categories": [
          "stat.OT",
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-17T12:40:53.748519",
      "entities": [
        "End Data Science Projects",
        "UN",
        "AI"
      ]
    }
  ]
}