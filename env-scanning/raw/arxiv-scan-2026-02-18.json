{
  "agent_metadata": {
    "agent_name": "arxiv-agent",
    "model_used": "sonnet",
    "papers_collected": 120,
    "steeps_categories_scanned": 6,
    "scan_date": "2026-02-18",
    "status": "success",
    "execution_time": 15.1,
    "process_id": 21795
  },
  "items": [
    {
      "id": "arxiv-2602.16712v1",
      "title": "One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16712v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Dexterous manipulation policies today largely assume fixed hand designs, severely restricting their generalization to new embodiments with varied kinematic and structural layouts. To overcome this limitation, we introduce a parameterized canonical representation that unifies a broad spectrum of dexterous hand architectures. It comprises a unified parameter space and a canonical URDF format, offering three key advantages. 1) The parameter space captures essential morphological and kinematic variations for effective conditioning in learning algorithms. 2) A structured latent manifold can be learned over our space, where interpolations between embodiments yield smooth and physically meaningful morphology transitions. 3) The canonical URDF standardizes the action space while preserving dynamic and functional properties of the original URDFs, enabling efficient and reliable cross-embodiment policy learning. We validate these advantages through extensive analysis and experiments, including grasp policy replay, VAE latent encoding, and cross-embodiment zero-shot transfer. Specifically, we train a VAE on the unified representation to obtain a compact, semantically rich latent embedding, and develop a grasping policy conditioned on the canonical representation that generalizes across dexterous hands. We demonstrate, through simulation and real-world tasks on unseen morphologies (e.g., 81.9% zero-shot success rate on 3-finger LEAP Hand), that our framework unifies both the representational and action spaces of structurally diverse hands, providing a scalable foundation for cross-hand learning toward universal dexterous manipulation.",
        "keywords": [
          "cs.RO"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16712v1",
        "authors": [
          "Zhenyu Wei",
          "Yunchao Yao",
          "Mingyu Ding"
        ],
        "arxiv_categories": [
          "cs.RO"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.678688",
      "entities": [
        "Unified Dexterous Manipulation Dexterous",
        "Canonical Representations",
        "Framework",
        "Standard",
        "One Hand",
        "Policy",
        "LEAP",
        "URDF",
        "Act",
        "MIT",
        "VAE",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16711v1",
      "title": "TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16711v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16711v1",
        "authors": [
          "Namitha Padmanabhan",
          "Matthew Gwilliam",
          "Abhinav Shrivastava"
        ],
        "arxiv_categories": [
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.678867",
      "entities": [
        "Videos Implicit Neural Representations",
        "Compressible Neural Representations",
        "Leveraging Temporal Coherence",
        "Framework",
        "HEVC",
        "PSNR",
        "MCL",
        "INR",
        "EPA",
        "MIT",
        "UVG",
        "JCV",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16710v1",
      "title": "EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16710v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior.",
        "keywords": [
          "cs.RO"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16710v1",
        "authors": [
          "Ruijie Zheng",
          "Dantong Niu",
          "Yuqi Xie",
          "Jing Wang",
          "Mengda Xu"
        ],
        "arxiv_categories": [
          "cs.RO"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.679032",
      "entities": [
        "Scaling Dexterous Manipulation",
        "Diverse Egocentric Human Data",
        "Vision Language Action",
        "Framework",
        "Policy",
        "Robot",
        "Intel",
        "Act",
        "VLA",
        "NSF",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16708v1",
      "title": "Policy Compiler for Secure Agentic Systems",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16708v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement. Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning. PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
        "keywords": [
          "cs.CR",
          "cs.AI",
          "cs.MA"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16708v1",
        "authors": [
          "Nils Palumbo",
          "Sarthak Choudhary",
          "Jihye Choi",
          "Prasad Chalasani",
          "Mihai Christodorescu"
        ],
        "arxiv_categories": [
          "cs.CR",
          "cs.AI",
          "cs.MA"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.679203",
      "entities": [
        "Secure Agentic Systems",
        "Agentic Systems",
        "Policy Compiler",
        "Protocol",
        "Policy",
        "PCAS",
        "NIST",
        "Act",
        "LLM",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16705v1",
      "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16705v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.",
        "keywords": [
          "cs.RO",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16705v1",
        "authors": [
          "Runpei Dong",
          "Ziyan Li",
          "Xialin He",
          "Saurabh Gupta"
        ],
        "arxiv_categories": [
          "cs.RO",
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.679403",
      "entities": [
        "Vocabulary Visual Loco",
        "Learning Humanoid End",
        "Manipulation Visual",
        "Machine Learning",
        "Effector Control",
        "Policy",
        "Robot",
        "Apple",
        "HERO",
        "Act",
        "MIT",
        "RGB",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16704v1",
      "title": "Reinforced Fast Weights with Next-Sequence Prediction",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16704v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.",
        "keywords": [
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16704v1",
        "authors": [
          "Hee Seung Hwang",
          "Xindi Wu",
          "Sanghyuk Chun",
          "Olga Russakovsky"
        ],
        "arxiv_categories": [
          "cs.CL"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.679571",
      "entities": [
        "Sequence Prediction Fast",
        "Reinforced Fast Weights",
        "Reinforced Fast",
        "Transformer",
        "DeltaNet-1",
        "Framework",
        "Policy",
        "REFINE",
        "GRPO",
        "Act",
        "NSP",
        "MIT",
        "NTP",
        "NSF",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.16703v1",
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16703v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16703v1",
        "authors": [
          "Shen Zhou Hong",
          "Alex Kleinman",
          "Alyssa Mathiowetz",
          "Adam Howes",
          "Julian Cohen"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.679756",
      "entities": [
        "Novice Performance",
        "Biology Large",
        "Measuring Mid",
        "Laboratory",
        "Mid-2025",
        "LLM",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16702v1",
      "title": "Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16702v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \\emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16702v1",
        "authors": [
          "Mingjia Shi",
          "Yinhan He",
          "Yaochen Zhu",
          "Jundong Li"
        ],
        "arxiv_categories": [
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.679930",
      "entities": [
        "Language Reasoning Vision",
        "Revisiting Vision",
        "Aware Principle",
        "Route Thinking",
        "Aware Multi",
        "LLM",
        "SAP",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16699v1",
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16699v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
        "keywords": [
          "cs.CL",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16699v1",
        "authors": [
          "Wenxuan Ding",
          "Nicholas Tomlin",
          "Greg Durrett"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.680087",
      "entities": [
        "Aware Exploration",
        "Framework",
        "LLM",
        "Act",
        "MIT",
        "CTA",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16693v1",
      "title": "Numerical study of non-relativistic quantum systems and small oscillations induced in a helically twisted geometry",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16693v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "We investigate bound states of a non-relativistic scalar particle in a three-dimensional helically twisted (torsional) geometry, considering both the free case and the presence of external radial interactions. The dynamics is described by the Schrödinger equation on a curved spatial background and, when included, by minimal coupling to a magnetic vector potential incorporating an Aharonov--Bohm flux. After separation of variables, the problem reduces to a one-dimensional radial eigenvalue equation governed by an effective potential that combines torsion-induced Coulomb-like and centrifugal-like structures with magnetic/flux-dependent terms and optional model interactions. Because closed-form analytic solutions are not reliable over the parameter ranges required for systematic scans, we compute spectra and eigenfunctions numerically by formulating the radial equation as a self-adjoint Sturm--Liouville problem and solving it with a finite-difference discretization on a truncated radial domain, with explicit convergence control. We analyze four representative scenarios: (i) no external potential, (ii) Cornell-type confinement, (iii) Kratzer-type interaction, and (iv) the small-oscillation regime around the minimum of a Morse potential. We present systematic trends of the low-lying levels as functions of the torsion parameter, magnetic field, and azimuthal sector, and we show that geometric couplings alone can produce effective confinement even in the absence of an external interaction.",
        "keywords": [
          "quant-ph",
          "math-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16693v1",
        "authors": [
          "C. F. S. Pereira",
          "R. L. L. Vitória",
          "A. R. Soares",
          "B. B. Silva",
          "H. Belich"
        ],
        "arxiv_categories": [
          "quant-ph",
          "math-ph"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.680478",
      "entities": [
        "Act",
        "EPA",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.16689v1",
      "title": "Are Object-Centric Representations Better At Compositional Generalization?",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16689v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.",
        "keywords": [
          "cs.CV",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16689v1",
        "authors": [
          "Ferdinand Kapl",
          "Amir Mohammad Karimi Mamaghan",
          "Maximilian Seitzer",
          "Karl Henrik Johansson",
          "Carsten Marr"
        ],
        "arxiv_categories": [
          "cs.CV",
          "cs.LG"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.680658",
      "entities": [
        "Centric Representations Better At",
        "Compositional Generalization",
        "Visual Question Answering",
        "Machine Learning",
        "Are Object",
        "CLEVR",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16687v1",
      "title": "Scaling Open Discrete Audio Foundation Models with Interleaved Semantic, Acoustic, and Text Tokens",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16687v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Current audio language models are predominantly text-first, either extending pre-trained text LLM backbones or relying on semantic-only audio tokens, limiting general audio modeling. This paper presents a systematic empirical study of native audio foundation models that apply next-token prediction to audio at scale, jointly modeling semantic content, acoustic details, and text to support both general audio generation and cross-modal capabilities. We provide comprehensive empirical insights for building such models: (1) We systematically investigate design choices -- data sources, text mixture ratios, and token composition -- establishing a validated training recipe. (2) We conduct the first scaling law study for discrete audio models via IsoFLOP analysis on 64 models spanning $3{\\times}10^{18}$ to $3{\\times}10^{20}$ FLOPs, finding that optimal data grows 1.6$\\times$ faster than optimal model size. (3) We apply these lessons to train SODA (Scaling Open Discrete Audio), a suite of models from 135M to 4B parameters on 500B tokens, comparing against our scaling predictions and existing models. SODA serves as a flexible backbone for diverse audio/text tasks -- we demonstrate this by fine-tuning for voice-preserving speech-to-speech translation, using the same unified architecture.",
        "keywords": [
          "cs.SD",
          "cs.CL",
          "eess.AS"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16687v1",
        "authors": [
          "Potsawee Manakul",
          "Woody Haosheng Gan",
          "Martijn Bartelds",
          "Guangzhi Sun",
          "William Held"
        ],
        "arxiv_categories": [
          "cs.SD",
          "cs.CL",
          "eess.AS"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.680814",
      "entities": [
        "Scaling Open Discrete Audio",
        "Interleaved Semantic",
        "Text Tokens Current",
        "Foundation Models",
        "SODA",
        "LLM",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16682v1",
      "title": "Learning Situated Awareness in the Real World",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16682v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16682v1",
        "authors": [
          "Chuhan Li",
          "Ruilin Han",
          "Joy Hsu",
          "Yongyuan Liang",
          "Rajiv Dhawan"
        ],
        "arxiv_categories": [
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.680982",
      "entities": [
        "Learning Situated Awareness",
        "Situated Awareness",
        "Real World",
        "Ban Meta",
        "Intel",
        "Meta",
        "Act",
        "SAW",
        "MFM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16681v1",
      "title": "VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16681v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16681v1",
        "authors": [
          "Yingyuan Yang",
          "Tian Lan",
          "Yifei Gao",
          "Yimeng Lu",
          "Wenjun He"
        ],
        "arxiv_categories": [
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.681140",
      "entities": [
        "Anomaly Window Contrastive Learning",
        "Reversible Image Conversion",
        "Shot Time Series Anomaly",
        "Level Temporal Alignment",
        "Vision Enhanced Zero",
        "Context Anomalies",
        "Point Anomalies",
        "Adaptive Multi",
        "Detection Time",
        "Modal Fusion",
        "Framework",
        "Fusion",
        "TSAD",
        "Wind",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.16680v1",
      "title": "Intermodal quantum key distribution over an 18 km free-space channel with adaptive optics and room-temperature detectors",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16680v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Intermodal quantum key distribution at telecom wavelengths provides a hybrid interface between fiber connections and free-space links, both essential for the realization of scalable and interoperable quantum networks. Although demonstrated over short-range free-space links, long-distance implementations of intermodal quantum key distribution remain challenging, due to turbulence-induced wavefront aberrations which limit efficient single-mode fiber coupling at the optical receiver. Here, we demonstrate a real-time intermodal quantum key distribution field trial over an 18 km free-space link, connecting a remote terminal to an urban optical ground station equipped with a 40 cm-class telescope. An adaptive optics system, implementing direct wavefront sensing and high-order aberration correction, enables efficient single-mode fiber coupling and allows secure key generation of 200 bit/s using a compact state analyzer equipped with room-temperature detectors. We further validate through experimental data a turbulence-based model for predicting fiber coupling efficiency, providing practical design guidelines for future intermodal quantum networks.",
        "keywords": [
          "quant-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16680v1",
        "authors": [
          "Edoardo Rossi",
          "Ilektra Karakosta-Amarantidou",
          "Matteo Padovan",
          "Marco Nardi",
          "Marco Avesani"
        ],
        "arxiv_categories": [
          "quant-ph"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.681279",
      "entities": [
        "Guideline",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16675v1",
      "title": "Learning to unfold cloth: Scaling up world models to deformable object manipulation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16675v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Learning to manipulate cloth is both a paradigmatic problem for robotic research and a problem of immediate relevance to a variety of applications ranging from assistive care to the service industry. The complex physics of the deformable object makes this problem of cloth manipulation nontrivial. In order to create a general manipulation strategy that addresses a variety of shapes, sizes, fold and wrinkle patterns, in addition to the usual problems of appearance variations, it becomes important to carefully consider model structure and their implications for generalisation performance. In this paper, we present an approach to in-air cloth manipulation that uses a variation of a recently proposed reinforcement learning architecture, DreamerV2. Our implementation modifies this architecture to utilise surface normals input, in addition to modiying the replay buffer and data augmentation procedures. Taken together these modifications represent an enhancement to the world model used by the robot, addressing the physical complexity of the object being manipulated by the robot. We present evaluations both in simulation and in a zero-shot deployment of the trained policies in a physical robot setup, performing in-air unfolding of a variety of different cloth types, demonstrating the generalisation benefits of our proposed architecture.",
        "keywords": [
          "cs.RO"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16675v1",
        "authors": [
          "Jack Rome",
          "Stephen James",
          "Subramanian Ramamoorthy"
        ],
        "arxiv_categories": [
          "cs.RO"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.681434",
      "entities": [
        "Robot",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16671v1",
      "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16671v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
        "keywords": [
          "cs.SE",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16671v1",
        "authors": [
          "Jaid Monwar Chowdhury",
          "Chi-An Fu",
          "Reyhaneh Jabbarvand"
        ],
        "arxiv_categories": [
          "cs.SE",
          "cs.AI"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.681613",
      "entities": [
        "Unit Test Generation Automated",
        "While Large Language Models",
        "Control Flow Graph",
        "Scenario Planning",
        "Operation Map",
        "Framework",
        "SPARC",
        "KLEE",
        "LLM",
        "Act",
        "CFG",
        "EPA",
        "MIT",
        "AI",
        "EU"
      ]
    },
    {
      "id": "arxiv-2602.16669v1",
      "title": "PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16669v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16669v1",
        "authors": [
          "Bo Lang",
          "Nirav Savaliya",
          "Zhihao Zheng",
          "Jinglun Feng",
          "Zheng-Hang Yeh"
        ],
        "arxiv_categories": [
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.681791",
      "entities": [
        "Vectorized Map Construction High",
        "History Rasterized Map Memory",
        "Aware Query Generator",
        "Term Future Guidance",
        "Historical Reasoning",
        "Map Guidance Module",
        "Consistent Online",
        "Framework",
        "SOTA",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.16666v1",
      "title": "Towards a Science of AI Agent Reliability",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16666v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16666v1",
        "authors": [
          "Stephan Rabanser",
          "Sayash Kapoor",
          "Peter Kirgis",
          "Kangheng Liu",
          "Saiteja Utpala"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.681911",
      "entities": [
        "Agent Reliability",
        "Standard",
        "Act",
        "EPA",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16664v1",
      "title": "Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16664v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16664v1",
        "authors": [
          "Jiaming Liu",
          "Felix Petersen",
          "Yunhe Gao",
          "Yabin Zhang",
          "Hyojin Kim"
        ],
        "arxiv_categories": [
          "cs.CV"
        ],
        "steeps_mapping": "T_Technological"
      },
      "preliminary_category": "T",
      "collected_at": "2026-02-18T19:01:47.682037",
      "entities": [
        "Supervised Semantic Bridge Adversarial",
        "Supervised Semantic Bridge",
        "Image Translation",
        "Unpaired Image",
        "Framework",
        "Fusion",
        "MIT",
        "SSB",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16631v1",
      "title": "Can Wearable Exoskeletons Reduce Gender and Disability Gaps in the Construction Industry?",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16631v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "The share of construction trade jobs held by women and people with disabilities has remained stubbornly low in the face of chronic shortages of skilled labor. This study explores the potential of wearable assistive technologies to reduce these disparities. We use U.S. worker-level data to estimate employment and wage differences by gender and by mobility/strength impairments in construction and non-construction jobs. We also use occupational-level data to examine variations in workforce composition, physical skill requirements, and earnings across detailed construction occupations. Regression estimates indicate that being a woman and having strength and mobility impairments are associated with substantial employment and pay gaps in construction compared to non-construction jobs. Further analysis shows a high negative correlation between the representation of women and the ability levels required in those occupations. Finally, we discuss several wearable exoskeletons under development for people with upper-body and lower-body impairments, focusing on how these innovations could be integrated into construction jobs. These findings suggest that wearable exoskeletons that enhance manual dexterity, balance, and strength may improve the representation of women and people with disabilities in some of the higher-paying occupations in construction.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16631v1",
        "authors": [
          "Yana Rodgers",
          "Xiangmin Liu",
          "Jingang Yi",
          "Liang Zhang"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.674583",
      "entities": [
        "Can Wearable Exoskeletons Reduce",
        "Construction Industry",
        "Disability Gaps",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16527v1",
      "title": "Model selection confidence sets for time series models with applications to electricity load data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16527v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "This paper studies the Model Selection Confidence Set (MSCS) methodology for univariate time series models involving autoregressive and moving average components, and applies it to study model selection uncertainty in the Italian electricity load data. Rather than relying on a single model selected by an arbitrary criterion, the MSCS identifies a set of models that are statistically indistinguishable from the true data-generating process at a given confidence level. The size and composition of this set reveal crucial information about model selection uncertainty: noisy data scenarios produce larger sets with many candidate models, while more informative cases narrow the set considerably. To study the importance of each model term, we consider numerical statistics measuring the frequency with which each term is included in both the entire MSCS and in Lower Boundary Models (LBM), its most parsimonious specifications. Applied to Italian hourly electricity load data, the MSCS methodology reveals marked intraday variation in model selection uncertainty and isolates a collection of model specifications that deliver competitive short-term forecasts while highlighting key drivers of electricity load like intraday hourly lags, temperature, calendar effects and solar energy generation.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16527v1",
        "authors": [
          "Piersilvio De Bortoli",
          "Davide Ferrari",
          "Francesco Ravazzolo",
          "Luca Rossini"
        ],
        "arxiv_categories": [
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.674899",
      "entities": [
        "Model Selection Confidence Set",
        "Lower Boundary Models",
        "Solar",
        "MSCS",
        "LBM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16376v1",
      "title": "Two-way Clustering Robust Variance Estimator in Quantile Regression Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16376v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "We study inference for linear quantile regression with two-way clustered data. Using a separately exchangeable array framework and a projection decomposition of the quantile score, we characterize regime-dependent convergence rates and establish a self-normalized Gaussian approximation. We propose a two-way cluster-robust sandwich variance estimator with a kernel-based density ``bread'' and a projection-matched ``meat'', and prove consistency and validity of inference in Gaussian regimes. We also show an impossibility result for uniform inference in a non-Gaussian interaction regime.",
        "keywords": [
          "econ.EM",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16376v1",
        "authors": [
          "Ulrich Hounyo",
          "Jiahao Lin"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.AP"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.675056",
      "entities": [
        "Clustering Robust Variance Estimator",
        "Quantile Regression Models We",
        "Framework",
        "Act",
        "EPA",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16310v1",
      "title": "Introducing the b-value: combining unbiased and biased estimators from a sensitivity analysis perspective",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16310v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "In empirical research, when we have multiple estimators for the same parameter of interest, a central question arises: how do we combine unbiased but less precise estimators with biased but more precise ones to improve the inference? Under this setting, the point estimation problem has attracted considerable attention. In this paper, we focus on a less studied inference question: how can we conduct valid statistical inference in such settings with unknown bias? We propose a strategy to combine unbiased and biased estimators from a sensitivity analysis perspective. We derive a sequence of confidence intervals indexed by the magnitude of the bias, which enable researchers to assess how conclusions vary with the bias levels. Importantly, we introduce the notion of the b-value, a critical value of the unknown maximum relative bias at which combining estimators does not yield a significant result. We apply this strategy to three canonical combined estimators: the precision-weighted estimator, the pretest estimator, and the soft-thresholding estimator. For each estimator, we characterize the sequence of confidence intervals and determine the bias threshold at which the conclusion changes. Based on the theory, we recommend reporting the b-value based on the soft-thresholding estimator and its associated confidence intervals, which are robust to unknown bias and achieve the lowest worst-case risk among the alternatives.",
        "keywords": [
          "stat.ME",
          "econ.EM",
          "math.ST",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16310v1",
        "authors": [
          "Zhexiao Lin",
          "Peter J. Bickel",
          "Peng Ding"
        ],
        "arxiv_categories": [
          "stat.ME",
          "econ.EM",
          "math.ST",
          "stat.AP"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.675370",
      "entities": [
        "Act",
        "DOE",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16078v1",
      "title": "AI as Coordination-Compressing Capital: Task Reallocation, Organizational Redesign, and the Regime Fork",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16078v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Task-based models of AI and labor hold organizational structure fixed, analyzing how technology shifts task assignments within a given firm architecture. Yet emerging evidence shows firms flattening hierarchies in response to AI adoption -- a phenomenon these models cannot generate. We extend the task-based framework by introducing agent capital (K_A): AI systems that reduce coordination costs within organizations, expanding managerial spans of control and enabling endogenous task creation. We derive five propositions characterizing how coordination compression affects output, hierarchy, manager demand, wage dispersion, and the task frontier. The model generates a regime fork: depending on whether agent capital complements all workers broadly (general infrastructure) or high-skill managers disproportionately (elite complementarity), the same technology produces either broad-based productivity gains or superstar concentration, with sharply divergent distributional consequences. Numerical simulations with heterogeneous managers and workers across a 2x2 parameter space (elite complementarity x endogenous task creation) confirm sharp regime divergence: in settings where coordination compression substantially expands employment, economy-wide inequality falls in all regimes, but the rate of reduction is regime-dependent and the manager-worker wage gap widens universally. The distributional impact of AI hinges not on the technology itself but on the elasticity of organizational structure -- and on who controls that elasticity.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16078v1",
        "authors": [
          "Alex Farach"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.675713",
      "entities": [
        "Organizational Redesign",
        "Compressing Capital",
        "Task Reallocation",
        "Regime Fork Task",
        "Framework",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16061v1",
      "title": "Partial Identification under Missing Data Using Weak Shadow Variables from Pretrained Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16061v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Estimating population quantities such as mean outcomes from user feedback is fundamental to platform evaluation and social science, yet feedback is often missing not at random (MNAR): users with stronger opinions are more likely to respond, so standard estimators are biased and the estimand is not identified without additional assumptions. Existing approaches typically rely on strong parametric assumptions or bespoke auxiliary variables that may be unavailable in practice. In this paper, we develop a partial identification framework in which sharp bounds on the estimand are obtained by solving a pair of linear programs whose constraints encode the observed data structure. This formulation naturally incorporates outcome predictions from pretrained models, including large language models (LLMs), as additional linear constraints that tighten the feasible set. We call these predictions weak shadow variables: they satisfy a conditional independence assumption with respect to missingness but need not meet the completeness conditions required by classical shadow-variable methods. When predictions are sufficiently informative, the bounds collapse to a point, recovering standard identification as a special case. In finite samples, to provide valid coverage of the identified set, we propose a set-expansion estimator that achieves slower-than-$\\sqrt{n}$ convergence rate in the set-identified regime and the standard $\\sqrt{n}$ rate under point identification. In simulations and semi-synthetic experiments on customer-service dialogues, we find that LLM predictions are often ill-conditioned for classical shadow-variable methods yet remain highly effective in our framework. They shrink identification intervals by 75--83\\% while maintaining valid coverage under realistic MNAR mechanisms.",
        "keywords": [
          "stat.ML",
          "cs.LG",
          "econ.EM",
          "stat.ME"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16061v1",
        "authors": [
          "Hongyu Chen",
          "David Simchi-Levi",
          "Ruoxuan Xiong"
        ],
        "arxiv_categories": [
          "stat.ML",
          "cs.LG",
          "econ.EM",
          "stat.ME"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.676056",
      "entities": [
        "Pretrained Models Estimating",
        "Missing Data Using Weak",
        "Partial Identification",
        "Shadow Variables",
        "Framework",
        "Standard",
        "MNAR",
        "LLM",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15980v1",
      "title": "Dutch Disease and the Resource Curse: The Progression of Views from Exchange Rates to Women's Agency and Well-Being",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15980v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "This article provides an overview of the history of economic thought on natural resource extraction, which has long been considered an enclave industry with few benefits for areas beyond the local economy. We focus on more recent scholarship examining the social impacts of natural resource extraction, emphasizing gender-related outcomes and determinants. An important lesson from this scholarship is that it is difficult to discuss sustainable development in its contemporary sense without paying due diligence to the gender dimensions of natural resource extraction. A lesson highlighted is that the \"resource curse\" view of natural capital may not be as pervasive as previously thought.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15980v1",
        "authors": [
          "Nidhiya Menon",
          "Yana Rodgers"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.676217",
      "entities": [
        "Resource Curse",
        "Exchange Rates",
        "Dutch Disease",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15730v1",
      "title": "Causal Effect Estimation with Latent Textual Treatments",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15730v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.",
        "keywords": [
          "cs.CL",
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15730v1",
        "authors": [
          "Omri Feldman",
          "Amar Venugopal",
          "Jann Spiess",
          "Amir Feder"
        ],
        "arxiv_categories": [
          "cs.CL",
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.676455",
      "entities": [
        "Latent Textual Treatments Understanding",
        "Causal Effect Estimation",
        "LLM",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15722v1",
      "title": "Pricing Discrete and Nonlinear Markets With Semidefinite Relaxations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15722v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Nonconvexities in markets with discrete decisions and nonlinear constraints make efficient pricing challenging, often necessitating subsidies. A prime example is the unit commitment (UC) problem in electricity markets, where costly subsidies are commonly required. We propose a new pricing scheme for nonconvex markets with both discreteness and nonlinearity, by convexifying nonconvex structures through a semidefinite programming (SDP) relaxation and deriving prices from the relaxation's dual variables. When the choice set is bounded, we establish strong duality for the SDP, which allows us to extend the envelope theorem to the value function of the relaxation. This extension yields a marginal price signal for demand, which we use as our pricing mechanism. We demonstrate that under certain conditions-for instance, when the relaxation's right hand sides are linear in demand-the resulting lost opportunity cost is bounded by the relaxation's optimality gap. This result highlights the importance of achieving tight relaxations. The proposed framework applies to nonconvex electricity market problems, including for both direct current and alternating current UC. Our numerical experiments indicate that the SDP relaxations are often tight, reinforcing the effectiveness of the proposed pricing scheme. Across a suite of IEEE benchmark instances, the lost opportunity cost under our pricing scheme is, on average, 46% lower than that of the commonly used fixed-binary pricing scheme.",
        "keywords": [
          "math.OC",
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15722v1",
        "authors": [
          "Cheng Guo",
          "Lauren Henderson",
          "Ryan Cory-Wright",
          "Boshi Yang"
        ],
        "arxiv_categories": [
          "math.OC",
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.676750",
      "entities": [
        "Nonlinear Markets With Semidefinite",
        "Relaxations Nonconvexities",
        "Pricing Discrete",
        "Framework",
        "IEEE",
        "MIT",
        "SDP",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15699v1",
      "title": "Understanding Classical Decomposability of Inequality Measures: A Graphical Analysis",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15699v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "This paper's objective is pedagogical and interpretive. Namely, it gives a simple geometric analysis of classical (by which I mean population-share-weighted or income-share-weighted) inequality decomposability in the simplest nontrivial setting of three individuals. Income distributions in this case can be represented as points on the two-dimensional income-share simplex. In this representation, classical decomposability translates into concrete geometric restrictions of within- and between-group components. The geometric framework makes it possible to localize and compare violations of decomposability across inequality measures. The analysis is applied to the Mean Log Deviation, the Gini coefficient, the coefficient of variation, and the Theil index.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15699v1",
        "authors": [
          "Tatiana Komarova"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.676917",
      "entities": [
        "Understanding Classical Decomposability",
        "Inequality Measures",
        "Mean Log Deviation",
        "Framework",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15690v1",
      "title": "Income Inequality and Economic Growth: A Meta-Analytic Approach",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15690v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "The empirical literature on the relationship between income inequality and economic growth has produced highly heterogeneous and often conflicting results. This paper investigates the sources of this heterogeneity using a meta-analytic approach that systematically combines and analyzes evidence from relevant studies published between 1994 and 2025. We find an economically small but statistically significant negative average effect of income inequality on subsequent economic growth, together with strong evidence of substantial heterogeneity and selective publication based on statistical significance, but no evidence of systematic directional bias. To explain the observed heterogeneity, we estimate a meta-regression. The results indicate that both real-world characteristics and research design choices shape reported effect sizes. In particular, inequality measured net of taxes and transfers is associated with more negative growth effects, and the adverse impact of inequality is weaker - or even reversed - in high-income economies relative to developing countries. Methodological choices also matter: cross-sectional studies tend to report more negative estimates, while fixed-effects, instrumental-variable, and GMM estimators are associated with more positive estimates in panel settings.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15690v1",
        "authors": [
          "Lisa Cpretti",
          "Lorenzo Tonni"
        ],
        "arxiv_categories": [
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.677171",
      "entities": [
        "Income Inequality",
        "Economic Growth",
        "Meta",
        "Act",
        "GMM",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15607v1",
      "title": "Agent-based macroeconomics for the UK's Seventh Carbon Budget",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15607v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "In June 2026, the UK government will set its carbon budget for the period 2038 to 2042, the seventh such carbon budget (CB7) since the Climate Change Act became law in 2008. For the first time, this carbon budget will be accompanied by a macroeconomic assessment of its impact on growth, employment, inflation and inequality. Researchers from the Institute of New Economic Thinking (INET) Oxford are working in partnership with the Department for Energy Security and Net Zero to deliver this assessment using our data-driven macroeconomic agent-based model (ABM). This extended abstract presents the work in progress towards this pioneering policymaking using our data-driven macroeconomic ABM. We are conducting our work in three work packages. By the time of the workshop, we hope to be able to present preliminary findings from the first two work packages. In WP1, we adapt an existing macro-ABM prototype and build a UK macroeconomic baseline. The main task for this is initialising the model with suitable UK household microdata. We present the options considered and the approach settled upon. In WP2, we conduct preliminary modelling that represents UK decarbonisation as an external shock to financial flows and technical coefficients. In order to present results in time to influence the June 2026 policy decision, this second work package exogenously forces the ABM to follow the CB7 green investment and associated technological change projections provided by the Climate Change Committee. Finally, we will implement more sophisticated social and technological learning packages in WP3, building our own projections of likely decarbonisation pathways that may diverge from UK government plans. For the workshop, we will present the progress of WP1 and WP2.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15607v1",
        "authors": [
          "Tom Youngman",
          "Tim Lennox",
          "M. Lopes Alves",
          "Pirta Palola",
          "Brendon Tankwa"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.677507",
      "entities": [
        "Climate Change Committee",
        "Seventh Carbon Budget In",
        "New Economic Thinking",
        "Climate Change Act",
        "Energy Security",
        "Institute",
        "Net Zero",
        "Policy",
        "INET",
        "Act",
        "EPA",
        "MIT",
        "ABM",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15600v1",
      "title": "The geometry of online conversations and the causal antecedents of conflictual discourse",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15600v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "This article investigates the causal antecedents of conflictual language and the geometry of interaction in online threaded conversations related to climate change. We employ three annotation dimensions, inferred through LLM prompting and averaging, to capture complementary aspects of discursive conflict (such as stance: agreement vs disagreement; tone: attacking vs respectful; and emotional versus factual framing) and use data from a threaded online forum to examine how these dimensions respond to temporal, conversational, and arborescent structural features of discussions. We show that, as suggested by the literature, longer delays between successive posts in a thread are associated with replies that are, on average, more respectful, whereas longer delays relative to the parent post are associated with slightly less disagreement but more emotional (less factual) language. Second, we characterize alignment with the local conversational environment and find strong convergence both toward the average stance, tone and emotional framing of older sibling posts replying to the same parent and toward those of the parent post itself, with parent post effects generally stronger than sibling effects. We further show that early branch-level responses condition these alignment dynamics, such that parent-child stance alignment is amplified or attenuated depending on whether a branch is initiated in agreement or disagreement with the discussion's root message. These influences are largely additive for civility-related dimensions (attacking vs respectful, disagree vs agree), whereas for emotional versus factual framing there is a significant interaction: alignment with the parent's emotionality is amplified when older siblings are similarly aligned.",
        "keywords": [
          "cs.SI",
          "cs.AI",
          "econ.EM",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15600v1",
        "authors": [
          "Carlo Santagiustina",
          "Caterina Cruciani"
        ],
        "arxiv_categories": [
          "cs.SI",
          "cs.AI",
          "econ.EM",
          "stat.AP"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.677841",
      "entities": [
        "Agreement",
        "LLM",
        "Act"
      ]
    },
    {
      "id": "arxiv-2602.15559v1",
      "title": "Fixed-Horizon Self-Normalized Inference for Adaptive Experiments via Martingale AIPW/DML with Logged Propensities",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15559v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Adaptive randomized experiments update treatment probabilities as data accrue, but still require an end-of-study interval for the average treatment effect (ATE) at a prespecified horizon. Under adaptive assignment, propensities can keep changing, so the predictable quadratic variation of AIPW/DML score increments may remain random. When no deterministic variance limit exists, Wald statistics normalized by a single long-run variance target can be conditionally miscalibrated given the realized variance regime. We assume no interference, sequential randomization, i.i.d. arrivals, and executed overlap on a prespecified scored set, and we require two auditable pipeline conditions: the platform logs the executed randomization probability for each unit, and the nuisance regressions used to score unit $t$ are constructed predictably from past data only. These conditions make the centered AIPW/DML scores an exact martingale difference sequence. Using self-normalized martingale limit theory, we show that the Studentized statistic, with variance estimated by realized quadratic variation, is asymptotically N(0,1) at the prespecified horizon, even without variance stabilization. Simulations validate the theory and highlight when standard fixed-variance Wald reporting fails.",
        "keywords": [
          "stat.ME",
          "econ.EM",
          "math.ST",
          "stat.ML"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15559v1",
        "authors": [
          "Gabriel Saco"
        ],
        "arxiv_categories": [
          "stat.ME",
          "econ.EM",
          "math.ST",
          "stat.ML"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.678101",
      "entities": [
        "Logged Propensities Adaptive",
        "Normalized Inference",
        "Adaptive Experiments",
        "Horizon Self",
        "Standard",
        "NIST",
        "AIPW",
        "Act",
        "DML",
        "ATE",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15312v1",
      "title": "Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15312v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.",
        "keywords": [
          "cs.CL",
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15312v1",
        "authors": [
          "Stephan Ludwig",
          "Peter J. Danaher",
          "Xiaohao Yang",
          "Yu-Ting Lin",
          "Ehsan Abedin"
        ],
        "arxiv_categories": [
          "cs.CL",
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.678392",
      "entities": [
        "Evaluation Measurement Accurately",
        "Large Language Model Approach",
        "Extracting Consumer Insight",
        "Amazon",
        "GPT-4",
        "BERT",
        "Act",
        "MIT",
        "GPT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15289v1",
      "title": "A Projection Approach to Nonparametric Significance and Conditional Independence Testing",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15289v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "This paper develops a novel nonparametric significance test based on a tailored nonparametric-type projected weighting function that exhibits appealing theoretical and numerical properties. We derive the asymptotic properties of the proposed test and show that it can detect local alternatives at the parametric rate. Using the nonparametric orthogonal projection, we construct a computationally convenient multiplier bootstrap to obtain critical values from the case-dependent asymptotic null distribution. Compared with the existing literature, our approach overcomes the need for a stronger compact support assumption on the density of covariates arising from random denominators. We also extend the tailor-made projection procedure to test the conditional independence assumption. The simulation experiments further illustrate the advantages of our proposed method in testing significance and conditional independence in finite samples.",
        "keywords": [
          "econ.EM"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15289v1",
        "authors": [
          "Xiaojun Song",
          "Jichao Yuan"
        ],
        "arxiv_categories": [
          "econ.EM"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.678587",
      "entities": [
        "Nonparametric Significance",
        "Projection Approach",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15182v1",
      "title": "Autodeleveraging as Online Learning",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15182v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Autodeleveraging (ADL) is a last-resort loss socialization mechanism used by perpetual futures venues when liquidation and insurance buffers are insufficient to restore solvency. Despite the scale of perpetual futures markets, ADL has received limited formal treatment as a sequential control problem. This paper provides a concise formalization of ADL as online learning on a PNL-haircut domain: at each round, the venue selects a solvency budget and a set of profitable trader accounts. The profitable accounts are liquidated to cover shortfalls up to the solvency budget, with the aim of recovering exchange-wide solvency. In this model, ADL haircuts apply to positive PNL (unrealized gains), not to posted collateral principal. Using our online learning model, we provide robustness results and theoretical upper bounds on how poorly a mechanism can perform at recovering solvency. We apply our model to the October 10, 2025 Hyperliquid stress episode. The regret caused by Hyperliquid's production ADL queue is about 50\\% of an upper bound on regret, calibrated to this event, while our optimized algorithm achieves about 2.6\\% of the same bound. In dollar terms, the production ADL model over liquidates trader profits by up to \\$51.7M. We also counterfactually evaluated algorithms inspired by our online learning framework that perform better and found that the best algorithm reduces overshoot to \\$3M. Our results provide simple, implementable mechanisms for improving ADL in live perpetuals exchanges.",
        "keywords": [
          "cs.GT",
          "q-fin.RM",
          "q-fin.TR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15182v1",
        "authors": [
          "Tarun Chitra",
          "Nagu Thogiti",
          "Mauricio Jean Pieer Trujillo Ramirez",
          "Victor Xu"
        ],
        "arxiv_categories": [
          "cs.GT",
          "q-fin.RM",
          "q-fin.TR"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.678869",
      "entities": [
        "Online Learning Autodeleveraging",
        "Framework",
        "Act",
        "MIT",
        "PNL",
        "ADL",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14774v1",
      "title": "The unintended effects of universalizing social pensions: Evidence from Mexico",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14774v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "This paper examines the effects of the 2019 universalization of Mexico's Social Pension Program (PAM), one of the country's most expansive and politically salient social programs. The reform simultaneously increased the cash transfer and extended eligibility to all individuals aged 65 and over, regardless of income or contributory pension status. Using nationally representative data from the ENIGH and a triple-differences (DDD) identification strategy, we estimate the causal effect of the universalization on poverty and labor market outcomes. Our empirical approach exploits variation across time (pre- and post-reform), age (eligible vs. ineligible), and pension scheme status (non-contributory vs. contributory), allowing us to separate the effects of expanded eligibility from those of increased benefit levels. We find strong increases in take-up rates and no significant change in overall poverty rates, suggesting that many new beneficiaries were not economically vulnerable. However, we document a surprising increase in extreme poverty, concentrated among low-income elderly who responded to the reform by exiting the labor force. This reduction in labor supply, driven by a significant drop in employment among individuals in the bottom income quartile, suggests that the pension acted as a substitute for labor income rather than a supplement. Taken together, the results highlight the trade-offs inherent in universal pension programs: while broader access reduces administrative exclusion, extending transfers to economically secure individuals may dilute redistributive impacts and generate behavioral responses that offset potential welfare gains.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14774v1",
        "authors": [
          "Oscar Galvez-Soriano",
          "Raymundo Ramirez Peralta"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.679190",
      "entities": [
        "Social Pension Program",
        "ENIGH",
        "NIST",
        "Act",
        "EPA",
        "PAM",
        "WHO",
        "DDD",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14670v1",
      "title": "FactorMiner: A Self-Evolving Agent with Skills and Experience Memory for Financial Alpha Discovery",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14670v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Formulaic alpha factor mining is a critical yet challenging task in quantitative investment, characterized by a vast search space and the need for domain-informed, interpretable signals. However, finding novel signals becomes increasingly difficult as the library grows due to high redundancy. We propose FactorMiner, a lightweight and flexible self-evolving agent framework designed to navigate this complex landscape through continuous knowledge accumulation. FactorMiner combines a Modular Skill Architecture that encapsulates systematic financial evaluation into executable tools with a structured Experience Memory that distills historical mining trials into actionable insights (successful patterns and failure constraints). By instantiating the Ralph Loop paradigm -- retrieve, generate, evaluate, and distill -- FactorMiner iteratively uses memory priors to guide exploration, reducing redundant search while focusing on promising directions. Experiments on multiple datasets across different assets and Markets show that FactorMiner constructs a diverse library of high-quality factors with competitive performance, while maintaining low redundancy among factors as the library scales. Overall, FactorMiner provides a practical approach to scalable discovery of interpretable formulaic alpha factors under the \"Correlation Red Sea\" constraint.",
        "keywords": [
          "q-fin.TR",
          "cs.MA"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14670v1",
        "authors": [
          "Yanlong Wang",
          "Jian Xu",
          "Hongkang Zhang",
          "Shao-Lun Huang",
          "Danny Dongning Sun"
        ],
        "arxiv_categories": [
          "q-fin.TR",
          "cs.MA"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.679459",
      "entities": [
        "Financial Alpha Discovery Formulaic",
        "Modular Skill Architecture",
        "Correlation Red Sea",
        "Experience Memory",
        "Evolving Agent",
        "Ralph Loop",
        "Framework",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14455v1",
      "title": "How Well Are State-Dependent Local Projections Capturing Nonlinearities?",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14455v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We evaluate how well state-dependent local projections recover true impulse responses in nonlinear environments. Using quadratic vector autoregressions as a laboratory, we show that linear local projections fail to capture any nonlinearities when shocks are symmetrically distributed. Popular state-dependent local projections specifications capture distinct aspects of nonlinearity: those interacting shocks with their signs capture higher-order effects, while those interacting shocks with lagged states capture state dependence. However, their gains over linear specifications are concentrated in tail shocks or tail states; and, for lag-based specifications, hinge on how well the chosen observable proxies the latent state. Our proposed specification-which augments the linear specification with a squared shock term and an interaction between the shock and lagged observables-best approximates the true responses across the entire joint distribution of shocks and states. An application to monetary policy reveals economically meaningful state dependence, whereas higher-order effects, though statistically significant, prove economically modest.",
        "keywords": [
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14455v1",
        "authors": [
          "Zhiheng You"
        ],
        "arxiv_categories": [
          "econ.GN"
        ],
        "steeps_mapping": "E_Economic"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:50.679690",
      "entities": [
        "Dependent Local Projections Capturing",
        "How Well Are State",
        "Laboratory",
        "Policy",
        "Act",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.16633v1",
      "title": "Behavioral change models for infectious disease transmission: a systematic review (2020-2025)",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16633v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Background: Human behavior shapes infectious disease dynamics, yet its integration into transmission models remains fragmented. Recent epidemics, particularly COVID-19, highlight the need for models capturing adaptation to perceived risk, social influence, and policy signals. This review synthesizes post-2020 models incorporating behavioral adaptation, examines their theoretical grounding, and evaluates how behavioral constructs modify transmission, vaccination, and compliance. Methods: Following PRISMA guidelines, we searched Scopus and PubMed (2020-2025), screening 1,274 records with citation chaining. We extracted data on disease context, country, modeling framework, behavioral mechanisms (prevalence-dependent, policy/media, imitation/social learning), and psychosocial constructs (personal threat, coping appraisal, barriers, social norms, cues to action). A total of 216 studies met inclusion criteria. Results: COVID-19 accounted for 73% of studies. Most used compartmental ODE models (81%) and focused on theoretical or U.S. settings. Behavioral change was mainly reactive: 47% applied prevalence-dependent feedback, 25% included awareness/media dynamics, and 19% relied on exogenous policy triggers. Game-theoretic or social learning approaches were rare (less or equal than 5%). Behavioral effects primarily modified contact or transmission rates (91%). Psychosocial constructs were unevenly represented: cues to action (n=159) and personal threat (n=145) dominated, whereas coping appraisal (n=82), barriers (n=36), and social norms (n=25) were less common. Conclusions: We propose a taxonomy structured by behavioral drivers, social scale, and memory to clarify dominant paradigms and their empirical basis. Mapping models to psychosocial constructs provides guidance for more theory-informed and data grounded-integration of behavioral processes in epidemiological modeling.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16633v1",
        "authors": [
          "Youngji Jo",
          "Sileshi Sintayehu Sharbayta",
          "Bruno Buonomo"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.680841",
      "entities": [
        "Guideline",
        "Framework",
        "COVID-19",
        "Policy",
        "PRISMA",
        "COVID",
        "Act",
        "MIT",
        "ODE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16515v1",
      "title": "Generative deep learning improves reconstruction of global historical climate records",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16515v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Accurate assessment of anthropogenic climate change relies on historical instrumental data, yet observations from the early 20th century are sparse, fragmented, and uncertain. Conventional reconstructions rely on disparate statistical interpolation, which excessively smooths local features and creates unphysical artifacts, leading to systematic underestimation of intrinsic variability and extremes. Here, we present a unified, probabilistic generative deep learning framework that overcomes these limitations and reveals previously unresolved historical climate variability back to 1850. Leveraging a learned generative prior of Earth system dynamics, our model performs probabilistic inference to recover spatiotemporally consistent historical temperature and precipitation fields from sparse observations. Our approach preserves the higher-order statistics of climate dynamics, transforming reconstruction into a robust uncertainty-aware assessment. We demonstrate that our reconstruction overcomes pronounced biases in widely used historical reference products, including those underlying IPCC assessments, especially regarding extreme weather events. Notably, we uncover higher early 20th-century global warming levels compared to existing reconstructions, primarily driven by more pronounced polar warming, with mean Arctic warming trends exceeding established benchmarks by 0.15--0.29°C per decade for 1900--1980. Conversely, for the modern era, our reconstruction indicates that the broad Arctic warming trend is likely overestimated in recent assessments, yet explicitly resolves previously unrecognized intense, localized hotspots in the Barents Sea and Northeastern Greenland. Furthermore, based on our seamless global reconstruction that recovers precipitation variability across the oceans and under-monitored regions, we uncover an intensification of the global hydrological cycle.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16515v1",
        "authors": [
          "Zhen Qian",
          "Teng Liu",
          "Sebastian Bathiany",
          "Shangshang Yang",
          "Philipp Hess"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.681909",
      "entities": [
        "Northeastern Greenland",
        "Deep Learning",
        "Barents Sea",
        "Framework",
        "IPCC",
        "Act",
        "MIT",
        "IoT",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16447v1",
      "title": "Evolutionary Advantage of Diversity-Generating Retroelements in Switching Environments",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16447v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Diversity-Generating Retroelements (DGRs) create rapid, targeted variation within specific genomic regions in phages and bacteria. They operate through stochastic retro-transcription of a template region (TR) into a variable region (VR), which typically encodes ligand-binding proteins. Despite their prevalence, the evolutionary conditions that maintain such hypermutating systems remain unclear. Here we introduce a two-timescale framework separating fast VR diversification from slow TR evolution, allowing the dynamics of DGR-controlled loci to be analytically understood from the TR design point of view. We quantity the fitness gain provided by the diversification mechanism of DGR in the presence of environmental switching with respect to standard mutagenesis. Our framework accounts for observed patterns of DGR activity in human-gut \\textit{Bacteroides} and clarifies when constitutive DGR activation is evolutionarily favored.",
        "keywords": [
          "q-bio.PE",
          "cond-mat.stat-mech"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16447v1",
        "authors": [
          "Léo Régnier",
          "Paul Rochette",
          "Raphaël Laurenceau",
          "David Bikard",
          "Simona Cocco"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "cond-mat.stat-mech"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.682179",
      "entities": [
        "Switching Environments Diversity",
        "Generating Retroelements",
        "Evolutionary Advantage",
        "Framework",
        "Standard",
        "Act",
        "EPA",
        "DGR",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16403v1",
      "title": "Quantifying the Role of 3D Fault Geometry Complexities on Slow and Fast Earthquakes",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16403v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Traditional models of slow slip events (SSEs) often oversimplify fault geometry, yet imaging studies show that real subduction faults are segmented and complex. We investigate how fault interactions influence slip behavior using 3D quasi-dynamic earthquake sequence simulations of two parallel faults with uniform rate-weakening friction, accelerated with hierarchical matrices. Our results identify four slip regimes-periodic earthquakes, coexisting SSEs and earthquakes, only SSEs, and complex sequences-while a single planar fault under the same conditions produces only earthquakes. We quantify fault interaction using the maximum Coulomb stress induced on a target fault by unit, spatially uniform stress drop on a neighboring fault. Because the source stress drop is normalized, the metric depends only on geometry and is independent of friction and nucleation length, and it can be extended to arbitrary fault configurations. The occurrence of SSEs is confined to an intermediate range of interaction strength. We also reproduce the observed moment-duration scaling and show that it depends on event detection thresholds. These results demonstrate that complex fault geometry can naturally generate both slow and fast earthquakes through evolving traction heterogeneities.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16403v1",
        "authors": [
          "J. Cheng",
          "H. S. Bhat",
          "M. Almakari",
          "B. Lecampion",
          "P. Dubernet"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.682536",
      "entities": [
        "Fast Earthquakes Traditional",
        "Fault Geometry Complexities",
        "Act",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16287v1",
      "title": "Ponomarenko dynamo sustained by a free swirling jet",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16287v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "We present numerical results on dynamo action in a flow driven by an azimuthal body force localized near the end of an elongated cylindrical container. The analysis focuses on the central region of the cylinder, where axial variations in the flow are relatively weak, allowing the magnetic field to be represented as a helically traveling wave. Four magnetic impeller configurations and multiple forcing intensities are examined. In all cases, the velocity profiles in the central region display a similar \\propto r^{-2} dependence across a wide range of Reynolds numbers and forcing region widths. The magnetic field is found to start growing under conditions similar to those of the Riga dynamo. However, the growing modes exhibit a substantial nonzero group velocity, indicating that the associated instability is convective: the flow can amplify an externally applied magnetic field but cannot sustain it autonomously. We outline several approaches for overcoming this limitation in order to realize a working laboratory dynamo based on an internally unconstrained swirling jet-type flow.",
        "keywords": [
          "physics.geo-ph",
          "physics.flu-dyn"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16287v1",
        "authors": [
          "I. Grants",
          "J. Priede"
        ],
        "arxiv_categories": [
          "physics.geo-ph",
          "physics.flu-dyn"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.682789",
      "entities": [
        "Laboratory",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16282v1",
      "title": "Neutral species facilitate coexistence among cyclically competing species under birth and death processes",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16282v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Natural birth and death are fundamental mechanisms of population dynamics in ecosystems and have played pivotal roles in shaping population dynamics. Nevertheless, in studies of cyclic competition systems governed by the rock-paper-scissors (RPS) game, these mechanisms have often been ignored in analyses of biodiversity. On the other hand, given the prevalence and profound impact on biodiversity, understanding how higher-order interactions (HOIs) can affect biodiversity is one of the most challenging issues, and thus HOIs have been continuously studied for their effects on biodiversity in systems of cyclic competing populations, with a focus on neutral species. However, in real ecosystems, species can evolve and die naturally or be preyed upon by predators, whereas previous studies have considered only classic reaction rules among three species with a neutral, nonparticipant species. To identify how neutral species can affect the biodiversity of the RPS system when species' natural birth and death are assumed, we consider a model of neutral species in higher-order interactions within the spatial RPS system, assuming birth-and-death processes. Extensive simulations show that when neutral species interfere positively, they dominate the available space, thereby reducing the proportion of other species. Conversely, when the interference is harmful, the density of competing species increases. In addition, unlike traditional RPS dynamics, biodiversity can be effectively maintained even in high-mobility regimes. Our study reaffirms the critical role of neutral species in preserving biodiversity.",
        "keywords": [
          "q-bio.PE",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16282v1",
        "authors": [
          "Yikang Lu",
          "Wenhao She",
          "Xiaofang Duan",
          "Junpyo Park"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "physics.soc-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.683140",
      "entities": [
        "Act",
        "RPS",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16090v1",
      "title": "Examining Fast Radiative Feedbacks Using Machine-Learning Weather Emulators",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16090v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "The response of the climate system to increased greenhouse gases and other radiative perturbations is governed by a combination of fast and slow feedbacks. Slow feedbacks are typically activated in response to changes in ocean temperatures on decadal timescales and manifest as changes in climatic state with no recent historical analogue. However, fast feedbacks are activated in response to rapid atmospheric physical processes on weekly timescales, and they are already operative in the present-day climate. This distinction implies that the physics of fast radiative feedbacks is present in the historical meteorological reanalyses used to train many recent successful machine-learning-based (ML) emulators of weather and climate. In addition, these feedbacks are functional under the historical boundary conditions pertaining to the top-of-atmosphere radiative balance and sea-surface temperatures. Together, these factors imply that we can use historically trained ML weather emulators to study the response of radiative-convective equilibrium (RCE), and hence the global hydrological cycle, to perturbations in carbon dioxide and other well-mixed greenhouse gases. Without retraining on prospective Earth system conditions, we use ML weather emulators to quantify the fast precipitation response to reduced and elevated carbon dioxed concentrations with no recent historical precedent. We show that the responses from historically trained emulators agree with those produced by full-physics Earth System Models (ESMs). In conclusion, we discuss the prospects for and advantages from using ESMs and ML emulators to study fast processes in global climate.",
        "keywords": [
          "physics.ao-ph",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16090v1",
        "authors": [
          "Ankur Mahesh",
          "William D. Collins",
          "Travis A. O'Brien",
          "Paul B. Goddard",
          "Sinclaire Zebaze"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "cs.LG"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.683502",
      "entities": [
        "Examining Fast Radiative Feedbacks",
        "Earth System Models",
        "Using Machine",
        "Act",
        "RCE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16059v1",
      "title": "Properties of biodiversity indices that model future extinction risk",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16059v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "The loss of biodiversity due to the likely widespread extinction of species in the near future is a focus of current concern in conservation biology. One approach to measure the impact of this extinction is based on the predicted loss of phylogenetic diversity. These predictions have become a focus of the Zoological Society of London's 'EDGE2' program for quantifying biodiversity loss and involves considering the HED (heightened evolutionary distinctiveness) and HEDGE (heightened evolutionary distinctiveness and globally endangered) indices. Here, we show how to generalise the HED(GE) indices by expanding their application to more general settings (to phylogenetic networks, to feature diversity on discrete traits, and to arbitrary biodiversity measures). We provide a simple and explicit description of the mean and variance of such measures, and illustrate our results by an application to the phylogeny of all 27 extant Crocodilians. We also derive various equalities for feature diversity, and an inequality if species extinction rates are correlated with feature types.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16059v1",
        "authors": [
          "Mike Steel",
          "Kristina Wicke",
          "Arne Mooers"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.683750",
      "entities": [
        "Zoological Society",
        "HEDGE",
        "Act",
        "HED",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.16022v1",
      "title": "The lingering phenomenon and pattern formation in a nonlocal population model with cognitive map",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16022v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "The rates at which individuals memorize and forget environmental information strongly influence their movement paths and long-term space use. To understand how these cognitive time scales shape population-level patterns, we propose and analyze a nonlocal population model with a cognitive map. The population density moves by a Fokker--Planck type diffusion driven by a cognitive map that stores a habitat quality information nonlocally. The map is updated through local presence with learning and forgetting rates, and we consider both truncated and normalized perception kernels. We first study the movement-only system without growth. We show that finite perceptual range generates spatial heterogeneity in the cognitive map even in nearly homogeneous habitats, and we prove a lingering phenomenon on unimodal landscapes: for the fixed high learning rate, the peak density near the best location is maximized at an intermediate forgetting rate. We then couple cognitive diffusion to logistic growth. We establish local well-posedness and persistence by proving instability of the extinction equilibrium and the existence of a positive steady state, with uniqueness under an explicit condition on the motility function. Numerical simulations show that lingering persists under logistic growth and reveal a trade-off between the lingering and total population size, since near the strongest-lingering regime the total mass can fall below the total resource, in contrast to classical random diffusive--logistic models.",
        "keywords": [
          "math.AP",
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16022v1",
        "authors": [
          "Kyung-Han Choi",
          "Thomas Hillen"
        ],
        "arxiv_categories": [
          "math.AP",
          "q-bio.PE"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.684086",
      "entities": [
        "Fusion",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15993v1",
      "title": "An Interpretable Physics Informed Multi-Stream Deep Learning Architecture for the Discrimination between Earthquake, Quarry Blast and Noise",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15993v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "The reliable discrimination of tectonic earthquakes from anthropogenic quarry blasts and transient noise remains a critical challenge in single station seismic monitoring. In this study, we introduce a novel Physics Informed Convolutional Recurrent Neural Network (PI CRNN) that embeds seismological domain knowledge directly into the feature extraction and learning process. We adapt a multistream architecture with three parallel encoders: (i) Time Domain: SincNet Encoder, (ii) MultiResolution Spectrogram branch, and, (iii) Physics Branch, followed by a fusion and a bidirectionalLSTM module. Evaluated on the Curated Pacific Northwest AI ready Seismic Dataset, the PI CRNN achieves an overall classification accuracy of 97.56 percent on an independent test set. It outperforms a standard CRNN baseline, a classical P to S amplitude ratio method, and a Physics Informed Neural Network (PINN) that enforces physical constraints via the loss function. Furthermore, the model demonstrates perfect precision in noise rejection (100 percent Recall). Interpretability analysis using saliency maps confirms that the architecture successfully learns distinct physical signatures, identifying bimodal P- and S-wave arrivals for earthquakes versus singular impulsive onsets for blasts. This work establishes a scalable, transparent framework for AI-driven seismology, proving that architectural inductive bias provides an alternative reliable approach compared to purely data-driven approaches.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15993v1",
        "authors": [
          "Nishtha Srivastava",
          "Johannes Faber",
          "Dhruv Aditya Srivastava"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.684450",
      "entities": [
        "Physics Informed Convolutional Recurrent",
        "Stream Deep Learning Architecture",
        "An Interpretable Physics Informed",
        "Physics Informed Neural Network",
        "Curated Pacific Northwest",
        "Seismic Dataset",
        "Neural Network",
        "Physics Branch",
        "Deep Learning",
        "Quarry Blast",
        "Time Domain",
        "Framework",
        "Standard",
        "Fusion",
        "PINN"
      ]
    },
    {
      "id": "arxiv-2602.15957v1",
      "title": "Evolutionary Systems Thinking -- From Equilibrium Models to Open-Ended Adaptive Dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15957v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Complex change is often described as \"evolutionary\" in economics, policy, and technology, yet most system dynamics models remain constrained to fixed state spaces and equilibrium-seeking behavior. This paper argues that evolutionary dynamics should be treated as a core system-thinking problem rather than as a biological metaphor. We introduce Stability-Driven Assembly (SDA) as a minimal, non-equilibrium framework in which stochastic interactions combined with differential persistence generate endogenous selection without genes, replication, or predefined fitness functions. In SDA, longer-lived patterns accumulate in the population, biasing future interactions and creating feedback between population composition and system dynamics. This feedback yields fitness-proportional sampling as an emergent property, realizing a natural genetic algorithm driven solely by stability. Using SDA, we demonstrate why equilibrium-constrained models, even when simulated numerically, cannot exhibit open-ended evolution: evolutionary systems require population-dependent, non-stationary dynamics in which structure and dynamics co-evolve. We conclude by discussing implications for system dynamics, economics, and policy modeling, and outline how agent-based and AI-enabled approaches may support evolutionary models capable of sustained novelty and structural emergence.",
        "keywords": [
          "q-bio.PE",
          "cs.NE",
          "econ.TH"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15957v1",
        "authors": [
          "Dan Adler"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "cs.NE",
          "econ.TH"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.684748",
      "entities": [
        "Ended Adaptive Dynamics Complex",
        "Evolutionary Systems Thinking",
        "From Equilibrium Models",
        "Driven Assembly",
        "Framework",
        "Policy",
        "Meta",
        "Act",
        "SDA",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15830v1",
      "title": "Ensemble-size-dependence of deep-learning post-processing methods that minimize an (un)fair score: motivating examples and a proof-of-concept solution",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15830v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Fair scores reward ensemble forecast members that behave like samples from the same distribution as the verifying observations. They are therefore an attractive choice as loss functions to train data-driven ensemble forecasts or post-processing methods when large training ensembles are either unavailable or computationally prohibitive. The adjusted continuous ranked probability score (aCRPS) is fair and unbiased with respect to ensemble size, provided forecast members are exchangeable and interpretable as conditionally independent draws from an underlying predictive distribution. However, distribution-aware post-processing methods that introduce structural dependency between members can violate this assumption, rendering aCRPS unfair. We demonstrate this effect using two approaches designed to minimize the expected aCRPS of a finite ensemble: (1) a linear member-by-member calibration, which couples members through a common dependency on the sample ensemble mean, and (2) a deep-learning method, which couples members via transformer self-attention across the ensemble dimension. In both cases, the results are sensitive to ensemble size and apparent gains in aCRPS can correspond to systematic unreliability characterized by over-dispersion. We introduce trajectory transformers as a proof-of-concept that ensemble-size independence can be achieved. This approach is an adaptation of the Post-processing Ensembles with Transformers (PoET) framework and applies self-attention over lead time while preserving the conditional independence required by aCRPS. When applied to weekly mean $T_{2m}$ forecasts from the ECMWF subseasonal forecasting system, this approach successfully reduces systematic model biases whilst also improving or maintaining forecast reliability regardless of the ensemble size used in training (3 vs 9 members) or real-time forecasts (9 vs 100 members).",
        "keywords": [
          "physics.ao-ph",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15830v1",
        "authors": [
          "Christopher David Roberts"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "cs.LG"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.685171",
      "entities": [
        "Transformer",
        "Framework",
        "ECMWF",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15768v1",
      "title": "Southern Ocean latent heat flux variability driven by oceanic meso- and submesoscale motions",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15768v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Latent heat flux is a primary pathway for ocean-atmosphere exchange of heat and moisture, yet the influence of sea surface temperature variability at fine scales ($\\leq$ 100 km) on latent heat flux variability, particularly over the Southern Ocean, remains poorly understood. Here we quantify the scale-dependent drivers of latent heat flux (LHF) variability using a year-long, global, fully coupled ocean-atmosphere simulation with kilometer-scale resolution. Annual-mean LHF in eddy-rich regions reaches $\\approx$ 215 W m$^{-2}$, approximately three times larger than in eddy-poor regions. Spectral analyses show that ocean mesoscale [$\\mathcal{O}$(100 km)] and submesoscale [$\\mathcal{O}$(1-10 km)] variability accounts for up to $\\approx$ 80% of the total LHF variance in eddy-rich sectors, but as little as 10% in eddy-poor regions, and increases proportionally with eddy kinetic energy and sea surface temperature (SST) variance. We also find that strong submesoscale SST fronts ($\\approx$ 5 $^\\circ$C over 10 km) force a localized secondary circulation that extends well above the marine boundary layer into the mid-troposphere. Comparison with ERA5 shows that fine ocean scales, responsible for about 17% of the ocean-driven LHF variance in the simulation, are largely unresolved in the reanalysis, leading to a muted atmospheric response lacking any secondary circulation. Despite a strong heterogeneity in LHF variability, the atmospheric dynamics are mostly uniform across the domain, suggesting a non local atmospheric response to ocean forcing. These results highlight the potential for ocean meso- and submesoscales, commonly under-resolved in climate models and reanalysis, to influence Southern Ocean air-sea coupling and atmosphere both locally and remotely.",
        "keywords": [
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15768v1",
        "authors": [
          "Lucie Reymondet",
          "Lia Siegelman",
          "Luc Lenain"
        ],
        "arxiv_categories": [
          "physics.ao-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.685559",
      "entities": [
        "Southern Ocean",
        "SST",
        "LHF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15447v1",
      "title": "Household size can explain 40% of the variance in cumulative COVID-19 incidence across Europe",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15447v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Household size impacts the spread of respiratory infectious diseases: Larger households tend to boost transmission by acquiring external infections more frequently and subsequently transmitting them back into the community. Furthermore, mandatory interventions primarily modulate contagion between households rather than within them. We developed an approach to quantify the role of household size in epidemics by separating within-household from out-household transmission, and found that household size explains 41% of the variability in cumulative COVID-19 incidence across 34 European countries (95% confidence interval: [15%, 46%]). The contribution of households to the overall dynamics can be quantified by a boost factor that increases with the effective household size, implying that countries with larger households require more stringent interventions to achieve the same levels of containment. This suggests that households constitute a structural (dis-)advantage that must be considered when designing and evaluating mitigation strategies.",
        "keywords": [
          "q-bio.PE",
          "math.DS",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15447v1",
        "authors": [
          "Seba Contreras",
          "Philipp Dönges",
          "Maciej Filinski",
          "Joel Wagner",
          "Viktor Bezborodov"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "math.DS",
          "physics.soc-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.685796",
      "entities": [
        "Europe Household",
        "COVID-19",
        "COVID",
        "Act",
        "EPA",
        "MIT",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15440v1",
      "title": "The First Instrumentally Documented Fall of an Iron Meteorite: atmospheric trajectory and ground impact",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15440v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Iron meteorite falls are rare compared to stony meteorites, and until recently no iron meteorite had a reliably determined pre-atmospheric orbit. This changed on 2020 November 7, when a bright fireball was observed across Sweden and neighboring regions, with optical, acoustic, and seismic detections extending up to 665 km from the trajectory. After a month-long recovery effort, a 13.8 kg iron meteorite was discovered near Ådalen, representing the first instrumentally recorded and recovered fall of its type and the first iron meteorite with a derivable heliocentric orbit; the event also exhibited the lowest terminal height measured for a well-documented fireball. We combine optical, infrasound, and seismic data to reconstruct the luminous trajectory and employ a Monte Carlo model to simulate the dark flight phase and predicted strewn field, while also investigating the plausibility of a ricochet prior to final deposition. Our analysis identifies distinct aerodynamic properties of iron meteoroids compared to stony bodies, including the influence of streamlined shapes and deep regmaglypts on drag and flight stability, underscoring the need to incorporate iron-specific parameters into entry models to constrain atmospheric dynamics and improve recovery predictions for future events.",
        "keywords": [
          "astro-ph.EP",
          "astro-ph.IM",
          "physics.geo-ph",
          "physics.ins-det",
          "physics.pop-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15440v1",
        "authors": [
          "Jarmo Moilanen",
          "Maria Gritsevich",
          "Jaakko Visuri"
        ],
        "arxiv_categories": [
          "astro-ph.EP",
          "astro-ph.IM",
          "physics.geo-ph",
          "physics.ins-det",
          "physics.pop-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.686443",
      "entities": [
        "Iron Meteorite",
        "Monte Carlo",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15262v1",
      "title": "Multi-Arrival Infrasound from Meteoroids: Fragmentation Signatures versus Propagation Effects in a Fine-Scale Layered Atmosphere",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15262v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Infrasonic signatures of meteoroid fragmentation are frequently ambiguous: do multiple arrivals signify a complex breakup or merely the distorting effects of a layered atmosphere? Resolving this ambiguity is critical for accurate energy estimates and source reconstruction. In this study, we address this challenge by analyzing a unique regional dataset of well-constrained meteoroid events observed by the Southern Ontario Meteor Network and the co-located Elginfield Infrasound Array. We employ pseudo-differential parabolic equation (PPE) simulations to quantify how fine-scale gravity-wave structures in the stratosphere and lower thermosphere modify acoustic waveforms at ranges <300 km. Our modeling reveals that while fine-scale layering can stretch signals and generate diffuse oscillatory tails, it does not produce discrete, high-amplitude pulse splitting at ranges below ~140 km. By applying these results to the rare multi-arrival event 20060305, we demonstrate that its distinct double arrival at 100 km range is inconsistent with atmospheric multipathing and provides definitive evidence of separate fragmentation episodes. These findings establish new diagnostic criteria for separating source physics from propagation artifacts, improving the reliability of infrasound as a monitoring tool for natural bolides, space debris re-entries, and catastrophic launch failures.",
        "keywords": [
          "astro-ph.EP",
          "physics.ao-ph",
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15262v1",
        "authors": [
          "Igor P. Chunchuzov",
          "Oleg E. Popov",
          "Elizabeth A. Silber",
          "Sergey N. Kulichkov"
        ],
        "arxiv_categories": [
          "astro-ph.EP",
          "physics.ao-ph",
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.686757",
      "entities": [
        "Scale Layered Atmosphere Infrasonic",
        "Southern Ontario Meteor Network",
        "Elginfield Infrasound Array",
        "Fragmentation Signatures",
        "Propagation Effects",
        "Arrival Infrasound",
        "Act",
        "EPA",
        "PPE",
        "DOE",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15004v1",
      "title": "PDE foundation models are skillful AI weather emulators for the Martian atmosphere",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15004v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.",
        "keywords": [
          "cs.LG",
          "physics.ao-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15004v1",
        "authors": [
          "Johannes Schmude",
          "Sujit Roy",
          "Liping Wang",
          "Theodore van Kessel",
          "Levente Klein"
        ],
        "arxiv_categories": [
          "cs.LG",
          "physics.ao-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.686990",
      "entities": [
        "Act",
        "PDE",
        "GPU",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14905v1",
      "title": "Groundwater feedbacks on ice sheets and subglacial hydrology",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14905v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "The dynamics of many of Antarctica's glaciers are modulated by a hydrological system at the base of the ice. Sedimentary basins beneath the ice bed contribute to the water budget in this hydrological system by discharging or taking up water. However, sedimentary basins are not included in most current models of ice dynamics, and little is known about their effect. In this paper we develop an idealised model of a glacier whose sliding is coupled to a subglacial hydrological system, which includes a sedimentary basin. We find that groundwater discharge (exfiltration) and recharge (infiltration) are controlled by the shape of the ice sheet and of the sedimentary basin, and that exfiltration promotes sliding whereas infiltration hinders it. Overall, the presence of a sedimentary basin leads to thicker and slower-flowing ice in the steady state. We also find that, when the ice sheet is undergoing retreating, groundwater exfiltration can lead to a positive feedback which accelerates this retreat. Our results shed light on the potential role and importance of Antarctic sedimentary basins, and how these might be incorporated into existing models of ice and subglacial hydrology.",
        "keywords": [
          "physics.geo-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14905v1",
        "authors": [
          "Gabriel J. Cairns",
          "Graham P. Benham",
          "Ian J. Hewitt"
        ],
        "arxiv_categories": [
          "physics.geo-ph"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.687252",
      "entities": [
        "WHO",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.14863v1",
      "title": "Quasilocalization under coupled mutation-selection dynamics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.14863v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "When mutations are rampant, quasispecies theory or Eigen's model predicts that the fittest type in a population may not dominate. Beyond a critical mutation rate, the population may even be delocalized completely from the peak of the fitness landscape and the fittest is ironically lost. Extensive efforts have been made to understand this exceptional scenario. But in general, there is no simple prescription that predicts the eventual degree of localization for arbitrary fitness landscapes and mutation rates. Here, we derive a simple and general relation linking the quasispecies' Hill numbers, which are diversity metrics in ecology, and the ratio of an effective fitness variance to the mean mutation rate squared. This ratio, which we call the localization factor, emerges from mean approximations of decomposed surprisal or stochastic entropy change rates. On the side of application, the relation we obtained here defines a combination of Hill numbers that may complement other complexity or diversity measures for real viral quasispecies. Its advantage being that there is an underlying biological interpretation under Eigen's model.",
        "keywords": [
          "q-bio.PE"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.14863v1",
        "authors": [
          "C. J. Palpal-latoc",
          "Ian Vega"
        ],
        "arxiv_categories": [
          "q-bio.PE"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.687509",
      "entities": [
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.15088v1",
      "title": "IT-DPC-SRI: A Cloud-Optimized Archive of Italian Radar Precipitation (2010-2025)",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15088v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "We present IT-DPC-SRI, the first publicly available long-term archive of Italian weather radar precipitation estimates, spanning 16 years (2010--2025). The dataset contains Surface Rainfall Intensity (SRI) observations from the Italian Civil Protection Department's national radar mosaic, harmonized into a coherent Analysis-Ready Cloud-Optimized (ARCO) Zarr datacube. The archive comprises over one million timesteps at temporal resolutions from 15 to 5 minutes, covering a $1200\\times1400$ kilometer domain at 1 kilometer spatial resolution, compressed from 7TB to 51GB on disk. We address the historical fragmentation of Italian radar data - previously scattered across heterogeneous formats (OPERA BUFR, HDF5, GeoTIFF) with varying spatial domains and projections - by reprocessing the entire record into a unified store. The dataset is accessible as a static versioned snapshot on Zenodo, via cloud-native access on the ECMWF European Weather Cloud, and as a continuously updated live version on the ArcoDataHub platform. This release fills a significant gap in European radar data availability, as Italy does not participate in the EUMETNET OPERA pan-European radar composite. The dataset is released under a CC BY-SA 4.0 license.",
        "keywords": [
          "physics.ao-ph",
          "astro-ph.IM",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15088v1",
        "authors": [
          "Gabriele Franch",
          "Elena Tomasi",
          "Uladzislau Azhel",
          "Giacomo Tomezzoli",
          "Alessandro Camilletti"
        ],
        "arxiv_categories": [
          "physics.ao-ph",
          "astro-ph.IM",
          "cs.LG"
        ],
        "steeps_mapping": "E_Environmental"
      },
      "preliminary_category": "E",
      "collected_at": "2026-02-18T19:01:53.687804",
      "entities": [
        "Italian Civil Protection Department",
        "Italian Radar Precipitation",
        "Surface Rainfall Intensity",
        "European Weather Cloud",
        "Optimized Archive",
        "Ready Cloud",
        "OPERA",
        "ECMWF",
        "BUFR",
        "ARCO",
        "EPA",
        "DPC",
        "SRI",
        "DOE",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.16703v1",
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16703v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16703v1",
        "authors": [
          "Shen Zhou Hong",
          "Alex Kleinman",
          "Alyssa Mathiowetz",
          "Adam Howes",
          "Julian Cohen"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.779514",
      "entities": [
        "Novice Performance",
        "Biology Large",
        "Measuring Mid",
        "Laboratory",
        "Mid-2025",
        "LLM",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16695v1",
      "title": "Fairness Dynamics in Digital Economy Platforms with Biased Ratings",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16695v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "The digital services economy consists of online platforms that facilitate interactions between service providers and consumers. This ecosystem is characterized by short-term, often one-off, transactions between parties that have no prior familiarity. To establish trust among users, platforms employ rating systems which allow users to report on the quality of their previous interactions. However, while arguably crucial for these platforms to function, rating systems can perpetuate negative biases against marginalised groups. This paper investigates how to design platforms around biased reputation systems, reducing discrimination while maintaining incentives for all service providers to offer high quality service for users. We introduce an evolutionary game theoretical model to study how digital platforms can perpetuate or counteract rating-based discrimination. We focus on the platforms' decisions to promote service providers who have high reputations or who belong to a specific protected group. Our results demonstrate a fundamental trade-off between user experience and fairness: promoting highly-rated providers benefits users, but lowers the demand for marginalised providers against which the ratings are biased. Our results also provide evidence that intervening by tuning the demographics of the search results is a highly effective way of reducing unfairness while minimally impacting users. Furthermore, we show that even when precise measurements on the level of rating bias affecting marginalised service providers is unavailable, there is still potential to improve upon a recommender system which ignores protected characteristics. Altogether, our model highlights the benefits of proactive anti-discrimination design in systems where ratings are used to promote cooperative behaviour.",
        "keywords": [
          "cs.MA",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16695v1",
        "authors": [
          "J. Martin Smit",
          "Fernando P. Santos"
        ],
        "arxiv_categories": [
          "cs.MA",
          "cs.CY"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.779967",
      "entities": [
        "Digital Economy Platforms",
        "Fairness Dynamics",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16666v1",
      "title": "Towards a Science of AI Agent Reliability",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16666v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16666v1",
        "authors": [
          "Stephan Rabanser",
          "Sayash Kapoor",
          "Peter Kirgis",
          "Kangheng Liu",
          "Saiteja Utpala"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.780201",
      "entities": [
        "Agent Reliability",
        "Standard",
        "Act",
        "EPA",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16616v1",
      "title": "Design and Analysis Strategies for Pooling in High Throughput Screening: Application to the Search for a New Anti-Microbial",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16616v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "A major public health issue is the growing resistance of bacteria to antibiotics. An important part of the needed response is the discovery and development of new antimicrobial strategies. These require the screening of potential new drugs, typically accomplished using high-throughput screening (HTS). Traditionally, HTS is performed by examining one compound per well, but a more efficient strategy pools multiple compounds per well. In this work, we study several recently proposed pooling construction methods, as well as a variety of pooled high-throughput screening analysis methods, in order to provide guidance to practitioners on which methods to use. This is done in the context of an application of the methods to the search for new drugs to combat bacterial infection. We discuss both an extensive pilot study as well as a small screening campaign, and highlight both the successes and challenges of the pooling approach.",
        "keywords": [
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16616v1",
        "authors": [
          "Byran Smucker",
          "Benjamin Brennan",
          "Emily Rego",
          "Meng Wu",
          "Zhihong Lin"
        ],
        "arxiv_categories": [
          "stat.AP"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.780430",
      "entities": [
        "High Throughput Screening",
        "Analysis Strategies",
        "New Anti",
        "Act",
        "IoT",
        "HTS",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16583v1",
      "title": "Physical Activity Trajectories Preceding Incident Major Depressive Disorder Diagnosis Using Consumer Wearable Devices in the All of Us Research Program: Case-Control Study",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16583v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Low physical activity is a known risk factor for major depressive disorder (MDD), but changes in activity before a first clinical diagnosis remain unclear, especially using long-term objective measurements. This study characterized trajectories of wearable-measured physical activity during the year preceding incident MDD diagnosis. We conducted a retrospective nested case-control study using linked electronic health record and Fitbit data from the All of Us Research Program. Adults with at least 6 months of valid wearable data in the year before diagnosis were eligible. Incident MDD cases were matched to controls on age, sex, body mass index, and index time (up to four controls per case). Daily step counts and moderate-to-vigorous physical activity (MVPA) were aggregated into monthly averages. Linear mixed-effects models compared trajectories from 12 months before diagnosis to diagnosis. Within cases, contrasts identified when activity first significantly deviated from levels 12 months prior. The cohort included 4,104 participants (829 cases and 3,275 controls; 81.7% women; median age 48.4 years). Compared with controls, cases showed consistently lower activity and significant downward trajectories in both step counts and MVPA during the year before diagnosis (P < 0.001). Significant declines appeared about 4 months before diagnosis for step counts and 5 months for MVPA. Exploratory analyses suggested subgroup differences, including steeper declines in men, greater intensity reductions at older ages, and persistently low activity among individuals with obesity. Sustained within-person declines in physical activity emerged months before incident MDD diagnosis. Longitudinal wearable monitoring may provide early signals to support risk stratification and earlier intervention.",
        "keywords": [
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16583v1",
        "authors": [
          "Yuezhou Zhang",
          "Amos Folarin",
          "Hugh Logan Ellis",
          "Rongrong Zhong",
          "Callum Stewart"
        ],
        "arxiv_categories": [
          "stat.AP"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.780837",
      "entities": [
        "Physical Activity Trajectories Preceding",
        "Incident Major Depressive Disorder",
        "Diagnosis Using Consumer Wearable",
        "Us Research Program",
        "Control Study Low",
        "MVPA",
        "MDD",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16567v1",
      "title": "Scattering and sputtering on the lunar surface; Insights from negative ions observed at the surface",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16567v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Context. Airless planetary bodies are directly exposed to solar wind ions, which can scatter or become implanted upon impact with the regolith-covered surface, while also sputtering surface atoms. Aims. We construct a semi-analytical model for the scattering of ions of hundreds of eV and the sputtering of surface atoms, both resulting in the emission of negative ions from the lunar surface. Our model contains a novel description of the scattering process that is physics-based and constrained by observations. Methods. We use data from the Negative Ions at the Lunar Surface (NILS) instrument on the Chang'e-6 lander to update prior knowledge of ion scattering and sputtering from lunar regolith through Bayesian inference. Results. Our model shows good agreement with the NILS data. A precipitating solar wind proton has roughly a 22% chance of scattering from the lunar surface in any charge state, and about an 8% chance of sputtering a surface hydrogen atom. The resulting ratio of scattered to sputtered hydrogen flux is eta_sc / eta_sp = 1.5 for a proton speed of 300 km/s. We find a high probability (7-20%) that a hydrogen atom leaves the surface negatively charged. The angular emission distributions at near-grazing angles for both scattered and sputtered fluxes are controlled by surface roughness. Our model also indicates significant inelastic energy losses for hydrogen interacting with the regolith, suggesting a longer effective path length than previously assumed. Finally, we estimate a surface binding energy of 5.5 eV, consistent with the observations. Conclusions. Our model describes the scattering and sputtering of particles of any charge state from any homogeneous, multi-species surface. Using NILS data, we successfully applied the model to update our understanding of solar wind interacting with lunar regolith, and the emission of negative hydrogen ions.",
        "keywords": [
          "physics.space-ph",
          "physics.atom-ph",
          "physics.ins-det",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16567v1",
        "authors": [
          "Romain Canu-Blot",
          "Martin Wieser",
          "Umberto Rollero",
          "Thomas Maynadié",
          "Stas Barabash"
        ],
        "arxiv_categories": [
          "physics.space-ph",
          "physics.atom-ph",
          "physics.ins-det",
          "stat.AP"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.781237",
      "entities": [
        "Negative Ions",
        "Lunar Surface",
        "Agreement",
        "Hydrogen",
        "Solar",
        "Wind",
        "NILS",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16561v1",
      "title": "Hidden in Plain Sight: Detecting Illicit Massage Businesses from Mobility Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16561v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Illicit massage businesses (IMBs) masquerade as legitimate massage parlors while facilitating commercial sex and human trafficking. Law enforcement must identify these businesses within a dense population of lawful establishments, but investigative resources are limited and the illicit status of each location is unknown until inspection. Detection methods based on online reviews offer some insight, yet operators can manipulate these signals, leaving covert establishments undetected. IMBs constitute one of the largest segments of indoor sex trafficking in the United States, with an estimated 9,000 establishments. Mobility data offers an alternative to online signals, covering establishments that avoid digital visibility entirely. We derive features from mobility data spanning temporal visitation patterns, dwell times, visitor catchment areas, and demand stability. Because confirmed labels exist only for establishments identified through advertising platforms, we employ positive-unlabeled learning to address the label asymmetry in ground truth. The model achieves 0.97 AUC and 0.84 Average Precision. Four operational signatures characterize high-risk establishments: demand consistency, evening-concentrated visits, compressed service durations, and locally drawn clientele. The model produces risk scores for each business-week observation. Aggregating to the business level, prioritizing the highest-risk 10% of massage establishments captures 53% of known illicit operations, a 5.3-fold improvement over uninformed inspection. We develop a decision-support system that produces calibrated prioritization scores for law enforcement, enabling investigators to concentrate inspections on the highest-risk venues. The operational signatures may resist strategic manipulation because they reflect actual operations rather than online signals that operators can control.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16561v1",
        "authors": [
          "Roya Shomali",
          "Nick Freeman",
          "Greg Bott",
          "Iman Dayarian",
          "Jason Parton"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.781785",
      "entities": [
        "Detecting Illicit Massage Businesses",
        "Mobility Data Illicit",
        "Average Precision",
        "United States",
        "Plain Sight",
        "Act",
        "MIT",
        "AUC",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16553v1",
      "title": "Agentic AI, Medical Morality, and the Transformation of the Patient-Physician Relationship",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16553v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "The emergence of agentic AI marks a new phase in the digital transformation of healthcare. Distinct from conventional generative AI, agentic AI systems are capable of autonomous, goal-directed actions and complex task coordination. They promise to support or even collaborate with clinicians and patients in increasingly independent ways. While agentic AI raises familiar moral concerns regarding safety, accountability, and bias, this article focuses on a less explored dimension: its capacity to transform the moral fabric of healthcare itself. Drawing on the framework of techno-moral change and the three domains of decision, relation and perception, we investigate how agentic AI might reshape the patient-physician relationship and reconfigure core concepts of medical morality. We argue that these shifts, while not fully predictable, demand ethical attention before widespread deployment. Ultimately, the paper calls for integrating ethical foresight into the design and use of agentic AI.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16553v1",
        "authors": [
          "Robert Ranisch",
          "Sabine Salloch"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.782164",
      "entities": [
        "Medical Morality",
        "Framework",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16541v1",
      "title": "From Latent to Observable Position-Based Click Models in Carousel Interfaces",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16541v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Click models are a central component of learning and evaluation in recommender systems, yet most existing models are designed for single ranked-list interfaces. In contrast, modern recommender platforms increasingly use complex interfaces such as carousels, which consist of multiple swipeable lists that enable complex user browsing behaviors. In this paper, we study position-based click models in carousel interfaces and examine optimization methods, model structure, and alignment with user behavior. We propose three novel position-based models tailored to carousels, including the first position-based model without latent variables that incorporates observed examination signals derived from eye tracking data, called the Observed Examination Position-Based Model (OEPBM). We develop a general implementation of these carousel click models, supporting multiple optimization techniques and conduct experiments comparing gradient-based methods with classical approaches, namely expectation-maximization and maximum likelihood estimation. Our results show that gradient-based optimization consistently achieve better click likelihoods. Among the evaluated models, the OEPBM achieves the strongest performance in click prediction and produces examination patterns that most closely align to user behavior. However, we also demonstrate that strong click fit does not imply realistic modeling of user examination and browsing patterns. This reveals a fundamental limitation of click-only models in complex interfaces and the need for incorporating additional behavioral signals when designing click models for carousel-based recommender systems.",
        "keywords": [
          "cs.IR",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16541v1",
        "authors": [
          "Santiago de Leon-Martinez",
          "Robert Moro",
          "Branislav Kveton",
          "Maria Bielikova"
        ],
        "arxiv_categories": [
          "cs.IR",
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.782663",
      "entities": [
        "Observed Examination Position",
        "Carousel Interfaces Click",
        "Observable Position",
        "Based Click Models",
        "From Latent",
        "Based Model",
        "OEPBM",
        "MIT",
        "DOE",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16376v1",
      "title": "Two-way Clustering Robust Variance Estimator in Quantile Regression Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16376v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "We study inference for linear quantile regression with two-way clustered data. Using a separately exchangeable array framework and a projection decomposition of the quantile score, we characterize regime-dependent convergence rates and establish a self-normalized Gaussian approximation. We propose a two-way cluster-robust sandwich variance estimator with a kernel-based density ``bread'' and a projection-matched ``meat'', and prove consistency and validity of inference in Gaussian regimes. We also show an impossibility result for uniform inference in a non-Gaussian interaction regime.",
        "keywords": [
          "econ.EM",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16376v1",
        "authors": [
          "Ulrich Hounyo",
          "Jiahao Lin"
        ],
        "arxiv_categories": [
          "econ.EM",
          "stat.AP"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.782879",
      "entities": [
        "Clustering Robust Variance Estimator",
        "Quantile Regression Models We",
        "Framework",
        "Act",
        "EPA",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16352v1",
      "title": "Machine Learning in Epidemiology",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16352v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "In the age of digital epidemiology, epidemiologists are faced by an increasing amount of data of growing complexity and dimensionality. Machine learning is a set of powerful tools that can help to analyze such enormous amounts of data. This chapter lays the methodological foundations for successfully applying machine learning in epidemiology. It covers the principles of supervised and unsupervised learning and discusses the most important machine learning methods. Strategies for model evaluation and hyperparameter optimization are developed and interpretable machine learning is introduced. All these theoretical parts are accompanied by code examples in R, where an example dataset on heart disease is used throughout the chapter.",
        "keywords": [
          "stat.ML",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16352v1",
        "authors": [
          "Marvin N. Wright",
          "Lukas Burk",
          "Pegah Golchian",
          "Jan Kapar",
          "Niklas Koenen"
        ],
        "arxiv_categories": [
          "stat.ML",
          "cs.CY",
          "cs.LG"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.783125",
      "entities": [
        "Machine Learning",
        "Epidemiology In",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16323v1",
      "title": "Wearable AR for Restorative Breaks: How Interactive Narrative Experiences Support Relaxation for Young Adults",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16323v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Young adults often take breaks from screen-intensive work by consuming digital content on mobile phones, which undermines rest through visual fatigue and inactivity. We introduce a design framework that embeds light break activities into media content on AR smart glasses, balancing engagement and recovery. The framework employs three strategies: (1) seamlessly guiding users by embedding activity cues aligned with media elements; (2) transitioning to audio-centric formats to reduce visual load while sustaining immersion; and (3) structuring sessions with \"rise-peak-closure\" pacing for smooth transitions. In a within-subjects study (N = 16) comparing passive viewing, reminder-based breaks, and non-narrative activities, InteractiveBreak instantiated from our framework seamlessly guided activities, sustained engagement, and enhanced break quality. These findings demonstrate wearable AR's potential to support restorative relaxation by transforming breaks into engaging and meaningful experiences.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16323v1",
        "authors": [
          "Jindu Wang",
          "Runze Cai",
          "Shuchang Xu",
          "Tianrui Hu",
          "Huamin Qu"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.783473",
      "entities": [
        "How Interactive Narrative Experiences",
        "Restorative Breaks",
        "Support Relaxation",
        "Young Adults Young",
        "Framework",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16310v1",
      "title": "Introducing the b-value: combining unbiased and biased estimators from a sensitivity analysis perspective",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16310v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "In empirical research, when we have multiple estimators for the same parameter of interest, a central question arises: how do we combine unbiased but less precise estimators with biased but more precise ones to improve the inference? Under this setting, the point estimation problem has attracted considerable attention. In this paper, we focus on a less studied inference question: how can we conduct valid statistical inference in such settings with unknown bias? We propose a strategy to combine unbiased and biased estimators from a sensitivity analysis perspective. We derive a sequence of confidence intervals indexed by the magnitude of the bias, which enable researchers to assess how conclusions vary with the bias levels. Importantly, we introduce the notion of the b-value, a critical value of the unknown maximum relative bias at which combining estimators does not yield a significant result. We apply this strategy to three canonical combined estimators: the precision-weighted estimator, the pretest estimator, and the soft-thresholding estimator. For each estimator, we characterize the sequence of confidence intervals and determine the bias threshold at which the conclusion changes. Based on the theory, we recommend reporting the b-value based on the soft-thresholding estimator and its associated confidence intervals, which are robust to unknown bias and achieve the lowest worst-case risk among the alternatives.",
        "keywords": [
          "stat.ME",
          "econ.EM",
          "math.ST",
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16310v1",
        "authors": [
          "Zhexiao Lin",
          "Peter J. Bickel",
          "Peng Ding"
        ],
        "arxiv_categories": [
          "stat.ME",
          "econ.EM",
          "math.ST",
          "stat.AP"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.783915",
      "entities": [
        "Act",
        "DOE",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16307v1",
      "title": "Generative AI Usage of University Students: Navigating Between Education and Business",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16307v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "This study investigates generative artificial intelligence (GenAI) usage of university students who study alongside their professional career. Previous literature has paid little attention to part-time students and the intersectional use of GenAI between education and business. This study examines with a grounded theory approach the characteristics of GenAI usage of part-time students. Eleven students from a distance learning university were interviewed. Three causal and four intervening conditions, as well as strategies were identified, to influence the use of GenAI. The study highlights both the potential and challenges of GenAI usage in education and business. While GenAI can significantly enhance productivity and learning outcomes, concerns about ethical implications, reliability, and the risk of academic misconduct persist. The developed grounded model offers a comprehensive understanding of GenAI usage among students, providing valuable insights for educators, policymakers, and developers of GenAI tools seeking to bridge the gap between education and business.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16307v1",
        "authors": [
          "Fabian Walke",
          "Veronika Föller"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.784257",
      "entities": [
        "Navigating Between Education",
        "Artificial Intelligence",
        "University Students",
        "University",
        "Policy",
        "Intel",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16302v1",
      "title": "\"What I'm Interested in is Something that Violates the Law\": Regulatory Practitioner Views on Automated Detection of Deceptive Design Patterns",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16302v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Although deceptive design patterns are subject to growing regulatory oversight, enforcement races to keep up with the scale of the problem. One promising solution is automated detection tools, many of which are developed within academia. We interviewed nine experienced practitioners working within or alongside regulatory bodies to understand their work against deceptive design patterns, including the use of supporting tools and the prospect of automation. Computing technologies have their place in regulatory practice, but not as envisioned in research. For example, investigations require utmost transparency and accountability in all the activities we identify as accompanying dark pattern detection, which many existing tools cannot provide. Moreover, tools need to map interfaces to legal violations to be of use. We thus recommend conducting user requirement research to maximize research impact, supporting ancillary activities beyond detection, and establishing practical tech adoption pathways that account for the needs of both scientific and regulatory activities.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16302v1",
        "authors": [
          "Arianna Rossi",
          "Simon Parkin"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.784620",
      "entities": [
        "Deceptive Design Patterns Although",
        "Regulatory Practitioner Views",
        "Automated Detection",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16279v1",
      "title": "Flow on Social Media? Rarer Than You'd Think",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16279v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Researchers often attribute social media's appeal to its ability to elicit flow experiences of deep absorption and effortless engagement. Yet prolonged use has also been linked to distraction, fatigue, and lower mood. This paradox remains poorly understood, in part because prior studies rely on habitual or one-shot reports that ask participants to directly attribute flow to social media. To address this gap, we conducted a five-day field study with 40 participants, combining objective smartphone app tracking with daily reconstructions of flow-inducing activities. Across 673 reported flow occurrences, participants rarely associated flow with social media (2 percent). Instead, heavier social media use predicted fewer daily flow occurrences. We further examine this relationship through the effects of social media use on fatigue, mood, and motivation. Altogether, our findings suggest that flow and social media may not align as closely as assumed - and might even compete - underscoring the need for further research.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16279v1",
        "authors": [
          "Michael T. Knierim",
          "Thimo Schulz",
          "Moritz Schiller",
          "Jwan Shaban",
          "Mario Nadj"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.784940",
      "entities": [
        "Think Researchers",
        "Rarer Than You",
        "Social Media",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16251v1",
      "title": "RelianceScope: An Analytical Framework for Examining Students' Reliance on Generative AI Chatbots in Problem Solving",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16251v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Generative AI chatbots enable personalized problem-solving, but effective learning requires students to self-regulate both how they seek help and how they use AI-generated responses. Considering engagement modes across these two actions reveals nuanced reliance patterns: for example, a student may actively engage in help-seeking by clearly specifying areas of need, yet engage passively in response-use by copying AI outputs, or vice versa. However, existing research lacks systematic tools for jointly capturing engagement across help-seeking and response-use, limiting the analysis of such reliance behaviors. We introduce RelianceScope, an analytical framework that characterizes students' reliance on chatbots during problem-solving. RelianceScope (1) operationalizes reliance into nine patterns based on combinations of engagement modes in help-seeking and response-use, and (2) situates these patterns within a knowledge-context lens that accounts for students' prior knowledge and the instructional significance of knowledge components. Rather than prescribing optimal AI use, the framework enables fine-grained analysis of reliance in open-ended student-AI interactions. As an illustrative application, we applied RelianceScope to analyze chat and code-edit logs from 79 college students in a web programming course. Results show that active help-seeking is associated with active response-use, whereas reliance patterns remain similar across knowledge mastery levels. Students often struggled to articulate their knowledge gaps and to adapt AI responses. Using our annotated dataset as a benchmark, we further demonstrate that large language models can reliably detect reliance during help-seeking and response-use. We conclude by discussing the implications of RelianceScope and the design guidelines for AI-supported educational systems.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16251v1",
        "authors": [
          "Hyoungwook Jin",
          "Minju Yoo",
          "Jieun Han",
          "Zixin Chen",
          "So-Yeon Ahn"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.785500",
      "entities": [
        "Problem Solving Generative",
        "An Analytical Framework",
        "Examining Students",
        "Guideline",
        "Framework",
        "Act",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16201v1",
      "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16201v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives. We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16201v1",
        "authors": [
          "Sanket Badhe",
          "Deep Shah",
          "Nehal Kathrotia"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.785896",
      "entities": [
        "Large Language Models",
        "Implications Large",
        "Tail Knowledge",
        "Framework",
        "LLM",
        "Act",
        "MIT",
        "IoT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16195v1",
      "title": "Phase Transitions in Collective Damage of Civil Structures under Natural Hazards",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16195v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "The fate of cities under natural hazards depends not only on hazard intensity but also on the coupling of structural damage, a collective process that remains poorly understood. Here we show that urban structural damage exhibits phase-transition phenomena. As hazard intensity increases, the system can shift abruptly from a largely safe to a largely damaged state, analogous to a first-order phase transition in statistical physics. Higher diversity in the building portfolio smooths this transition, but multiscale damage clustering traps the system in an extended critical-like regime (analogous to a Griffiths phase), suppressing the emergence of a more predictable disordered (Gaussian) phase. These phenomenological patterns are characterized by a random-field Ising model, with the external field, disorder strength, and temperature interpreted as the effective hazard demand, structural diversity, and modeling uncertainty, respectively. Applying this framework to real urban inventories reveals that widely used engineering modeling practices can shift urban damage patterns between synchronized and volatile regimes, systematically biasing exceedance-based risk metrics by up to 50% under moderate earthquakes ($M_w \\approx 5.5$--$6.0$), equivalent to a several-fold gap in repair costs. This phase-aware description turns the collective behavior of civil infrastructure damage into actionable diagnostics for urban risk assessment and planning.",
        "keywords": [
          "stat.AP"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16195v1",
        "authors": [
          "Sebin Oh",
          "Jinyan Zhao",
          "Raul Rincon",
          "Jamie E. Padgett",
          "Ziqi Wang"
        ],
        "arxiv_categories": [
          "stat.AP"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.786276",
      "entities": [
        "Collective Damage",
        "Phase Transitions",
        "Civil Structures",
        "Framework",
        "Act",
        "EPA",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16157v1",
      "title": "Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16157v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Field studies are irreplaceable but costly, time-consuming, and error-prone, which need careful preparation. Inspired by rapid-prototyping in manufacturing, we propose a fast, low-cost evaluation method using Vision-Language Model (VLM) personas to simulate outcomes comparable to field results. While LLMs show human-like reasoning and language capabilities, autonomous vehicle (AV)-pedestrian interaction requires spatial awareness, emotional empathy, and behavioral generation. This raises our research question: To what extent can VLM personas mimic human responses in field studies? We conducted parallel studies: 1) one real-world study with 20 participants, and 2) one video-study using 20 VLM personas, both on a street-crossing task. We compared their responses and interviewed five HCI researchers on potential applications. Results show that VLM personas mimic human response patterns (e.g., average crossing times of 5.25 s vs. 5.07 s) lack the behavioral variability and depth. They show promise for formative studies, field study preparation, and human data augmentation.",
        "keywords": [
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16157v1",
        "authors": [
          "Xinyue Gui",
          "Ding Xia",
          "Mark Colley",
          "Yuan Li",
          "Vishal Chauhan"
        ],
        "arxiv_categories": [
          "cs.HC"
        ],
        "steeps_mapping": "S_Social"
      },
      "preliminary_category": "S",
      "collected_at": "2026-02-18T19:01:56.786581",
      "entities": [
        "Autonomous Vehicle",
        "Embodied Studies",
        "Language Model",
        "Support Tools",
        "Peeking Ahead",
        "Field Study",
        "LLM",
        "Act",
        "VLM",
        "EPA",
        "HCI",
        "AI"
      ]
    },
    {
      "id": "arxiv-2602.16703v1",
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16703v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16703v1",
        "authors": [
          "Shen Zhou Hong",
          "Alex Kleinman",
          "Alyssa Mathiowetz",
          "Adam Howes",
          "Julian Cohen"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.696424",
      "entities": [
        "Novice Performance",
        "Biology Large",
        "Measuring Mid",
        "Laboratory",
        "Mid-2025",
        "LLM",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16695v1",
      "title": "Fairness Dynamics in Digital Economy Platforms with Biased Ratings",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16695v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "The digital services economy consists of online platforms that facilitate interactions between service providers and consumers. This ecosystem is characterized by short-term, often one-off, transactions between parties that have no prior familiarity. To establish trust among users, platforms employ rating systems which allow users to report on the quality of their previous interactions. However, while arguably crucial for these platforms to function, rating systems can perpetuate negative biases against marginalised groups. This paper investigates how to design platforms around biased reputation systems, reducing discrimination while maintaining incentives for all service providers to offer high quality service for users. We introduce an evolutionary game theoretical model to study how digital platforms can perpetuate or counteract rating-based discrimination. We focus on the platforms' decisions to promote service providers who have high reputations or who belong to a specific protected group. Our results demonstrate a fundamental trade-off between user experience and fairness: promoting highly-rated providers benefits users, but lowers the demand for marginalised providers against which the ratings are biased. Our results also provide evidence that intervening by tuning the demographics of the search results is a highly effective way of reducing unfairness while minimally impacting users. Furthermore, we show that even when precise measurements on the level of rating bias affecting marginalised service providers is unavailable, there is still potential to improve upon a recommender system which ignores protected characteristics. Altogether, our model highlights the benefits of proactive anti-discrimination design in systems where ratings are used to promote cooperative behaviour.",
        "keywords": [
          "cs.MA",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16695v1",
        "authors": [
          "J. Martin Smit",
          "Fernando P. Santos"
        ],
        "arxiv_categories": [
          "cs.MA",
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.696892",
      "entities": [
        "Digital Economy Platforms",
        "Fairness Dynamics",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16666v1",
      "title": "Towards a Science of AI Agent Reliability",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16666v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16666v1",
        "authors": [
          "Stephan Rabanser",
          "Sayash Kapoor",
          "Peter Kirgis",
          "Kangheng Liu",
          "Saiteja Utpala"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.697161",
      "entities": [
        "Agent Reliability",
        "Standard",
        "Act",
        "EPA",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16561v1",
      "title": "Hidden in Plain Sight: Detecting Illicit Massage Businesses from Mobility Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16561v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Illicit massage businesses (IMBs) masquerade as legitimate massage parlors while facilitating commercial sex and human trafficking. Law enforcement must identify these businesses within a dense population of lawful establishments, but investigative resources are limited and the illicit status of each location is unknown until inspection. Detection methods based on online reviews offer some insight, yet operators can manipulate these signals, leaving covert establishments undetected. IMBs constitute one of the largest segments of indoor sex trafficking in the United States, with an estimated 9,000 establishments. Mobility data offers an alternative to online signals, covering establishments that avoid digital visibility entirely. We derive features from mobility data spanning temporal visitation patterns, dwell times, visitor catchment areas, and demand stability. Because confirmed labels exist only for establishments identified through advertising platforms, we employ positive-unlabeled learning to address the label asymmetry in ground truth. The model achieves 0.97 AUC and 0.84 Average Precision. Four operational signatures characterize high-risk establishments: demand consistency, evening-concentrated visits, compressed service durations, and locally drawn clientele. The model produces risk scores for each business-week observation. Aggregating to the business level, prioritizing the highest-risk 10% of massage establishments captures 53% of known illicit operations, a 5.3-fold improvement over uninformed inspection. We develop a decision-support system that produces calibrated prioritization scores for law enforcement, enabling investigators to concentrate inspections on the highest-risk venues. The operational signatures may resist strategic manipulation because they reflect actual operations rather than online signals that operators can control.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16561v1",
        "authors": [
          "Roya Shomali",
          "Nick Freeman",
          "Greg Bott",
          "Iman Dayarian",
          "Jason Parton"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.697622",
      "entities": [
        "Detecting Illicit Massage Businesses",
        "Mobility Data Illicit",
        "Average Precision",
        "United States",
        "Plain Sight",
        "Act",
        "MIT",
        "AUC",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16553v1",
      "title": "Agentic AI, Medical Morality, and the Transformation of the Patient-Physician Relationship",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16553v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "The emergence of agentic AI marks a new phase in the digital transformation of healthcare. Distinct from conventional generative AI, agentic AI systems are capable of autonomous, goal-directed actions and complex task coordination. They promise to support or even collaborate with clinicians and patients in increasingly independent ways. While agentic AI raises familiar moral concerns regarding safety, accountability, and bias, this article focuses on a less explored dimension: its capacity to transform the moral fabric of healthcare itself. Drawing on the framework of techno-moral change and the three domains of decision, relation and perception, we investigate how agentic AI might reshape the patient-physician relationship and reconfigure core concepts of medical morality. We argue that these shifts, while not fully predictable, demand ethical attention before widespread deployment. Ultimately, the paper calls for integrating ethical foresight into the design and use of agentic AI.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16553v1",
        "authors": [
          "Robert Ranisch",
          "Sabine Salloch"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.697857",
      "entities": [
        "Medical Morality",
        "Framework",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16352v1",
      "title": "Machine Learning in Epidemiology",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16352v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "In the age of digital epidemiology, epidemiologists are faced by an increasing amount of data of growing complexity and dimensionality. Machine learning is a set of powerful tools that can help to analyze such enormous amounts of data. This chapter lays the methodological foundations for successfully applying machine learning in epidemiology. It covers the principles of supervised and unsupervised learning and discusses the most important machine learning methods. Strategies for model evaluation and hyperparameter optimization are developed and interpretable machine learning is introduced. All these theoretical parts are accompanied by code examples in R, where an example dataset on heart disease is used throughout the chapter.",
        "keywords": [
          "stat.ML",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16352v1",
        "authors": [
          "Marvin N. Wright",
          "Lukas Burk",
          "Pegah Golchian",
          "Jan Kapar",
          "Niklas Koenen"
        ],
        "arxiv_categories": [
          "stat.ML",
          "cs.CY",
          "cs.LG"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.698032",
      "entities": [
        "Machine Learning",
        "Epidemiology In",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16307v1",
      "title": "Generative AI Usage of University Students: Navigating Between Education and Business",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16307v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "This study investigates generative artificial intelligence (GenAI) usage of university students who study alongside their professional career. Previous literature has paid little attention to part-time students and the intersectional use of GenAI between education and business. This study examines with a grounded theory approach the characteristics of GenAI usage of part-time students. Eleven students from a distance learning university were interviewed. Three causal and four intervening conditions, as well as strategies were identified, to influence the use of GenAI. The study highlights both the potential and challenges of GenAI usage in education and business. While GenAI can significantly enhance productivity and learning outcomes, concerns about ethical implications, reliability, and the risk of academic misconduct persist. The developed grounded model offers a comprehensive understanding of GenAI usage among students, providing valuable insights for educators, policymakers, and developers of GenAI tools seeking to bridge the gap between education and business.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16307v1",
        "authors": [
          "Fabian Walke",
          "Veronika Föller"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.HC"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.698281",
      "entities": [
        "Navigating Between Education",
        "Artificial Intelligence",
        "University Students",
        "University",
        "Policy",
        "Intel",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16201v1",
      "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16201v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives. We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16201v1",
        "authors": [
          "Sanket Badhe",
          "Deep Shah",
          "Nehal Kathrotia"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.698596",
      "entities": [
        "Large Language Models",
        "Implications Large",
        "Tail Knowledge",
        "Framework",
        "LLM",
        "Act",
        "MIT",
        "IoT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16151v1",
      "title": "Queer NLP: A Critical Survey on Literature Gaps, Biases and Trends",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16151v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Natural language processing (NLP) technologies are rapidly reshaping how language is created, processed, and analyzed by humans. With current and potential applications in hiring, law, healthcare, and other areas that impact people's lives, understanding and mitigating harms towards marginalized groups is critical. In this survey, we examine NLP research papers that explicitly address the relationship between LGBTQIA+ communities and NLP technologies. We systematically review all such papers published in the ACL Anthology, to answer the following research questions: (1) What are current research trends? (2) What gaps exist in terms of topics and methods? (3) What areas are open for future work? We find that while the number of papers on queer NLP has grown within the last few years, most papers take a reactive rather than a proactive approach, pointing out bias more often than mitigating it, and focusing on shortcomings of existing systems rather than creating new solutions. Our survey uncovers many opportunities for future work, especially regarding stakeholder involvement, intersectionality, interdisciplinarity, and languages other than English. We also offer an outlook from a queer studies perspective, highlighting understudied topics and gaps in the harms addressed in NLP papers. Beyond being a roadmap of what has been done, this survey is a call to action for work towards more just and inclusive NLP technologies.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16151v1",
        "authors": [
          "Sabine Weber",
          "Angelina Wang",
          "Ankush Gupta",
          "Arjun Subramonian",
          "Dennis Ulmer"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.698922",
      "entities": [
        "Critical Survey",
        "Literature Gaps",
        "Trends Natural",
        "Act",
        "NLP",
        "MIT",
        "ACL",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16080v1",
      "title": "Surgical Activation Steering via Generative Causal Mediation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16080v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.",
        "keywords": [
          "cs.CL",
          "cs.CY",
          "cs.HC",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16080v1",
        "authors": [
          "Aruna Sankaranarayanan",
          "Amir Zur",
          "Atticus Geiger",
          "Dylan Hadfield-Menell"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.CY",
          "cs.HC",
          "cs.LG"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.699155",
      "entities": [
        "Generative Causal Mediation Where",
        "Surgical Activation Steering",
        "Generative Causal Mediation",
        "Act",
        "GCM",
        "NSF"
      ]
    },
    {
      "id": "arxiv-2602.15968v1",
      "title": "From Reflection to Repair: A Scoping Review of Dataset Documentation Tools",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15968v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their adoption. We present a systematic review supported by mixed-methods analysis of 59 dataset documentation publications to examine the motivations behind building documentation tools, how authors conceptualize documentation practices, and how these tools connect to existing systems, regulations, and cultural norms. Our analysis shows four persistent patterns in dataset documentation conceptualization that potentially impede adoption and standardization: unclear operationalizations of documentation's value, decontextualized designs, unaddressed labor demands, and a tendency to treat integration as future work. Building on these findings, we propose a shift in Responsible AI tool design toward institutional rather than individual solutions, and outline actions the HCI community can take to enable sustainable documentation practices.",
        "keywords": [
          "cs.SE",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15968v1",
        "authors": [
          "Pedro Reynolds-Cuéllar",
          "Marisol Wong-Villacres",
          "Adriana Alvarado Garcia",
          "Heila Precel"
        ],
        "arxiv_categories": [
          "cs.SE",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.699412",
      "entities": [
        "Dataset Documentation Tools Dataset",
        "From Reflection",
        "Scoping Review",
        "Regulation",
        "Standard",
        "Act",
        "EPA",
        "HCI",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15638v1",
      "title": "Who Is Doing the Thinking? AI as a Dynamic Cognitive Partner: A Learner-Informed Framework",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15638v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Artificial intelligence is increasingly embedded in education, yet there remains a need to explain how students conceptualize AI's role in their thinking and learning. This study proposes a framework positioning AI as a dynamic cognitive partner whose function shifts across learning situations. Using qualitative analysis of written responses from 133 secondary students in Hong Kong following completion of an AI literacy course, we identified nine interrelated dimensions through which learners described AI as partnering with their cognition: conceptual scaffolding for difficult ideas; feedback and error detection; idea stimulation; cognitive organization; adaptive tutoring support; metacognitive monitoring support; task and cognitive load regulation; learning continuity beyond classroom boundaries; and explanation reframing through representational flexibility during moments of being stuck or overwhelmed. Across these dimensions, students distinguished between productive support that extends understanding and unproductive reliance that replaces cognitive effort, indicating situational awareness of when AI should and should not be used. Grounded in sociocultural theory, distributed cognition, self-regulated learning, and cognitive load perspectives, the framework clarifies how AI becomes integrated into learners' cognitive activity while illuminating the boundary between cognitive extension and substitution.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15638v1",
        "authors": [
          "C. K. Y Chan"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.699720",
      "entities": [
        "Informed Framework Artificial",
        "Dynamic Cognitive Partner",
        "Artificial Intelligence",
        "Who Is Doing",
        "Regulation",
        "Hong Kong",
        "Framework",
        "Intel",
        "Meta",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15476v1",
      "title": "How to Detect Information Voids Using Longitudinal Data from Social Media and Web Searches",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15476v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "The model of the attention economy, where content producers compete for the attention of users, relies on two key forces: information supply and demand. This study leverages the feedback loop between these forces to develop a method for detecting and quantifying information voids, i.e., periods in which little or no reliable information is available on a given topic. Using a case study on COVID-19 vaccines rollout in six European countries, and drawing on data from multiple platforms including Facebook, Google, Twitter, Wikipedia, and online news outlets, we examine how information voids emerge, persist and correlate with a decline in the proportion of high-quality information circulating online. By conceptualising information voids as a specific regime of information spreading, we also quantify their counterpart, information overabundance, which constitute a central component of the current definition of infodemic. We show that information voids are associated with a higher prevalence of misinformation, thus representing problematic hotspots in which individuals are more likely to be misled by low-quality online content. Overall, our findings provide empirical support for the inclusion of information voids in mechanistic explanations of misinformation emergence.",
        "keywords": [
          "cs.CY",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15476v1",
        "authors": [
          "Irene Scalco",
          "Francesco Gesualdo",
          "Roy Cerqueti",
          "Matteo Cinelli"
        ],
        "arxiv_categories": [
          "cs.CY",
          "physics.soc-ph"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.700011",
      "entities": [
        "Detect Information Voids Using",
        "Longitudinal Data",
        "Social Media",
        "COVID-19",
        "Vaccine",
        "Google",
        "COVID",
        "NIST",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15439v1",
      "title": "Algorithmic Approaches to Opinion Selection for Online Deliberation: A Comparative Study",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15439v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "During deliberation processes, mediators and facilitators typically need to select a small and representative set of opinions later used to produce digestible reports for stakeholders. In online deliberation platforms, algorithmic selection is increasingly used to automate this process. However, such automation is not without consequences. For instance, enforcing consensus-seeking algorithmic strategies can imply ignoring or flattening conflicting preferences, which may lead to erasing minority voices and reducing content diversity. More generally, across the variety of existing selection strategies (e.g., consensus, diversity), it remains unclear how each approach influences desired democratic criteria such as proportional representation. To address this gap, we benchmark several algorithmic approaches in this context. We also build on social choice theory to propose a novel algorithm that incorporates both diversity and a balanced notion of representation in the selection strategy. We find empirically that while no single strategy dominates across all democratic desiderata, our social-choice-inspired selection rule achieves the strongest trade-off between proportional representation and diversity.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.SI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15439v1",
        "authors": [
          "Salim Hafid",
          "Manon Berriche",
          "Jean-Philippe Cointet"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.SI"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.700289",
      "entities": [
        "Comparative Study During",
        "Algorithmic Approaches",
        "Online Deliberation",
        "Opinion Selection",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15432v1",
      "title": "From Earthquake Solidarity to Educational Equity: Conceptualizing a Sustainable, Volunteer-Driven P2P Learning Ecosystem at Scale",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15432v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "This study examines the evolution of a grassroots, volunteer-driven peer-to-peer (P2P) educational initiative from an emergency response to the 2023 Türkiye earthquake into a sustainable ecosystem that operated for over two years and supported 300+ middle-school learners with 40+ volunteer tutors. Employing an interpretive case study approach, we triangulated data from participant observation, focus groups, questionnaires, and collaborative visioning workshops to investigate the socio-technical dynamics enabling long-term resilience in a fully online, nonreciprocal far-peer tutoring setting. Our findings reveal that while age proximity fosters trust and open communication, it also poses challenges for tutors who must balance peer rapport with instructional authority. Volunteer engagement is driven primarily by intrinsic motives - educational impact and community belonging - while optional micro-earning is envisioned as a practical enabler for long-term sustainability. Tutees report significant gains in confidence, self-expression, and accelerated comprehension, attributing these outcomes to personalized, interactive sessions within a \"family-like\" safe space that combines academic instruction with socio-emotional support. Notably, tutees view tutors as aspirational role models and express strong intentions to return as tutors themselves, envisioning a self-regenerating cycle of intergenerational reciprocity that carries knowledge and solidarity from generation to generation. Both cohorts call for a dedicated platform featuring integrated scheduling, personalization, feedback, and quality assurance mechanisms. We synthesize these insights into theory-informed implications and five design principles for sustainable P2P learning ecosystems at scale.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15432v1",
        "authors": [
          "Öykü Kaplan",
          "Adam Przybyłek",
          "Michael Neumann",
          "Netta Iivari"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.701143",
      "entities": [
        "From Earthquake Solidarity",
        "Educational Equity",
        "Learning Ecosystem",
        "Act",
        "MIT",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15428v1",
      "title": "What makes an Expert? Comparing Problem-solving Practices in Data Science Notebooks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15428v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "The development of data science expertise requires tacit, process-oriented skills that are difficult to teach directly. This study addresses the resulting challenge of empirically understanding how the problem-solving processes of experts and novices differ. We apply a multi-level sequence analysis to 440 Jupyter notebooks from a public dataset, mapping low-level coding actions to higher-level problem-solving practices. Our findings reveal that experts do not follow fundamentally different transitions between data science phases than novices (e.g., Data Import, EDA, Model Training, Visualization). Instead, expertise is distinguished by the overall workflow structure from a problem-solving perspective and cell-level, fine-grained action patterns. Novices tend to follow long, linear processes, whereas experts employ shorter, more iterative strategies enacted through efficient, context-specific action sequences. These results provide data science educators with empirical insights for curriculum design and assessment, shifting the focus from final products toward the development of the flexible, iterative thinking that defines expertise-a priority in a field increasingly shaped by AI tools.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15428v1",
        "authors": [
          "Manuel Valle Torre",
          "Marcus Specht",
          "Catharine Oertel"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.701416",
      "entities": [
        "Comparing Problem",
        "Model Training",
        "Data Import",
        "Act",
        "EDA",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15371v1",
      "title": "From PhysioNet to Foundation Models -- A history and potential futures",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15371v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Over the last 35 years, the sharing of medical data and models for research has evolved from sneakernet to the internet - from mailing magnetic tapes and compact discs of a handful of well-curated recordings, to the high-speed download of relatively comprehensive hospital databases. More recently, the fervor around the potential for modern machine learning and 'AI' to catapult us into the next industrial revolution has led to a seemingly insatiable desire to pump almost any source of data into large models. Although this has great potential, it also presents a whole set of new challenges. In this article I examine these trends over the last 30 years, drawing on examples from cardiology, one of the oldest data-intensive fields that is undergoing a renaissance via machine learning. From the early days of computerized cardiology, the Research Resource for Complex Physiologic Signals (PhysioNet) has been at the cutting edge of this field. This article, therefore, includes much of the Resource's history and the contributions drawn from 25 years of firsthand experience of co-developing elements of the Resource with its founders. I identify the most promising future directions for the PhysioNet Resource, and more generally, the growing issues and opportunities around dissemination and use of massive physiological databases, associated open access code, and public competitions, along with potential solutions to the key issues facing our field. Topics range from how we should approach foundation models in the context of the rapidly growing AI carbon footprint, to the potential of Tiny-ML and edge computing. I also cover issues around prizes and incentives, funding models, and scientific repeatability, as well as how we might address these issues by leveraging the PhysioNet Challenges, consistent with the philosophy of open-access from the early days of the PhysioNet Resource.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15371v1",
        "authors": [
          "Gari D. Clifford"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.701820",
      "entities": [
        "Complex Physiologic Signals",
        "Foundation Models",
        "Research Resource",
        "Machine Learning",
        "Edge Computing",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15273v1",
      "title": "FrameRef: A Framing Dataset and Simulation Testbed for Modeling Bounded Rational Information Health",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15273v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Information ecosystems increasingly shape how people internalize exposure to adverse digital experiences, raising concerns about the long-term consequences for information health. In modern search and recommendation systems, ranking and personalization policies play a central role in shaping such exposure and its long-term effects on users. To study these effects in a controlled setting, we present FrameRef, a large-scale dataset of 1,073,740 systematically reframed claims across five framing dimensions: authoritative, consensus, emotional, prestige, and sensationalist, and propose a simulation-based framework for modeling sequential information exposure and reinforcement dynamics characteristic of ranking and recommendation systems. Within this framework, we construct framing-sensitive agent personas by fine-tuning language models with framing-conditioned loss attenuation, inducing targeted biases while preserving overall task competence. Using Monte Carlo trajectory sampling, we show that small, systematic shifts in acceptance and confidence can compound over time, producing substantial divergence in cumulative information health trajectories. Human evaluation further confirms that FrameRef's generated framings measurably affect human judgment. Together, our dataset and framework provide a foundation for systematic information health research through simulation, complementing and informing responsible human-centered research. We release FrameRef, code, documentation, human evaluation data, and persona adapter models at https://github.com/infosenselab/frameref.",
        "keywords": [
          "cs.CY",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15273v1",
        "authors": [
          "Victor De Lima",
          "Jiqun Liu",
          "Grace Hui Yang"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.CL"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.702166",
      "entities": [
        "Modeling Bounded Rational Information",
        "Simulation Testbed",
        "Health Information",
        "Using Monte Carlo",
        "Framing Dataset",
        "Framework",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15265v1",
      "title": "From Diagnosis to Inoculation: Building Cognitive Resistance to AI Disempowerment",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15265v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Recent empirical research by Sharma et al. (2026) demonstrated that AI assistant interactions carry meaningful potential for situational human disempowerment, including reality distortion, value judgment distortion, and action distortion. While this work provides a critical diagnosis of the problem, concrete pedagogical interventions remain underexplored. I present an AI literacy framework built around eight cross-cutting Learning Outcomes (LOs), developed independently through teaching practice and subsequently found to align with Sharma et al.'s disempowerment taxonomy. I report a case study from a publicly available online course, where a co-teaching methodology--with AI serving as an active voice co-instructor--was used to deliver this framework. Drawing on inoculation theory (McGuire, 1961)--a well-established persuasion research framework recently applied to misinformation prebunking by the Cambridge school (van der Linden, 2022; Roozenbeek & van der Linden, 2019)--I argue that AI literacy cannot be acquired through declarative knowledge alone, but requires guided exposure to AI failure modes, including the sycophantic validation and authority projection patterns identified by Sharma et al. This application of inoculation theory to AI-specific distortion is, to my knowledge, novel. I discuss the convergence between the pedagogically-derived framework and Sharma et al.'s empirically-derived taxonomy, and argue that this convergence--two independent approaches arriving at similar problem descriptions--strengthens the case for both the diagnosis and the proposed educational response.",
        "keywords": [
          "cs.HC",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15265v1",
        "authors": [
          "Aleksey Komissarov"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.AI",
          "cs.CY"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.702514",
      "entities": [
        "Building Cognitive Resistance",
        "Disempowerment Recent",
        "Learning Outcomes",
        "From Diagnosis",
        "Framework",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15259v1",
      "title": "Knowing Isn't Understanding: Re-grounding Generative Proactivity with Epistemic and Behavioral Insight",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15259v1",
        "published_date": "2026-02-16"
      },
      "content": {
        "abstract": "Generative AI agents equate understanding with resolving explicit queries, an assumption that confines interaction to what users can articulate. This assumption breaks down when users themselves lack awareness of what is missing, risky, or worth considering. In such conditions, proactivity is not merely an efficiency enhancement, but an epistemic necessity. We refer to this condition as epistemic incompleteness: where progress depends on engaging with unknown unknowns for effective partnership. Existing approaches to proactivity remain narrowly anticipatory, extrapolating from past behavior and presuming that goals are already well defined, thereby failing to support users meaningfully. However, surfacing possibilities beyond a user's current awareness is not inherently beneficial. Unconstrained proactive interventions can misdirect attention, overwhelm users, or introduce harm. Proactive agents, therefore, require behavioral grounding: principled constraints on when, how, and to what extent an agent should intervene. We advance the position that generative proactivity must be grounded both epistemically and behaviorally. Drawing on the philosophy of ignorance and research on proactive behavior, we argue that these theories offer critical guidance for designing agents that can engage responsibly and foster meaningful partnerships.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15259v1",
        "authors": [
          "Kirandeep Kaur",
          "Xingda Lyu",
          "Chirag Shah"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.LG"
        ],
        "steeps_mapping": "P_Political"
      },
      "preliminary_category": "P",
      "collected_at": "2026-02-18T19:01:59.702816",
      "entities": [
        "Behavioral Insight Generative",
        "Generative Proactivity",
        "Knowing Isn",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16703v1",
      "title": "Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16703v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a \"typical\" reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.",
        "keywords": [
          "cs.CY",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16703v1",
        "authors": [
          "Shen Zhou Hong",
          "Alex Kleinman",
          "Alyssa Mathiowetz",
          "Adam Howes",
          "Julian Cohen"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.689550",
      "entities": [
        "Novice Performance",
        "Biology Large",
        "Measuring Mid",
        "Laboratory",
        "Mid-2025",
        "LLM",
        "Act",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16695v1",
      "title": "Fairness Dynamics in Digital Economy Platforms with Biased Ratings",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16695v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "The digital services economy consists of online platforms that facilitate interactions between service providers and consumers. This ecosystem is characterized by short-term, often one-off, transactions between parties that have no prior familiarity. To establish trust among users, platforms employ rating systems which allow users to report on the quality of their previous interactions. However, while arguably crucial for these platforms to function, rating systems can perpetuate negative biases against marginalised groups. This paper investigates how to design platforms around biased reputation systems, reducing discrimination while maintaining incentives for all service providers to offer high quality service for users. We introduce an evolutionary game theoretical model to study how digital platforms can perpetuate or counteract rating-based discrimination. We focus on the platforms' decisions to promote service providers who have high reputations or who belong to a specific protected group. Our results demonstrate a fundamental trade-off between user experience and fairness: promoting highly-rated providers benefits users, but lowers the demand for marginalised providers against which the ratings are biased. Our results also provide evidence that intervening by tuning the demographics of the search results is a highly effective way of reducing unfairness while minimally impacting users. Furthermore, we show that even when precise measurements on the level of rating bias affecting marginalised service providers is unavailable, there is still potential to improve upon a recommender system which ignores protected characteristics. Altogether, our model highlights the benefits of proactive anti-discrimination design in systems where ratings are used to promote cooperative behaviour.",
        "keywords": [
          "cs.MA",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16695v1",
        "authors": [
          "J. Martin Smit",
          "Fernando P. Santos"
        ],
        "arxiv_categories": [
          "cs.MA",
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.689947",
      "entities": [
        "Digital Economy Platforms",
        "Fairness Dynamics",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16666v1",
      "title": "Towards a Science of AI Agent Reliability",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16666v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16666v1",
        "authors": [
          "Stephan Rabanser",
          "Sayash Kapoor",
          "Peter Kirgis",
          "Kangheng Liu",
          "Saiteja Utpala"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.LG"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.690182",
      "entities": [
        "Agent Reliability",
        "Standard",
        "Act",
        "EPA",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16561v1",
      "title": "Hidden in Plain Sight: Detecting Illicit Massage Businesses from Mobility Data",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16561v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Illicit massage businesses (IMBs) masquerade as legitimate massage parlors while facilitating commercial sex and human trafficking. Law enforcement must identify these businesses within a dense population of lawful establishments, but investigative resources are limited and the illicit status of each location is unknown until inspection. Detection methods based on online reviews offer some insight, yet operators can manipulate these signals, leaving covert establishments undetected. IMBs constitute one of the largest segments of indoor sex trafficking in the United States, with an estimated 9,000 establishments. Mobility data offers an alternative to online signals, covering establishments that avoid digital visibility entirely. We derive features from mobility data spanning temporal visitation patterns, dwell times, visitor catchment areas, and demand stability. Because confirmed labels exist only for establishments identified through advertising platforms, we employ positive-unlabeled learning to address the label asymmetry in ground truth. The model achieves 0.97 AUC and 0.84 Average Precision. Four operational signatures characterize high-risk establishments: demand consistency, evening-concentrated visits, compressed service durations, and locally drawn clientele. The model produces risk scores for each business-week observation. Aggregating to the business level, prioritizing the highest-risk 10% of massage establishments captures 53% of known illicit operations, a 5.3-fold improvement over uninformed inspection. We develop a decision-support system that produces calibrated prioritization scores for law enforcement, enabling investigators to concentrate inspections on the highest-risk venues. The operational signatures may resist strategic manipulation because they reflect actual operations rather than online signals that operators can control.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16561v1",
        "authors": [
          "Roya Shomali",
          "Nick Freeman",
          "Greg Bott",
          "Iman Dayarian",
          "Jason Parton"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.690580",
      "entities": [
        "Detecting Illicit Massage Businesses",
        "Mobility Data Illicit",
        "Average Precision",
        "United States",
        "Plain Sight",
        "Act",
        "MIT",
        "AUC",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16553v1",
      "title": "Agentic AI, Medical Morality, and the Transformation of the Patient-Physician Relationship",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16553v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "The emergence of agentic AI marks a new phase in the digital transformation of healthcare. Distinct from conventional generative AI, agentic AI systems are capable of autonomous, goal-directed actions and complex task coordination. They promise to support or even collaborate with clinicians and patients in increasingly independent ways. While agentic AI raises familiar moral concerns regarding safety, accountability, and bias, this article focuses on a less explored dimension: its capacity to transform the moral fabric of healthcare itself. Drawing on the framework of techno-moral change and the three domains of decision, relation and perception, we investigate how agentic AI might reshape the patient-physician relationship and reconfigure core concepts of medical morality. We argue that these shifts, while not fully predictable, demand ethical attention before widespread deployment. Ultimately, the paper calls for integrating ethical foresight into the design and use of agentic AI.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16553v1",
        "authors": [
          "Robert Ranisch",
          "Sabine Salloch"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.690844",
      "entities": [
        "Medical Morality",
        "Framework",
        "Act",
        "NSF",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16437v1",
      "title": "Mapping tuberculosis fatalities by region and age group in South Korea: A dataset for targeted health policy optimization",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16437v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "In South Korea, age-disaggregated tuberculosis (TB) data at the district level are not publicly available due to privacy constraints, limiting fine-scale analyses of healthcare accessibility. To address this limitation, we present a high-resolution, district-level dataset on tuberculosis (TB) fatalities and hospital accessibility in South Korea, covering the years 2014 to 2022 across 228 districts. The dataset is constructed using a reconstruction method that infers age-disaggregated TB cases and fatalities at the district level by integrating province-level age-specific statistics with district-level spatial and demographic data, enabling analyses that account for both spatial heterogeneity and age structure. Building on an existing hospital allocation framework, we extend the objective function to an age-weighted formulation and apply it to the reconstructed dataset to minimize TB fatalities under different age-weighting schemes. We demonstrate that incorporating age structure can give rise to distinct optimized hospital allocation patterns, even when the total number of minimized fatalities is similar, revealing trade-offs between efficiency and demographic targeting. In addition, the dataset supports temporal analyses of TB burden, hospital availability, and demographic variation over time, and provides a testbed for spatial epidemiology and optimization studies that require high-resolution demographic and healthcare data.",
        "keywords": [
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16437v1",
        "authors": [
          "Yongsung Kwon",
          "Deok-Sun Lee",
          "Mi Jin Lee",
          "Seung-Woo Son"
        ],
        "arxiv_categories": [
          "physics.soc-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.691191",
      "entities": [
        "In South Korea",
        "South Korea",
        "Framework",
        "Policy",
        "MIT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16417v1",
      "title": "Network geometry of the Drosophila brain",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16417v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "The recent reconstruction of the Drosophila brain provides a neural network of unprecedented size and level of details. In this work, we study the geometrical properties of this system by applying network embedding techniques to the graph of synaptic connections. Since previous analysis have revealed an inhomogeneous degree distribution, we first employ a hyperbolic embedding approach that maps the neural network onto a point cloud in the two-dimensional hyperbolic space. In general, hyperbolic embedding methods exploit the exponentially growing volume of hyperbolic space with increasing distance from the origin, allowing for an approximately uniform spatial distribution of nodes even in scale-free, small-world networks. By evaluating multiple embedding quality metrics, we find that the network structure is well captured by the resulting two-dimensional hyperbolic embedding, and in fact is more congruent with this representation than with the original neuron coordinates in three-dimensional Euclidean space. In order to examine the network geometry in a broader context, we also apply the well-known Euclidean network embedding approach Node2vec, where the dimension of the embedding space, $d$ can be set arbitrarily. In 3 dimensions, the Euclidean embedding of the network yields lower quality scores compared to the original neuron coordinates. However, as a function of the embedding dimension the scores show an improving tendency, surpassing the level of the 2d hyperbolic embedding roughly at $d=16$, and reaching a maximum around $d=64$. Since network embeddings can serve as valuable inputs for a variety of downstream machine learning tasks, our results offer new perspectives on the structure and representation of this recently revealed and biologically significant neural network.",
        "keywords": [
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16417v1",
        "authors": [
          "Bendegúz Sulyok",
          "Sámuel G. Balogh",
          "Gergely Palla"
        ],
        "arxiv_categories": [
          "physics.soc-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.691581",
      "entities": [
        "Machine Learning",
        "Neural Network",
        "Act",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16352v1",
      "title": "Machine Learning in Epidemiology",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16352v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "In the age of digital epidemiology, epidemiologists are faced by an increasing amount of data of growing complexity and dimensionality. Machine learning is a set of powerful tools that can help to analyze such enormous amounts of data. This chapter lays the methodological foundations for successfully applying machine learning in epidemiology. It covers the principles of supervised and unsupervised learning and discusses the most important machine learning methods. Strategies for model evaluation and hyperparameter optimization are developed and interpretable machine learning is introduced. All these theoretical parts are accompanied by code examples in R, where an example dataset on heart disease is used throughout the chapter.",
        "keywords": [
          "stat.ML",
          "cs.CY",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16352v1",
        "authors": [
          "Marvin N. Wright",
          "Lukas Burk",
          "Pegah Golchian",
          "Jan Kapar",
          "Niklas Koenen"
        ],
        "arxiv_categories": [
          "stat.ML",
          "cs.CY",
          "cs.LG"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.691767",
      "entities": [
        "Machine Learning",
        "Epidemiology In",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16307v1",
      "title": "Generative AI Usage of University Students: Navigating Between Education and Business",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16307v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "This study investigates generative artificial intelligence (GenAI) usage of university students who study alongside their professional career. Previous literature has paid little attention to part-time students and the intersectional use of GenAI between education and business. This study examines with a grounded theory approach the characteristics of GenAI usage of part-time students. Eleven students from a distance learning university were interviewed. Three causal and four intervening conditions, as well as strategies were identified, to influence the use of GenAI. The study highlights both the potential and challenges of GenAI usage in education and business. While GenAI can significantly enhance productivity and learning outcomes, concerns about ethical implications, reliability, and the risk of academic misconduct persist. The developed grounded model offers a comprehensive understanding of GenAI usage among students, providing valuable insights for educators, policymakers, and developers of GenAI tools seeking to bridge the gap between education and business.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16307v1",
        "authors": [
          "Fabian Walke",
          "Veronika Föller"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.HC"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.692016",
      "entities": [
        "Navigating Between Education",
        "Artificial Intelligence",
        "University Students",
        "University",
        "Policy",
        "Intel",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16282v1",
      "title": "Neutral species facilitate coexistence among cyclically competing species under birth and death processes",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16282v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Natural birth and death are fundamental mechanisms of population dynamics in ecosystems and have played pivotal roles in shaping population dynamics. Nevertheless, in studies of cyclic competition systems governed by the rock-paper-scissors (RPS) game, these mechanisms have often been ignored in analyses of biodiversity. On the other hand, given the prevalence and profound impact on biodiversity, understanding how higher-order interactions (HOIs) can affect biodiversity is one of the most challenging issues, and thus HOIs have been continuously studied for their effects on biodiversity in systems of cyclic competing populations, with a focus on neutral species. However, in real ecosystems, species can evolve and die naturally or be preyed upon by predators, whereas previous studies have considered only classic reaction rules among three species with a neutral, nonparticipant species. To identify how neutral species can affect the biodiversity of the RPS system when species' natural birth and death are assumed, we consider a model of neutral species in higher-order interactions within the spatial RPS system, assuming birth-and-death processes. Extensive simulations show that when neutral species interfere positively, they dominate the available space, thereby reducing the proportion of other species. Conversely, when the interference is harmful, the density of competing species increases. In addition, unlike traditional RPS dynamics, biodiversity can be effectively maintained even in high-mobility regimes. Our study reaffirms the critical role of neutral species in preserving biodiversity.",
        "keywords": [
          "q-bio.PE",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16282v1",
        "authors": [
          "Yikang Lu",
          "Wenhao She",
          "Xiaofang Duan",
          "Junpyo Park"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "physics.soc-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.692361",
      "entities": [
        "Act",
        "RPS",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16201v1",
      "title": "Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16201v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives. We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16201v1",
        "authors": [
          "Sanket Badhe",
          "Deep Shah",
          "Nehal Kathrotia"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.692671",
      "entities": [
        "Large Language Models",
        "Implications Large",
        "Tail Knowledge",
        "Framework",
        "LLM",
        "Act",
        "MIT",
        "IoT",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16151v1",
      "title": "Queer NLP: A Critical Survey on Literature Gaps, Biases and Trends",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16151v1",
        "published_date": "2026-02-18"
      },
      "content": {
        "abstract": "Natural language processing (NLP) technologies are rapidly reshaping how language is created, processed, and analyzed by humans. With current and potential applications in hiring, law, healthcare, and other areas that impact people's lives, understanding and mitigating harms towards marginalized groups is critical. In this survey, we examine NLP research papers that explicitly address the relationship between LGBTQIA+ communities and NLP technologies. We systematically review all such papers published in the ACL Anthology, to answer the following research questions: (1) What are current research trends? (2) What gaps exist in terms of topics and methods? (3) What areas are open for future work? We find that while the number of papers on queer NLP has grown within the last few years, most papers take a reactive rather than a proactive approach, pointing out bias more often than mitigating it, and focusing on shortcomings of existing systems rather than creating new solutions. Our survey uncovers many opportunities for future work, especially regarding stakeholder involvement, intersectionality, interdisciplinarity, and languages other than English. We also offer an outlook from a queer studies perspective, highlighting understudied topics and gaps in the harms addressed in NLP papers. Beyond being a roadmap of what has been done, this survey is a call to action for work towards more just and inclusive NLP technologies.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16151v1",
        "authors": [
          "Sabine Weber",
          "Angelina Wang",
          "Ankush Gupta",
          "Arjun Subramonian",
          "Dennis Ulmer"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.692984",
      "entities": [
        "Critical Survey",
        "Literature Gaps",
        "Trends Natural",
        "Act",
        "NLP",
        "MIT",
        "ACL",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.16080v1",
      "title": "Surgical Activation Steering via Generative Causal Mediation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.16080v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.",
        "keywords": [
          "cs.CL",
          "cs.CY",
          "cs.HC",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.16080v1",
        "authors": [
          "Aruna Sankaranarayanan",
          "Amir Zur",
          "Atticus Geiger",
          "Dylan Hadfield-Menell"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.CY",
          "cs.HC",
          "cs.LG"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.693237",
      "entities": [
        "Generative Causal Mediation Where",
        "Surgical Activation Steering",
        "Generative Causal Mediation",
        "Act",
        "GCM",
        "NSF"
      ]
    },
    {
      "id": "arxiv-2602.15968v1",
      "title": "From Reflection to Repair: A Scoping Review of Dataset Documentation Tools",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15968v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their adoption. We present a systematic review supported by mixed-methods analysis of 59 dataset documentation publications to examine the motivations behind building documentation tools, how authors conceptualize documentation practices, and how these tools connect to existing systems, regulations, and cultural norms. Our analysis shows four persistent patterns in dataset documentation conceptualization that potentially impede adoption and standardization: unclear operationalizations of documentation's value, decontextualized designs, unaddressed labor demands, and a tendency to treat integration as future work. Building on these findings, we propose a shift in Responsible AI tool design toward institutional rather than individual solutions, and outline actions the HCI community can take to enable sustainable documentation practices.",
        "keywords": [
          "cs.SE",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15968v1",
        "authors": [
          "Pedro Reynolds-Cuéllar",
          "Marisol Wong-Villacres",
          "Adriana Alvarado Garcia",
          "Heila Precel"
        ],
        "arxiv_categories": [
          "cs.SE",
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.693566",
      "entities": [
        "Dataset Documentation Tools Dataset",
        "From Reflection",
        "Scoping Review",
        "Regulation",
        "Standard",
        "Act",
        "EPA",
        "HCI",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15736v1",
      "title": "SVD Incidence Centrality: A Unified Spectral Framework for Node and Edge Analysis in Directed Networks and Hypergraphs",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15736v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Identifying influential nodes and edges in directed networks remains a fundamental challenge across domains from social influence to biological regulation. Most existing centrality measures face a critical limitation: they either discard directional information through symmetrization or produce sparse, implementation-dependent rankings that obscure structural importance. We introduce a unified spectral framework for centrality analysis in directed networks grounded in the Singular value decomposition of the incidence matrix. The proposed approach derives both vertex and edge centralities via the pseudoinverse of Hodge Laplacians, yielding dense and well-resolved rankings that overcome the sparsity limitations commonly observed in betweenness centrality for directed graphs. Unlike traditional measures that require graph symmetrization, our framework naturally preserves directional information, enabling principled hub/authority analysis while maintaining mathematical consistency through spectral graph theory. The method extends naturally to hypergraphs through the same mathematical foundation. Experimental validation on real-world networks demonstrates the framework's effectiveness across diverse domains where traditional centrality measures encounter limitations due to implementation dependencies and sparse outputs.",
        "keywords": [
          "cs.SI",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15736v1",
        "authors": [
          "Jorge Luiz Franco",
          "Thomas Peron",
          "Alcebiades Dal Col",
          "Fabiano Petronetto",
          "Filipe Alves Neto Verri"
        ],
        "arxiv_categories": [
          "cs.SI",
          "physics.soc-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.693843",
      "entities": [
        "Unified Spectral Framework",
        "Hypergraphs Identifying",
        "Incidence Centrality",
        "Directed Networks",
        "Hodge Laplacians",
        "Edge Analysis",
        "Regulation",
        "Framework",
        "MIT",
        "SVD",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15638v1",
      "title": "Who Is Doing the Thinking? AI as a Dynamic Cognitive Partner: A Learner-Informed Framework",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15638v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Artificial intelligence is increasingly embedded in education, yet there remains a need to explain how students conceptualize AI's role in their thinking and learning. This study proposes a framework positioning AI as a dynamic cognitive partner whose function shifts across learning situations. Using qualitative analysis of written responses from 133 secondary students in Hong Kong following completion of an AI literacy course, we identified nine interrelated dimensions through which learners described AI as partnering with their cognition: conceptual scaffolding for difficult ideas; feedback and error detection; idea stimulation; cognitive organization; adaptive tutoring support; metacognitive monitoring support; task and cognitive load regulation; learning continuity beyond classroom boundaries; and explanation reframing through representational flexibility during moments of being stuck or overwhelmed. Across these dimensions, students distinguished between productive support that extends understanding and unproductive reliance that replaces cognitive effort, indicating situational awareness of when AI should and should not be used. Grounded in sociocultural theory, distributed cognition, self-regulated learning, and cognitive load perspectives, the framework clarifies how AI becomes integrated into learners' cognitive activity while illuminating the boundary between cognitive extension and substitution.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15638v1",
        "authors": [
          "C. K. Y Chan"
        ],
        "arxiv_categories": [
          "cs.CY"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.694155",
      "entities": [
        "Informed Framework Artificial",
        "Dynamic Cognitive Partner",
        "Artificial Intelligence",
        "Who Is Doing",
        "Regulation",
        "Hong Kong",
        "Framework",
        "Intel",
        "Meta",
        "Act",
        "WHO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15476v1",
      "title": "How to Detect Information Voids Using Longitudinal Data from Social Media and Web Searches",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15476v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "The model of the attention economy, where content producers compete for the attention of users, relies on two key forces: information supply and demand. This study leverages the feedback loop between these forces to develop a method for detecting and quantifying information voids, i.e., periods in which little or no reliable information is available on a given topic. Using a case study on COVID-19 vaccines rollout in six European countries, and drawing on data from multiple platforms including Facebook, Google, Twitter, Wikipedia, and online news outlets, we examine how information voids emerge, persist and correlate with a decline in the proportion of high-quality information circulating online. By conceptualising information voids as a specific regime of information spreading, we also quantify their counterpart, information overabundance, which constitute a central component of the current definition of infodemic. We show that information voids are associated with a higher prevalence of misinformation, thus representing problematic hotspots in which individuals are more likely to be misled by low-quality online content. Overall, our findings provide empirical support for the inclusion of information voids in mechanistic explanations of misinformation emergence.",
        "keywords": [
          "cs.CY",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15476v1",
        "authors": [
          "Irene Scalco",
          "Francesco Gesualdo",
          "Roy Cerqueti",
          "Matteo Cinelli"
        ],
        "arxiv_categories": [
          "cs.CY",
          "physics.soc-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.694446",
      "entities": [
        "Detect Information Voids Using",
        "Longitudinal Data",
        "Social Media",
        "COVID-19",
        "Vaccine",
        "Google",
        "COVID",
        "NIST",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15470v1",
      "title": "The Skeletal Trap: Mapping Spatial Inequality and Ghost Stops in Ankara's Transit Network",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15470v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Ankara's public transport crisis is commonly framed as a shortage of buses or operational inefficiency. This study argues that the problem is fundamentally morphological and structural. The city's leapfrog urban expansion has produced fragmented peripheral clusters disconnected from a rigid, center-oriented bus network. As a result, demand remains intensely concentrated along the Kizilay-Ulus axis and western corridors, while peripheral districts experience either chronic under-service or enforced transfer dependency. The deficiency is therefore not merely quantitative but rooted in the misalignment between urban macroform and network architecture. The empirical analysis draws on a 173-day operational dataset derived from route-level passenger and trip reports published by EGO under the former \"Transparent Ankara\" initiative. To overcome the absence of stop-level geospatial data, a Connectivity-Based Weighted Distribution Model reallocates passenger volumes to 1 km x 1 km grid cells using network centrality. The findings reveal persistent center-periphery asymmetries, structural bottlenecks, and spatially embedded accessibility inequalities.",
        "keywords": [
          "physics.soc-ph",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15470v1",
        "authors": [
          "Elifnaz Kancan"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "cs.LG"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.694714",
      "entities": [
        "Based Weighted Distribution Model",
        "Mapping Spatial Inequality",
        "Transit Network Ankara",
        "Transparent Ankara",
        "Ghost Stops",
        "NSF",
        "EGO",
        "AI",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15447v1",
      "title": "Household size can explain 40% of the variance in cumulative COVID-19 incidence across Europe",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15447v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "Household size impacts the spread of respiratory infectious diseases: Larger households tend to boost transmission by acquiring external infections more frequently and subsequently transmitting them back into the community. Furthermore, mandatory interventions primarily modulate contagion between households rather than within them. We developed an approach to quantify the role of household size in epidemics by separating within-household from out-household transmission, and found that household size explains 41% of the variability in cumulative COVID-19 incidence across 34 European countries (95% confidence interval: [15%, 46%]). The contribution of households to the overall dynamics can be quantified by a boost factor that increases with the effective household size, implying that countries with larger households require more stringent interventions to achieve the same levels of containment. This suggests that households constitute a structural (dis-)advantage that must be considered when designing and evaluating mitigation strategies.",
        "keywords": [
          "q-bio.PE",
          "math.DS",
          "physics.soc-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15447v1",
        "authors": [
          "Seba Contreras",
          "Philipp Dönges",
          "Maciej Filinski",
          "Joel Wagner",
          "Viktor Bezborodov"
        ],
        "arxiv_categories": [
          "q-bio.PE",
          "math.DS",
          "physics.soc-ph"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.694950",
      "entities": [
        "Europe Household",
        "COVID-19",
        "COVID",
        "Act",
        "EPA",
        "MIT",
        "AI",
        "EU",
        "UN"
      ]
    },
    {
      "id": "arxiv-2602.15439v1",
      "title": "Algorithmic Approaches to Opinion Selection for Online Deliberation: A Comparative Study",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "https://arxiv.org/abs/2602.15439v1",
        "published_date": "2026-02-17"
      },
      "content": {
        "abstract": "During deliberation processes, mediators and facilitators typically need to select a small and representative set of opinions later used to produce digestible reports for stakeholders. In online deliberation platforms, algorithmic selection is increasingly used to automate this process. However, such automation is not without consequences. For instance, enforcing consensus-seeking algorithmic strategies can imply ignoring or flattening conflicting preferences, which may lead to erasing minority voices and reducing content diversity. More generally, across the variety of existing selection strategies (e.g., consensus, diversity), it remains unclear how each approach influences desired democratic criteria such as proportional representation. To address this gap, we benchmark several algorithmic approaches in this context. We also build on social choice theory to propose a novel algorithm that incorporates both diversity and a balanced notion of representation in the selection strategy. We find empirically that while no single strategy dominates across all democratic desiderata, our social-choice-inspired selection rule achieves the strongest trade-off between proportional representation and diversity.",
        "keywords": [
          "cs.CY",
          "cs.AI",
          "cs.SI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2602.15439v1",
        "authors": [
          "Salim Hafid",
          "Manon Berriche",
          "Jean-Philippe Cointet"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.AI",
          "cs.SI"
        ],
        "steeps_mapping": "s_spiritual"
      },
      "preliminary_category": "s",
      "collected_at": "2026-02-18T19:02:02.695227",
      "entities": [
        "Comparative Study During",
        "Algorithmic Approaches",
        "Online Deliberation",
        "Opinion Selection",
        "AI",
        "UN"
      ]
    }
  ]
}