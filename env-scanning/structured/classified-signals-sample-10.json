{
  "scan_metadata": {
    "date": "2026-01-30",
    "sources_configured": 1,
    "sources_scanned": 1,
    "sources_failed": 0,
    "total_items": 120,
    "execution_time": 15.13,
    "mode": "multi_source",
    "days_back": 7,
    "timestamp": "2026-01-30T10:59:17.858978"
  },
  "items": [
    {
      "id": "arxiv-2601.20861v1",
      "title": "Evolutionary Strategies lead to Catastrophic Forgetting in LLMs",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20861v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.",
        "keywords": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20861v1",
        "authors": [
          "Immanuel Abdi",
          "Akshat Gupta",
          "Micah Mok"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.AI",
          "cs.CL"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823018",
      "final_category": "T",
      "classification_confidence": 0.95,
      "classification_reasoning": "Technical research on AI training algorithms and their limitations. Focus is on machine learning methodology, not applications or impacts.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    },
    {
      "id": "arxiv-2601.20860v1",
      "title": "Quantum teleportation in expanding FRW universe",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20860v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "We investigate the process of quantum teleportation in an expanding universe modeled by Friedmann-Robertson-Walker spacetime, focusing on two cosmologically relevant scenarios: a power-law expansion and the de Sitter universe. Adopting a field-theoretical approach, we analyze the quantum correlations between two comoving observers who share an entangled mode of a scalar field. Using the Bogoliubov transformation, we compute the teleportation fidelity and examine its dependence on the expansion rate, initial entanglement, and the mode frequency. Our findings indicate that spacetime curvature and the underlying cosmological background significantly affect the efficiency of quantum teleportation, particularly through mode mixing and vacuum structure. We also compare our results with the flat Minkowski case to highlight the role of cosmic expansion in degrading or preserving quantum information.",
        "keywords": [
          "hep-th",
          "gr-qc",
          "quant-ph"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20860v1",
        "authors": [
          "Babak Vakili"
        ],
        "arxiv_categories": [
          "hep-th",
          "gr-qc",
          "quant-ph"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823026",
      "final_category": "T",
      "classification_confidence": 0.92,
      "classification_reasoning": "Quantum physics research on teleportation technology in cosmological contexts. Pure technological/scientific research.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    },
    {
      "id": "arxiv-2601.20858v1",
      "title": "When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20858v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to \"uncontaminated\" languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an uncontaminated control. We confirm Bloomz's FLORES contamination and demonstrate that machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization. Further analysis shows that recall of memorized references often persists despite various source-side perturbation efforts like paraphrasing and named entity replacement. However, replacing named entities leads to a consistent decrease in BLEU, suggesting an effective probing method for memorization in contaminated models.",
        "keywords": [
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20858v1",
        "authors": [
          "David Tan",
          "Pinzhen Chen",
          "Josef van Genabith"
        ],
        "arxiv_categories": [
          "cs.CL"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823028",
      "final_category": "T",
      "classification_confidence": 0.93,
      "classification_reasoning": "Technical research on LLM evaluation methodology and benchmark contamination. Focus on machine learning systems.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    },
    {
      "id": "arxiv-2601.20857v1",
      "title": "FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20857v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.",
        "keywords": [
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20857v1",
        "authors": [
          "Hongyu Zhou",
          "Zisen Shao",
          "Sheng Miao"
        ],
        "arxiv_categories": [
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823030",
      "final_category": "T",
      "classification_confidence": 0.94,
      "classification_reasoning": "Computer vision technology for 3D rendering. Technical innovation in neural radiance fields.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    },
    {
      "id": "arxiv-2601.20856v1",
      "title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20856v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.",
        "keywords": [
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20856v1",
        "authors": [
          "Sebastiano Monti",
          "Carlo Nicolini",
          "Gianni Pellegrini"
        ],
        "arxiv_categories": [
          "cs.AI"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823031",
      "final_category": "T",
      "classification_confidence": 0.91,
      "classification_reasoning": "Benchmark development for evaluating LLM capabilities. Technical research on AI systems.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    },
    {
      "id": "arxiv-2601.20854v1",
      "title": "Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20854v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.",
        "keywords": [
          "cs.LG",
          "cs.AI"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20854v1",
        "authors": [
          "Aníbal Silva",
          "Moisés Santos",
          "André Restivo"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.AI"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823033",
      "final_category": "T",
      "classification_confidence": 0.93,
      "classification_reasoning": "Technical research on generative model architecture for tabular data. Machine learning methodology.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    },
    {
      "id": "arxiv-2601.20852v1",
      "title": "C3Box: A CLIP-based Class-Incremental Learning Toolbox",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20852v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.",
        "keywords": [
          "cs.LG",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20852v1",
        "authors": [
          "Hao Sun",
          "Da-Wei Zhou"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823034",
      "final_category": "T",
      "classification_confidence": 0.94,
      "classification_reasoning": "Software toolbox for continual learning research. Technical infrastructure for machine learning.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    },
    {
      "id": "arxiv-2601.20848v1",
      "title": "Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20848v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.",
        "keywords": [
          "cs.LG",
          "cs.AI",
          "cs.IR",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20848v1",
        "authors": [
          "Weixin Chen",
          "Li Chen",
          "Yuhan Zhao"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.AI",
          "cs.IR",
          "cs.CY"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823035",
      "final_category": "s",
      "classification_confidence": 0.78,
      "classification_reasoning": "While technically implemented in recommender systems, the core focus is on fairness values and ethical treatment of stakeholders. Addresses normative questions about equity rather than technical optimization.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    },
    {
      "id": "arxiv-2601.20847v1",
      "title": "A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20847v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.",
        "keywords": [
          "cs.AI",
          "cs.CV"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20847v1",
        "authors": [
          "Willams de Lima Costa",
          "Thifany Ketuli Silva de Souza",
          "Jonas Ferreira Silva"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CV"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823036",
      "final_category": "T",
      "classification_confidence": 0.92,
      "classification_reasoning": "Technical framework for sensor fusion and classification. Focus on engineering methodology and robustness.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    },
    {
      "id": "arxiv-2601.20846v1",
      "title": "End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20846v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.",
        "keywords": [
          "cs.RO"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20846v1",
        "authors": [
          "Jamie Hathaway",
          "Alireza Rastegarpanah",
          "Rustam Stolkin"
        ],
        "arxiv_categories": [
          "cs.RO"
        ]
      },
      "preliminary_category": "T",
      "collected_at": "2026-01-30T10:59:02.823037",
      "final_category": "T",
      "classification_confidence": 0.93,
      "classification_reasoning": "Robotics and reinforcement learning technical research. Focus on sim-to-real transfer methodology.",
      "classification_method": "claude_code_direct",
      "classification_cost": 0.0
    }
  ],
  "classification_metadata": {
    "classifier": "claude_code_direct",
    "version": "sonnet-4.5",
    "timestamp": "2026-01-30T11:00:49.844247",
    "total_classified": 10,
    "avg_confidence": 0.915,
    "cost": 0.0,
    "category_distribution": {
      "T": 9,
      "s": 1
    }
  }
}