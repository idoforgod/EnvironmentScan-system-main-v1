{
  "scan_metadata": {
    "date": "2026-01-30",
    "sources_configured": 1,
    "sources_scanned": 1,
    "sources_failed": 0,
    "total_items": 120,
    "execution_time": 15.13,
    "mode": "multi_source",
    "days_back": 7,
    "timestamp": "2026-01-30T10:59:17.858978"
  },
  "batch_info": {
    "batch_number": 5,
    "total_batches": 6,
    "start_index": 80,
    "end_index": 100,
    "batch_size": 20
  },
  "items": [
    {
      "id": "arxiv-2601.20848v1",
      "title": "Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20848v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.",
        "keywords": [
          "cs.LG",
          "cs.AI",
          "cs.IR",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20848v1",
        "authors": [
          "Weixin Chen",
          "Li Chen",
          "Yuhan Zhao"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.AI",
          "cs.IR",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790933"
    },
    {
      "id": "arxiv-2601.20838v1",
      "title": "Reward Models Inherit Value Biases from Pretraining",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20838v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the \"Big Two\" psychological axes, we show a robust preference of Llama RMs for \"agency\" and a corresponding robust preference of Gemma RMs for \"communion.\" This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.",
        "keywords": [
          "cs.LG",
          "cs.AI",
          "cs.CL",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20838v1",
        "authors": [
          "Brian Christian",
          "Jessica A. F. Thompson",
          "Elle Michelle Yang"
        ],
        "arxiv_categories": [
          "cs.LG",
          "cs.AI",
          "cs.CL",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790946"
    },
    {
      "id": "arxiv-2601.20792v1",
      "title": "Jurisdiction as Structural Barrier: How Privacy Policy Organization May Reduce Visibility of Substantive Disclosures",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20792v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Privacy policies are supposed to provide notice. But what if substantive information appears only where users skip it? We identify a structural pattern we call jurisdiction-siloed disclosure: information about data practices appearing in specific, actionable form only within regional compliance sections labeled \"California Residents\" or \"EU/UK Users,\" while general sections use vague or qualified language for the same practices. Our audit of 123 major companies identifies 282 potential instances across 77 companies (62.6% of this purposive sample). A conservative estimate restricted to practice categories validated against OPP-115 human annotations finds 138 instances across 54 companies (44%); post-2018 categories central to our findings await independent validation. If users skip jurisdiction-labeled sections as information foraging theory predicts, users outside regulated jurisdictions would receive less specific information about practices affecting them--a transparency failure operating through document architecture rather than omission. We propose universal substantive disclosure: practices affecting all users should appear in the main policy body, with regional sections containing only procedural rights information. This standard finds support in analogous disclosure regimes (securities, truth-in-lending, nutritional labeling) where material information must reach all affected parties. Regulators could operationalize this through the FTC's \"clear and conspicuous\" standard and GDPR transparency principles. This work is hypothesis-generating: we establish that the structural pattern exists and ground the transparency concern in behavioral theory, but direct measurement of jurisdiction-specific section skipping remains the critical validation priority. We release our methodology and annotated dataset to enable replication.",
        "keywords": [
          "cs.CL",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20792v1",
        "authors": [
          "Thomas Brackin"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.CY",
          "cs.HC"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790949"
    },
    {
      "id": "arxiv-2601.20731v1",
      "title": "QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20731v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized \"unmarked\" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.",
        "keywords": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20731v1",
        "authors": [
          "Mae Sosto",
          "Delfina Sol Martinez Pandiani",
          "Laura Hollink"
        ],
        "arxiv_categories": [
          "cs.CL",
          "cs.AI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790951"
    },
    {
      "id": "arxiv-2601.20727v1",
      "title": "Audit Trails for Accountability in Large Language Models",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20727v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Large language models (LLMs) are increasingly embedded in consequential decisions across healthcare, finance, employment, and public services. Yet accountability remains fragile because process transparency is rarely recorded in a durable and reviewable form. We propose LLM audit trails as a sociotechnical mechanism for continuous accountability. An audit trail is a chronological, tamper-evident, context-rich ledger of lifecycle events and decisions that links technical provenance (models, data, training and evaluation runs, deployments, monitoring) with governance records (approvals, waivers, and attestations), so organizations can reconstruct what changed, when, and who authorized it. This paper contributes: (1) a lifecycle framework that specifies event types, required metadata, and governance rationales; (2) a reference architecture with lightweight emitters, append only audit stores, and an auditor interface supporting cross organizational traceability; and (3) a reusable, open-source Python implementation that instantiates this audit layer in LLM workflows with minimal integration effort. We conclude by discussing limitations and directions for adoption.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20727v1",
        "authors": [
          "Victor Ojewale",
          "Harini Suresh",
          "Suresh Venkatasubramanian"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790953"
    },
    {
      "id": "arxiv-2601.20617v1",
      "title": "Agent Benchmarks Fail Public Sector Requirements",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20617v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Deploying Large Language Model-based agents (LLM agents) in the public sector requires assuring that they meet the stringent legal, procedural, and structural requirements of public-sector institutions. Practitioners and researchers often turn to benchmarks for such assessments. However, it remains unclear what criteria benchmarks must meet to ensure they adequately reflect public-sector requirements, or how many existing benchmarks do so. In this paper, we first define such criteria based on a first-principles survey of public administration literature: benchmarks must be \\emph{process-based}, \\emph{realistic}, \\emph{public-sector-specific} and report \\emph{metrics} that reflect the unique requirements of the public sector. We analyse more than 1,300 benchmark papers for these criteria using an expert-validated LLM-assisted pipeline. Our results show that no single benchmark meets all of the criteria. Our findings provide a call to action for both researchers to develop public sector-relevant benchmarks and for public-sector officials to apply these criteria when evaluating their own agentic use cases.",
        "keywords": [
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20617v1",
        "authors": [
          "Jonathan Rystrøm",
          "Chris Schmitz",
          "Karolina Korgul"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790954"
    },
    {
      "id": "arxiv-2601.20413v1",
      "title": "Schadenfreude in the Digital Public Sphere: A cross-national and decade-long analysis of Facebook news engagement",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20413v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Schadenfreude, or the pleasure derived from others' misfortunes, has become a visible and performative feature of online news engagement, yet little is known about its prevalence, dynamics, or social patterning. We examine schadenfreude on Facebook over a ten-year period across nine major news publishers in the United States, the United Kingdom, and India (one left-leaning, one right-leaning, and one centrist per country). Using a combination of human annotation and machine-learning classification, we identify posts describing misfortune and detect schadenfreude in nearly one million associated comments. We find that while sadness and anger dominate reactions to misfortune posts, laughter and amusement form a substantial and patterned minority. Schadenfreude is most frequent in moralized and political contexts, higher among right-leaning audiences, and more pronounced in India than in the United States or United Kingdom. Temporal and regression analyses further reveal that schadenfreude generally increases when groups are politically out of power, but these patterns differ across party lines. Together, our findings move beyond anecdotal accounts to map schadenfreude as a dynamic, context-dependent feature of digital discourse, revealing how it evolves over time and across ideological and cultural divides.",
        "keywords": [
          "cs.SI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20413v1",
        "authors": [
          "Nouar Aldahoul",
          "Hazem Ibrahim",
          "Majd Mahmutoglu"
        ],
        "arxiv_categories": [
          "cs.SI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790956"
    },
    {
      "id": "arxiv-2601.20245v1",
      "title": "How AI Impacts Skill Formation",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20245v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "AI assistance produces significant productivity gains across professional domains, particularly for novice workers. Yet how this assistance affects the development of skills required to effectively supervise AI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise their own skill acquisition in the process. We conduct randomized experiments to study how developers gained mastery of a new asynchronous programming library with and without the assistance of AI. We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average. Participants who fully delegated coding tasks showed some productivity improvements, but at the cost of learning the library. We identify six distinct AI interaction patterns, three of which involve cognitive engagement and preserve learning outcomes even when participants receive AI assistance. Our findings suggest that AI-enhanced productivity is not a shortcut to competence and AI assistance should be carefully adopted into workflows to preserve skill formation -- particularly in safety-critical domains.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20245v1",
        "authors": [
          "Judy Hanwen Shen",
          "Alex Tamkin"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.HC"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790958"
    },
    {
      "id": "arxiv-2601.20241v1",
      "title": "Adequately Tailoring Age Verification Regulations",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20241v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "The Supreme Court decision in Free Speech Coalition v. Paxton upheld the constitutionality of Texas H.B. 1181, one of the most constitutionally vulnerable of these age verification laws, holding that it was subject to and satisfied intermediate scrutiny and the requirement that age verification regulations be \"adequately tailored\". However, the decision leaves unresolved practical challenges. What is the current state of age verification legislation in the United States? How can \"adequate tailoring\" be interpreted in a way that is accessible to non-legal experts, particularly those in technical and engineering domains? What age verification approaches are used today, what infrastructures and standards support them, and what tradeoffs do they introduce? This paper addresses those questions by proposing an analytical model to interpret \"adequate tailoring\" from multiple perspectives with associated governmental goals and interests, and by applying that model to evaluate both current state laws and widely used verification methods. This paper's major contributions include: (1) we mapped the current U.S. age-verification legislative landscape; (2) we introduce an analytical model to analyze \"adequate tailoring\" for age verification and potential application to other online regulatory policies; and (3) we analyze the main technical approaches to age verification, highlighting the practical challenges and tradeoffs from a technical perspective. Further, while we focus on U.S. State laws, the principles underlying our framework are applicable to age-verification debates and methods worldwide.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20241v1",
        "authors": [
          "Shuang Liu",
          "Sarah Scheffler"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790959"
    },
    {
      "id": "arxiv-2601.20141v1",
      "title": "Large language models accurately predict public perceptions of support for climate action worldwide",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20141v1",
        "published_date": "2026-01-28"
      },
      "content": {
        "abstract": "Although most people support climate action, widespread underestimation of others' support stalls individual and systemic changes. In this preregistered experiment, we test whether large language models (LLMs) can reliably predict these perception gaps worldwide. Using country-level indicators and public opinion data from 125 countries, we benchmark four state-of-the-art LLMs against Gallup World Poll 2021/22 data and statistical regressions. LLMs, particularly Claude, accurately capture public perceptions of others' willingness to contribute financially to climate action (MAE approximately 5 p.p.; r = .77), comparable to statistical models, though performance declines in less digitally connected, lower-GDP countries. Controlled tests show that LLMs capture the key psychological process - social projection with a systematic downward bias - and rely on structured reasoning rather than memorized values. Overall, LLMs provide a rapid tool for assessing perception gaps in climate action, serving as an alternative to costly surveys in resource-rich countries and as a complement in underrepresented populations.",
        "keywords": [
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20141v1",
        "authors": [
          "Nattavudh Powdthavee",
          "Sandra J. Geiger"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790961"
    },
    {
      "id": "arxiv-2601.20100v1",
      "title": "Taming Toxic Talk: Using chatbots to intervene with users posting toxic comments",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20100v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "Generative AI chatbots have proven surprisingly effective at persuading people to change their beliefs and attitudes in lab settings. However, the practical implications of these findings are not yet clear. In this work, we explore the impact of rehabilitative conversations with generative AI chatbots on users who share toxic content online. Toxic behaviors -- like insults or threats of violence, are widespread in online communities. Strategies to deal with toxic behavior are typically punitive, such as removing content or banning users. Rehabilitative approaches are rarely attempted, in part due to the emotional and psychological cost of engaging with aggressive users. In collaboration with seven large Reddit communities, we conducted a large-scale field experiment (N=893) to invite people who had recently posted toxic content to participate in conversations with AI chatbots. A qualitative analysis of the conversations shows that many participants engaged in good faith and even expressed remorse or a desire to change. However, we did not observe a significant change in toxic behavior in the following month compared to a control group. We discuss possible explanations for our findings, as well as theoretical and practical implications based on our results.",
        "keywords": [
          "cs.HC",
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20100v1",
        "authors": [
          "Jeremy Foote",
          "Deepak Kumar",
          "Bedadyuti Jha"
        ],
        "arxiv_categories": [
          "cs.HC",
          "cs.AI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790963"
    },
    {
      "id": "arxiv-2601.20099v1",
      "title": "Dynamics of Human-AI Collective Knowledge on the Web: A Scalable Model and Insights for Sustainable Growth",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20099v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "Humans and large language models (LLMs) now co-produce and co-consume the web's shared knowledge archives. Such human-AI collective knowledge ecosystems contain feedback loops with both benefits (e.g., faster growth, easier learning) and systemic risks (e.g., quality dilution, skill reduction, model collapse). To understand such phenomena, we propose a minimal, interpretable dynamical model of the co-evolution of archive size, archive quality, model (LLM) skill, aggregate human skill, and query volume. The model captures two content inflows (human, LLM) controlled by a gate on LLM-content admissions, two learning pathways for humans (archive study vs. LLM assistance), and two LLM-training modalities (corpus-driven scaling vs. learning from human feedback). Through numerical experiments, we identify different growth regimes (e.g., healthy growth, inverted flow, inverted learning, oscillations), and show how platform and policy levers (gate strictness, LLM training, human learning pathways) shift the system across regime boundaries. Two domain configurations (PubMed, GitHub and Copilot) illustrate contrasting steady states under different growth rates and moderation norms. We also fit the model to Wikipedia's knowledge flow during pre-ChatGPT and post-ChatGPT eras separately. We find a rise in LLM additions with a concurrent decline in human inflow, consistent with a regime identified by the model. Our model and analysis yield actionable insights for sustainable growth of human-AI collective knowledge on the Web.",
        "keywords": [
          "cs.AI",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20099v1",
        "authors": [
          "Buddhika Nettasinghe",
          "Kang Zhao"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790964"
    },
    {
      "id": "arxiv-2601.20016v1",
      "title": "Fueling Volunteer Growth: the case of Wikipedia Administrators",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.20016v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "Wikipedia administrators are vital to the platform's success, performing over a million administrative actions annually. This multi-method study systematically analyzes adminship across 284 Wikipedia languages since 2018, revealing a critical two-sided trend: while over half of all Wikipedias show a net increase in administrators, almost two-thirds of highly active Wikipedias face decline. Our analysis, drawing from large-scale adminship log analysis, over 3000 surveys, and 12 interviews, reveals this decline is primarily driven by insufficient recruitment, not unusual attrition. We identify key barriers for potential administrators, including limited awareness, ambiguous requirements, a demanding selection process, and low initial interest. Recognizing that current administrators remain highly motivated and engaged, we propose actionable recommendations to strengthen recruitment pipelines and fuel Wikipedia administrator growth, crucial for Wikipedia's long-term sustainability.",
        "keywords": [
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.20016v1",
        "authors": [
          "Eli Asikin-Garmager",
          "Yu-Ming Liou",
          "Caroline Myrick"
        ],
        "arxiv_categories": [
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790966"
    },
    {
      "id": "arxiv-2601.19886v1",
      "title": "AI Cap-and-Trade: Efficiency Incentives for Accessibility and Sustainability",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19886v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "The race for artificial intelligence (AI) dominance often prioritizes scale over efficiency. Hyper-scaling is the common industry approach: larger models, more data, and as many computational resources as possible. Using more resources is a simpler path to improved AI performance. Thus, efficiency has been de-emphasized. Consequently, the need for costly computational resources has marginalized academics and smaller companies. Simultaneously, increased energy expenditure, due to growing AI use, has led to mounting environmental costs. In response to accessibility and sustainability concerns, we argue for research into, and implementation of, market-based methods that incentivize AI efficiency. We believe that incentivizing efficient operations and approaches will reduce emissions while opening new opportunities for academics and smaller companies. As a call to action, we propose a cap-and-trade system for AI. Our system provably reduces computations for AI deployment, thereby lowering emissions and monetizing efficiency to the benefit of of academics and smaller companies.",
        "keywords": [
          "cs.GT",
          "cs.AI",
          "cs.CY",
          "econ.GN"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19886v1",
        "authors": [
          "Marco Bornstein",
          "Amrit Singh Bedi"
        ],
        "arxiv_categories": [
          "cs.GT",
          "cs.AI",
          "cs.CY",
          "econ.GN"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790967"
    },
    {
      "id": "arxiv-2601.19837v1",
      "title": "Self-Sovereign Identity and eIDAS 2.0: An Analysis of Control, Privacy, and Legal Implications",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19837v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "European digital identity initiatives are grounded in regulatory frameworks designed to ensure interoperability and robust, harmonized security standards. The evolution of these frameworks culminates in eIDAS 2.0, whose origins trace back to the Electronic Signatures Directive 1999/93/EC, the first EU-wide legal foundation for the use of electronic signatures in cross-border electronic transactions. As technological capabilities advanced, the initial eIDAS 1.0 framework was increasingly criticized for its limitations and lack of comprehensiveness. Emerging decentralized approaches further exposed these shortcomings and introduced the possibility of integrating innovative identity paradigms, such as Self-Sovereign Identity (SSI) models. In this article, we analyse key provisions of the eIDAS 2.0 Regulation and its accompanying recitals, drawing on existing literature to identify legislative gaps and implementation challenges. Furthermore, we examine the European Digital Identity Architecture and Reference Framework (ARF), assessing its proposed guidelines and evaluating the extent to which its emerging implementations align with SSI principles.",
        "keywords": [
          "cs.CY",
          "cs.CR",
          "cs.ET",
          "cs.DC"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19837v1",
        "authors": [
          "Nacereddine Sitouah",
          "Marco Esposito",
          "Francesco Bruschi"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.CR",
          "cs.ET",
          "cs.DC"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790969"
    },
    {
      "id": "arxiv-2601.19814v1",
      "title": "Abundance and Economic diversity as a descriptor of cities' economic complexity",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19814v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "Intricate interactions among firms, institutions, and spatial structures shape urban economic systems. In this study, we propose a framework based on three structural dimensions -- abundance, diversity, and longevity (ADL) of economic units -- as proxies of urban economic complexity and resilience. Using a decade of georeferenced firm-level data from Mexico City, we analyze the relationships among ADL variables using regression, spatial correlation, and time-series clustering. Our results reveal nonlinear dynamics across urban space, with powerlaw behavior in central zones and logarithmic saturation in peripheral areas, suggesting differentiated growth regimes. Notably, firm longevity modulates the relationship between abundance and diversity, particularly in periurban transition zones. These spatial patterns point to an emerging polycentric restructuring within a traditionally monocentric metropolis. By integrating economic complexity theory with spatial analysis, our approach provides a scalable method to assess the adaptive capacity of urban economies. This has implications for understanding informality, designing inclusive urban policies, and navigating structural transitions in rapidly urbanizing regions.",
        "keywords": [
          "physics.soc-ph",
          "stat.OT",
          "stat.AP",
          "cs.CY"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19814v1",
        "authors": [
          "Marco A. Rosas Pulido",
          "Roberto Murcio",
          "Omar R. Vázquez"
        ],
        "arxiv_categories": [
          "physics.soc-ph",
          "stat.OT",
          "stat.AP",
          "cs.CY"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790971"
    },
    {
      "id": "arxiv-2601.19778v1",
      "title": "Reimagining Peer Review Process Through Multi-Agent Mechanism Design",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19778v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as \"broken.\" This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.",
        "keywords": [
          "cs.GT",
          "cs.AI",
          "cs.CY",
          "cs.SE",
          "cs.MA"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19778v1",
        "authors": [
          "Ahmad Farooq",
          "Kamran Iqbal"
        ],
        "arxiv_categories": [
          "cs.GT",
          "cs.AI",
          "cs.CY",
          "cs.SE",
          "cs.MA"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790972"
    },
    {
      "id": "arxiv-2601.19376v1",
      "title": "Teaching Machine Learning Fundamentals with LEGO Robotics",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19376v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "This paper presents the web-based platform Machine Learning with Bricks and an accompanying two-day course designed to teach machine learning concepts to students aged 12 to 17 through programming-free robotics activities. Machine Learning with Bricks is an open source platform and combines interactive visualizations with LEGO robotics to teach three core algorithms: KNN, linear regression, and Q-learning. Students learn by collecting data, training models, and interacting with robots via a web-based interface. Pre- and post-surveys with 14 students demonstrate significant improvements in conceptual understanding of machine learning algorithms, positive shifts in AI perception, high platform usability, and increased motivation for continued learning. This work demonstrates that tangible, visualization-based approaches can make machine learning concepts accessible and engaging for young learners while maintaining technical depth. The platform is freely available at https://learning-and-dynamics.github.io/ml-with-bricks/, with video tutorials guiding students through the experiments at https://youtube.com/playlist?list=PLx1grFu4zAcwfKKJZ1Ux4LwRqaePCOA2J.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.RO",
          "cs.HC",
          "cs.LG"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19376v1",
        "authors": [
          "Viacheslav Sydora",
          "Guner Dilsad Er",
          "Michael Muehlebach"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.RO",
          "cs.HC",
          "cs.LG"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790974"
    },
    {
      "id": "arxiv-2601.19342v1",
      "title": "Modeling Behavioral Signals in Job Scams: A Human-Centered Security Study",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19342v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "Job scams have emerged as a rapidly growing form of cybercrime that manipulates human decision-making processes. Existing countermeasures primarily focus on scam typologies or post-loss indicators, offering limited support for early-stage intervention. In this study, we examine how behavioral decision signals can be operationalized as computational features for identifying vulnerability-associated signals in job fraud. Using anonymous survey data collected from a university population, we analyze two dominant job scam pathways: payment-based scams that require upfront fees and task-based scams that begin with small rewards before escalating to financial demands. Drawing on behavioral economics, we operationalize sunk cost influence, urgency/time-pressure cues, and social proof as measurable behavioral signals, and analyze their association with payment behavior using exact inference under sparsity and uncertainty-aware estimation, with social proof treated as a context-dependent legitimacy cue rather than a standalone predictor. Our results show that urgency/time-pressure cues are significantly associated with payment behavior, consistent with their role as proximal compliance triggers during escalation. In contrast, opportunity-loss/FOMO cues were not reliably identifiable under the current operationalization in our encounter subset, highlighting the importance of measurement fidelity and cue-definition consistency. We further observe that emotional tone in victim narratives and selective non-response to sensitive questions vary systematically with financial loss and reporting behavior, suggesting that missingness may reflect a combination of survey fatigue and selective non-disclosure for sensitive items rather than purely random noise.",
        "keywords": [
          "cs.CY",
          "cs.CR"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19342v1",
        "authors": [
          "Goni Anagha",
          "Vishakha Dasi Agrawal",
          "Gargi Sarkar"
        ],
        "arxiv_categories": [
          "cs.CY",
          "cs.CR"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790976"
    },
    {
      "id": "arxiv-2601.19062v1",
      "title": "Who's in Charge? Disempowerment Patterns in Real-World LLM Usage",
      "source": {
        "name": "arXiv",
        "type": "academic",
        "url": "http://arxiv.org/abs/2601.19062v1",
        "published_date": "2026-01-27"
      },
      "content": {
        "abstract": "Although AI assistants are now deeply embedded in society, there has been limited empirical study of how their usage affects human empowerment. We present the first large-scale empirical analysis of disempowerment patterns in real-world AI assistant interactions, analyzing 1.5 million consumer Claude$.$ai conversations using a privacy-preserving approach. We focus on situational disempowerment potential, which occurs when AI assistant interactions risk leading users to form distorted perceptions of reality, make inauthentic value judgments, or act in ways misaligned with their values. Quantitatively, we find that severe forms of disempowerment potential occur in fewer than one in a thousand conversations, though rates are substantially higher in personal domains like relationships and lifestyle. Qualitatively, we uncover several concerning patterns, such as validation of persecution narratives and grandiose identities with emphatic sycophantic language, definitive moral judgments about third parties, and complete scripting of value-laden personal communications that users appear to implement verbatim. Analysis of historical trends reveals an increase in the prevalence of disempowerment potential over time. We also find that interactions with greater disempowerment potential receive higher user approval ratings, possibly suggesting a tension between short-term user preferences and long-term human empowerment. Our findings highlight the need for AI systems designed to robustly support human autonomy and flourishing.",
        "keywords": [
          "cs.AI",
          "cs.CY",
          "cs.HC",
          "cs.CL"
        ],
        "language": "en"
      },
      "metadata": {
        "arxiv_id": "2601.19062v1",
        "authors": [
          "Mrinank Sharma",
          "Miles McCain",
          "Raymond Douglas"
        ],
        "arxiv_categories": [
          "cs.AI",
          "cs.CY",
          "cs.HC",
          "cs.CL"
        ]
      },
      "preliminary_category": "P",
      "collected_at": "2026-01-30T10:59:14.790977"
    }
  ]
}