{
  "metadata": {
    "workflow": "wf2-arxiv",
    "scan_date": "2026-02-16",
    "total_signals_analyzed": 568,
    "top_15_selected": true,
    "temporal_exception": {
      "applied": true,
      "reason": "arXiv weekend posting gap",
      "original_window": "2026-02-13T21:48 to 2026-02-15T21:48 UTC",
      "actual_paper_dates": "2026-02-12"
    },
    "steeps_distribution": {
      "T_Technological": 7,
      "S_Social": 3,
      "E_Economic": 2,
      "E_Environmental": 1,
      "P_Political": 1,
      "s_spiritual": 1
    }
  },
  "ranked_signals": [
    {
      "rank": 1,
      "id": "arxiv-2602.12281v1",
      "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
      "url": "https://arxiv.org/abs/2602.12281v1",
      "published_date": "2026-02-12",
      "steeps": "T_Technological",
      "psst_score": 90.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 80,
        "ES": 84,
        "CC": 80,
        "IC": 75
      },
      "abstract": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.",
      "arxiv_categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "authors": [
        "Jacky Kwok",
        "Xilun Zhang",
        "Mengdi Xu",
        "Yuejiang Liu",
        "Azalia Mirhoseini"
      ],
      "entities": [
        "Scaling Verification Can Be",
        "Scaling Policy Learning",
        "More Effective",
        "Framework",
        "Policy",
        "Robot",
        "Act",
        "VLA",
        "VLM",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 2,
      "id": "arxiv-2602.12279v1",
      "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
      "url": "https://arxiv.org/abs/2602.12279v1",
      "published_date": "2026-02-12",
      "steeps": "T_Technological",
      "psst_score": 89.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 85,
        "ES": 84,
        "CC": 80,
        "IC": 80
      },
      "abstract": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.",
      "arxiv_categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "authors": [
        "Leon Liangyu Chen",
        "Haoyu Ma",
        "Zhipeng Fan",
        "Ziqi Huang",
        "Animesh Sinha"
      ],
      "entities": [
        "Unified Multimodal Chain",
        "Scaling Unified",
        "Thought Test",
        "Framework",
        "TTS",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 3,
      "id": "arxiv-2602.12092v1",
      "title": "DeepSight: An All-in-One LM Safety Toolkit",
      "url": "https://arxiv.org/abs/2602.12092v1",
      "published_date": "2026-02-12",
      "steeps": "T_Technological",
      "psst_score": 88.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 95,
        "ES": 87,
        "CC": 75,
        "IC": 80
      },
      "abstract": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.",
      "arxiv_categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "authors": [
        "Bo Zhang",
        "Jiaxuan Guo",
        "Lijun Li",
        "Dongrui Liu",
        "Sujin Chen"
      ],
      "entities": [
        "Multimodal Large Language Models",
        "Large Language Models",
        "Safety Toolkit As",
        "Large Models",
        "Protocol",
        "An All",
        "NSF",
        "LLM",
        "Act",
        "EPA",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 4,
      "id": "arxiv-2602.12207v1",
      "title": "VIRENA: Virtual Arena for Research, Education, and Democratic Innovation",
      "url": "https://arxiv.org/abs/2602.12207v1",
      "published_date": "2026-02-12",
      "steeps": "S_Social",
      "psst_score": 88.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 75,
        "ES": 84,
        "CC": 80,
        "IC": 75
      },
      "abstract": "Digital platforms shape how people communicate, deliberate, and form opinions. Studying these dynamics has become increasingly difficult due to restricted data access, ethical constraints on real-world experiments, and limitations of existing research tools. VIRENA (Virtual Arena) is a platform that enables controlled experimentation in realistic social media environments. Multiple participants interact simultaneously in realistic replicas of feed-based platforms (Instagram, Facebook, Reddit) and messaging apps (WhatsApp, Messenger). Large language model-powered AI agents participate alongside humans with configurable personas and realistic behavior. Researchers can manipulate content moderation approaches, pre-schedule stimulus content, and run experiments across conditions through a visual interface requiring no programming skills. VIRENA makes possible research designs that were previously impractical: studying human--AI interaction in realistic social contexts, experimentally comparing moderation interventions, and observing group deliberation as it unfolds. Built on open-source technologies that ensure data remain under institutional control and comply with data protection requirements, VIRENA is currently in use at the University of Zurich and available for pilot collaborations. Designed for researchers, educators, and public organizations alike, VIRENA's no-code interface makes controlled social media simulation accessible across disciplines and sectors. This paper documents its design, architecture, and capabilities.",
      "arxiv_categories": [
        "cs.HC",
        "cs.AI",
        "cs.SI"
      ],
      "authors": [
        "Emma Hoes",
        "K. Jonathan Klueser",
        "Fabrizio Gilardi"
      ],
      "entities": [
        "Democratic Innovation Digital",
        "Virtual Arena",
        "University",
        "VIRENA",
        "MIT",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 5,
      "id": "arxiv-2602.11897v1",
      "title": "Agentic AI for Cybersecurity: A Meta-Cognitive Architecture for Governable Autonomy",
      "url": "https://arxiv.org/abs/2602.11897v1",
      "published_date": "2026-02-12",
      "steeps": "T_Technological",
      "psst_score": 87.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 85,
        "ES": 81,
        "CC": 85,
        "IC": 80
      },
      "abstract": "Contemporary AI-driven cybersecurity systems are predominantly architected as model-centric detection and automation pipelines optimized for task-level performance metrics such as accuracy and response latency. While effective for bounded classification tasks, these architectures struggle to support accountable decision-making under adversarial uncertainty, where actions must be justified, governed, and aligned with organizational and regulatory constraints. This paper argues that cybersecurity orchestration should be reconceptualized as an agentic, multi-agent cognitive system, rather than a linear sequence of detection and response components. We introduce a conceptual architectural framework in which heterogeneous AI agents responsible for detection, hypothesis formation, contextual interpretation, explanation, and governance are coordinated through an explicit meta-cognitive judgement function. This function governs decision readiness and dynamically calibrates system autonomy when evidence is incomplete, conflicting, or operationally risky. By synthesizing distributed cognition theory, multi-agent systems research, and responsible AI governance frameworks, we demonstrate that modern security operations already function as distributed cognitive systems, albeit without an explicit organizing principle. Our contribution is to make this cognitive structure architecturally explicit and governable by embedding meta-cognitive judgement as a first-class system function. We discuss implications for security operations centers, accountable autonomy, and the design of next-generation AI-enabled cyber defence architectures. The proposed framework shifts the focus of AI in cybersecurity from optimizing isolated predictions to governing autonomy under uncertainty.",
      "arxiv_categories": [
        "cs.CR",
        "cs.AI"
      ],
      "authors": [
        "Andrei Kojukhov",
        "Arkady Bovshover"
      ],
      "entities": [
        "Governable Autonomy Contemporary",
        "Cognitive Architecture",
        "Framework",
        "Meta",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 6,
      "id": "arxiv-2602.11483v1",
      "title": "Understanding Persuasive Interactions between Generative Social Agents and Humans: The Knowledge-based Persuasion Model (KPM)",
      "url": "https://arxiv.org/abs/2602.11483v1",
      "published_date": "2026-02-12",
      "steeps": "S_Social",
      "psst_score": 87.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 80,
        "ES": 81,
        "CC": 80,
        "IC": 80
      },
      "abstract": "Generative social agents (GSAs) use artificial intelligence to autonomously communicate with human users in a natural and adaptive manner. Currently, there is a lack of theorizing regarding interactions with GSAs, and likewise, few guidelines exist for studying how they influence user attitudes and behaviors. Consequently, we propose the Knowledge-based Persuasion Model (KPM) as a novel theoretical framework. According to the KPM, a GSA's self, user, and context-related knowledge drives its persuasive behavior, which in turn shapes the attitudes and behaviors of a responding human user. By synthesizing existing research, the model offers a structured approach to studying interactions with GSAs, supporting the development of agents that motivate rather than manipulate humans. Accordingly, the KPM encourages the integration of responsible GSAs that adhere to social norms and ethical standards with the goal of increasing user wellbeing. Implications of the KPM for research and application domains such as healthcare and education are discussed.",
      "arxiv_categories": [
        "cs.HC",
        "cs.AI"
      ],
      "authors": [
        "Stephan Vonschallen",
        "Friederike Eyssel",
        "Theresa Schmiedel"
      ],
      "entities": [
        "Understanding Persuasive Interactions",
        "Generative Social Agents",
        "Artificial Intelligence",
        "Persuasion Model",
        "Framework",
        "Guideline",
        "Standard",
        "Intel",
        "GSA",
        "Act",
        "KPM",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 7,
      "id": "arxiv-2602.11829v1",
      "title": "Towards Sustainable Investment Policies Informed by Opponent Shaping",
      "url": "https://arxiv.org/abs/2602.11829v1",
      "published_date": "2026-02-12",
      "steeps": "E_Economic",
      "psst_score": 87.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 90,
        "ES": 81,
        "CC": 95,
        "IC": 75
      },
      "abstract": "Addressing climate change requires global coordination, yet rational economic actors often prioritize immediate gains over collective welfare, resulting in social dilemmas. InvestESG is a recently proposed multi-agent simulation that captures the dynamic interplay between investors and companies under climate risk. We provide a formal characterization of the conditions under which InvestESG exhibits an intertemporal social dilemma, deriving theoretical thresholds at which individual incentives diverge from collective welfare. Building on this, we apply Advantage Alignment, a scalable opponent shaping algorithm shown to be effective in general-sum games, to influence agent learning in InvestESG. We offer theoretical insights into why Advantage Alignment systematically favors socially beneficial equilibria by biasing learning dynamics toward cooperative outcomes. Our results demonstrate that strategically shaping the learning processes of economic agents can result in better outcomes that could inform policy mechanisms to better align market incentives with long-term sustainability goals.",
      "arxiv_categories": [
        "cs.LG",
        "cs.GT"
      ],
      "authors": [
        "Juan Agustin Duque",
        "Razvan Ciuca",
        "Ayoub Echchahed",
        "Hugo Larochelle",
        "Aaron Courville"
      ],
      "entities": [
        "Towards Sustainable Investment Policies",
        "Opponent Shaping Addressing",
        "Advantage Alignment",
        "Policy",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 8,
      "id": "arxiv-2602.11729v1",
      "title": "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs",
      "url": "https://arxiv.org/abs/2602.11729v1",
      "published_date": "2026-02-12",
      "steeps": "T_Technological",
      "psst_score": 86.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 95,
        "ES": 84,
        "CC": 75,
        "IC": 80
      },
      "abstract": "Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but have only ever been applied to base vs finetune comparisons. We provide the first application of crosscoders to cross-architecture model diffing and introduce Dedicated Feature Crosscoders (DFCs), an architectural modification designed to better isolate features unique to one model. Using this technique, we find in an unsupervised fashion features including Chinese Communist Party alignment in Qwen3-8B and Deepseek-R1-0528-Qwen3-8B, American exceptionalism in Llama3.1-8B-Instruct, and a copyright refusal mechanism in GPT-OSS-20B. Together, our results work towards establishing cross-architecture crosscoder model diffing as an effective method for identifying meaningful behavioral differences between AI models.",
      "arxiv_categories": [
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "authors": [
        "Thomas Jiralerspong",
        "Trenton Bricken"
      ],
      "entities": [
        "Dedicated Feature Crosscoders",
        "Architecture Model Diffing",
        "Chinese Communist Party",
        "Unsupervised Discovery",
        "Differences Between",
        "R1-0528",
        "NIST",
        "LLM",
        "OSS",
        "GPT",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 9,
      "id": "arxiv-2602.11924v1",
      "title": "Who Does What? Archetypes of Roles Assigned to LLMs During Human-AI Decision-Making",
      "url": "https://arxiv.org/abs/2602.11924v1",
      "published_date": "2026-02-12",
      "steeps": "S_Social",
      "psst_score": 86.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 85,
        "ES": 81,
        "CC": 85,
        "IC": 80
      },
      "abstract": "LLMs are increasingly supporting decision-making across high-stakes domains, requiring critical reflection on the socio-technical factors that shape how humans and LLMs are assigned roles and interact during human-in-the-loop decision-making. This paper introduces the concept of human-LLM archetypes -- defined as re-curring socio-technical interaction patterns that structure the roles of humans and LLMs in collaborative decision-making. We describe 17 human-LLM archetypes derived from a scoping literature review and thematic analysis of 113 LLM-supported decision-making papers. Then, we evaluate these diverse archetypes across real-world clinical diagnostic cases to examine the potential effects of adopting distinct human-LLM archetypes on LLM outputs and decision outcomes. Finally, we present relevant tradeoffs and design choices across human-LLM archetypes, including decision control, social hierarchies, cognitive forcing strategies, and information requirements. Through our analysis, we show that selection of human-LLM interaction archetype can influence LLM outputs and decisions, bringing important risks and considerations for the designers of human-AI decision-making systems",
      "arxiv_categories": [
        "cs.HC",
        "cs.AI"
      ],
      "authors": [
        "Shreya Chappidi",
        "Jatinder Singh",
        "Andra V. Krauze"
      ],
      "entities": [
        "Roles Assigned",
        "Who Does What",
        "During Human",
        "LLM",
        "DOE",
        "Act",
        "WHO",
        "AI"
      ]
    },
    {
      "rank": 10,
      "id": "arxiv-2602.11946v1",
      "title": "Towards a Sustainable Age of Information Metric: Carbon Footprint of Real-Time Status Updates",
      "url": "https://arxiv.org/abs/2602.11946v1",
      "published_date": "2026-02-12",
      "steeps": "E_Environmental",
      "psst_score": 86.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 75,
        "ES": 78,
        "CC": 80,
        "IC": 75
      },
      "abstract": "The timeliness of collected information is essential for monitoring and control in data-driven intelligent infrastructures. It is typically quantified using the Age of Information (AoI) metric, which has been widely adopted to capture the freshness of information received in the form of status updates. While AoI-based metrics quantify how timely the collected information is, they largely overlook the environmental impact associated with frequent transmissions, specifically, the resulting Carbon Footprint (CF). To address this gap, we introduce a carbon-aware AoI framework. We first derive closed-form expressions for the average AoI under constrained CF budgets for the baseline $M/M/1$ and $M/M/1^*$ queuing models, assuming fixed Carbon Intensity (CI). We then extend the analysis by treating CI as a dynamic, time-varying parameter and solve the AoI minimization problem. Our results show that minimizing AoI does not inherently minimize CF, highlighting a clear trade-off between information freshness and environmental impact. CI variability further affects achievable AoI, indicating that sustainable operation requires joint optimization of CF budgets, Signal-to-noise Ratio (SNR), and transmission scheduling. This work lays the foundation for carbon-aware information freshness optimization in next-generation networks.",
      "arxiv_categories": [
        "cs.IT"
      ],
      "authors": [
        "Shih-Kai Chou",
        "Maice Costa",
        "Mihael Mohorčič",
        "Jernej Hribar"
      ],
      "entities": [
        "Information Metric",
        "Carbon Footprint",
        "Carbon Intensity",
        "Sustainable Age",
        "Framework",
        "Intel",
        "DOE",
        "Act",
        "SNR",
        "EU",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 11,
      "id": "arxiv-2602.12276v1",
      "title": "Agentic Test-Time Scaling for WebAgents",
      "url": "https://arxiv.org/abs/2602.12276v1",
      "published_date": "2026-02-12",
      "steeps": "T_Technological",
      "psst_score": 85.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 75,
        "ES": 81,
        "CC": 75,
        "IC": 75
      },
      "abstract": "Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.",
      "arxiv_categories": [
        "cs.AI",
        "cs.CL"
      ],
      "authors": [
        "Nicholas Lee",
        "Lutfi Eren Erdogan",
        "Chris Joseph John",
        "Surya Krishnapillai",
        "Michael W. Mahoney"
      ],
      "entities": [
        "Neural Network",
        "Agentic Test",
        "Time Scaling",
        "Aware Test",
        "Standard",
        "CATTS",
        "LLM",
        "Act",
        "EU",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 12,
      "id": "arxiv-2602.12260v1",
      "title": "Legitimate Overrides in Decentralized Protocols",
      "url": "https://arxiv.org/abs/2602.12260v1",
      "published_date": "2026-02-12",
      "steeps": "P_Political",
      "psst_score": 85.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 85,
        "ES": 84,
        "CC": 80,
        "IC": 80
      },
      "abstract": "Decentralized protocols claim immutable, rule-based execution, yet many embed emergency mechanisms such as chain-level freezes, protocol pauses, and account quarantines. These overrides are crucial for responding to exploits and systemic failures, but they expose a core tension: when does intervention preserve trust and when is it perceived as illegitimate discretion? With approximately $10$ billion in technical exploit losses potentially addressable by onchain intervention (2016--2026), the design of these mechanisms has high practical stakes, but current approaches remain ad hoc and ideologically charged. We address this gap by developing a Scope $\\times$ Authority taxonomy that maps the design space of emergency architectures along two dimensions: the precision of the intervention and the concentration of trigger authority. We formalize the resulting tradeoffs of a standing centralization cost versus containment speed and collateral disruption as a stochastic cost-minimization problem; and derive three testable predictions. Assessing these predictions against 705 documented exploit incidents, we find that containment time varies systematically by authority type; that losses follow a heavy-tailed distribution ($α\\approx 1.33$) concentrating risk in rare catastrophic events; and that community sentiment measurably modulates the effective cost of maintaining intervention capability. The analysis yields concrete design principles that move emergency governance from ideological debate towards quantitative engineering.",
      "arxiv_categories": [
        "cs.CR",
        "cs.CY",
        "cs.DC"
      ],
      "authors": [
        "Oghenekaro Elem",
        "Nimrod Talmon"
      ],
      "entities": [
        "Decentralized Protocols Decentralized",
        "Legitimate Overrides",
        "Protocol",
        "Bill",
        "DOE",
        "Act",
        "UN",
        "AI"
      ]
    },
    {
      "rank": 13,
      "id": "arxiv-2602.12099v1",
      "title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
      "url": "https://arxiv.org/abs/2602.12099v1",
      "published_date": "2026-02-12",
      "steeps": "T_Technological",
      "psst_score": 84.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 80,
        "ES": 78,
        "CC": 80,
        "IC": 80
      },
      "abstract": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}.",
      "arxiv_categories": [
        "cs.CV"
      ],
      "authors": [
        " GigaBrain Team",
        "Boyuan Wang",
        "Chaojun Ni",
        "Guan Huang",
        "Guosheng Zhao"
      ],
      "entities": [
        "Based Reinforcement Learning Vision",
        "Espresso Preparation",
        "Laundry Folding",
        "GigaBrain-0.5",
        "Box Packing",
        "GigaBrain-0",
        "Policy",
        "RECAP",
        "Robot",
        "RAMP",
        "MIT",
        "Act",
        "EPA",
        "IoT",
        "VLA"
      ]
    },
    {
      "rank": 14,
      "id": "arxiv-2602.12181v1",
      "title": "Convex Markov Games and Beyond: New Proof of Existence, Characterization and Learning Algorithms for Nash Equilibria",
      "url": "https://arxiv.org/abs/2602.12181v1",
      "published_date": "2026-02-12",
      "steeps": "E_Economic",
      "psst_score": 84.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 80,
        "ES": 84,
        "CC": 80,
        "IC": 75
      },
      "abstract": "Convex Markov Games (cMGs) were recently introduced as a broad class of multi-agent learning problems that generalize Markov games to settings where strategic agents optimize general utilities beyond additive rewards. While cMGs expand the modeling frontier, their theoretical foundations, particularly the structure of Nash equilibria (NE) and guarantees for learning algorithms, are not yet well understood. In this work, we address these gaps for an extension of cMGs, which we term General Utility Markov Games (GUMGs), capturing new applications requiring coupling between agents' occupancy measures. We prove that in GUMGs, Nash equilibria coincide with the fixed points of projected pseudo-gradient dynamics (i.e., first-order stationary points), enabled by a novel agent-wise gradient domination property. This insight also yields a simple proof of NE existence using Brouwer's fixed-point theorem. We further show the existence of Markov perfect equilibria. Building on this characterization, we establish a policy gradient theorem for GUMGs and design a model-free policy gradient algorithm. For potential GUMGs, we establish iteration complexity guarantees for computing approximate-NE under exact gradients and provide sample complexity bounds in both the generative model and on-policy settings. Our results extend beyond prior work restricted to zero-sum cMGs, providing the first theoretical analysis of common-interest cMGs.",
      "arxiv_categories": [
        "cs.GT",
        "cs.LG",
        "cs.MA"
      ],
      "authors": [
        "Anas Barakat",
        "Ioannis Panageas",
        "Antonios Varvitsiotis"
      ],
      "entities": [
        "Nash Equilibria Convex Markov",
        "General Utility Markov Games",
        "Learning Algorithms",
        "Convex Markov Games",
        "New Proof",
        "Policy",
        "Act",
        "EU",
        "UN"
      ]
    },
    {
      "rank": 15,
      "id": "arxiv-2602.11956v1",
      "title": "TAVAE: A VAE with Adaptable Priors Explains Contextual Modulation in the Visual Cortex",
      "url": "https://arxiv.org/abs/2602.11956v1",
      "published_date": "2026-02-12",
      "steeps": "s_spiritual",
      "psst_score": 83.0,
      "psst_dimensions": {
        "SR": 90,
        "TC": 80,
        "DC": 75,
        "ES": 84,
        "CC": 75,
        "IC": 75
      },
      "abstract": "The brain interprets visual information through learned regularities, a computation formalized as probabilistic inference under a prior. The visual cortex establishes priors for this inference, some delivered through established top-down connections that inform low-level cortices about statistics represented at higher levels in the cortical hierarchy. While evidence shows that adaptation leads to priors reflecting the structure of natural images, it remains unclear whether similar priors can be flexibly acquired when learning a specific task. To investigate this, we built a generative model of V1 optimized for a simple discrimination task and analyzed it together with large-scale recordings from mice performing an analogous task. In line with recent approaches, we assumed that neuronal activity in V1 corresponds to latent posteriors in the generative model, enabling investigation of task-related priors in neuronal responses. To obtain a flexible test bed, we extended the VAE formalism so that a task can be acquired efficiently by reusing previously learned representations. Task-specific priors learned by this Task-Amortized VAE were used to investigate biases in mice and model when presenting stimuli that violated trained task statistics. Mismatch between learned task statistics and incoming sensory evidence produced signatures of uncertainty in stimulus category in the TAVAE posterior, reflecting properties of bimodal response profiles in V1 recordings. The task-optimized generative model accounted for key characteristics of V1 population activity, including within-day updates to population responses. Our results confirm that flexible task-specific contextual priors can be learned on demand by the visual system and deployed as early as the entry level of visual cortex.",
      "arxiv_categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "authors": [
        "Balázs Meszéna",
        "Keith T. Murray",
        "Julien Corbo",
        "O. Batuhan Erkat",
        "Márton A. Hajnal"
      ],
      "entities": [
        "Adaptable Priors Explains Contextual",
        "TAVAE",
        "Act",
        "VAE",
        "EU",
        "UN",
        "AI"
      ]
    }
  ]
}